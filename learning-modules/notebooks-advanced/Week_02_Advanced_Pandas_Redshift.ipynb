{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Advanced Pandas with Redshift\n",
    "\n",
    "## Learning Objectives\n",
    "- Read and process large datasets from Redshift efficiently\n",
    "- Master chunked processing with pandas\n",
    "- Implement memory optimization techniques\n",
    "- Optimize SQL queries for large tables\n",
    "- Write data back to Redshift efficiently\n",
    "- Use Dask for out-of-memory datasets\n",
    "- Benchmark and optimize performance\n",
    "- Apply production-ready patterns\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install pandas numpy psycopg2-binary sqlalchemy boto3\n",
    "pip install dask[complete] dask[dataframe] pyarrow fastparquet\n",
    "pip install s3fs awscli memory_profiler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Iterator, List, Dict, Optional\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# Database\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from sqlalchemy import create_engine, text\n",
    "import boto3\n",
    "\n",
    "# Utilities\n",
    "from io import StringIO\n",
    "import psutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Dask version: {dask.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Connection Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftManager:\n",
    "    \"\"\"Production-ready Redshift connection and query manager.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, str]):\n",
    "        self.config = config\n",
    "        self.engine = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._create_engine()\n",
    "    \n",
    "    def _create_engine(self):\n",
    "        \"\"\"Create SQLAlchemy engine with connection pooling.\"\"\"\n",
    "        connection_string = (\n",
    "            f\"postgresql+psycopg2://{self.config['user']}:{self.config['password']}\"\n",
    "            f\"@{self.config['host']}:{self.config.get('port', 5439)}/{self.config['database']}\"\n",
    "        )\n",
    "        \n",
    "        self.engine = create_engine(\n",
    "            connection_string,\n",
    "            pool_size=10,\n",
    "            max_overflow=20,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600,\n",
    "            connect_args={\n",
    "                'connect_timeout': 10,\n",
    "                'options': '-c statement_timeout=600000'  # 10 minutes\n",
    "            }\n",
    "        )\n",
    "        self.logger.info(\"Redshift engine created\")\n",
    "    \n",
    "    def read_query(self, query: str, chunksize: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"Execute query and return results as DataFrame.\"\"\"\n",
    "        self.logger.info(f\"Executing query (chunksize={chunksize})\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if chunksize:\n",
    "                return pd.read_sql(query, self.engine, chunksize=chunksize)\n",
    "            else:\n",
    "                df = pd.read_sql(query, self.engine)\n",
    "                elapsed = time.time() - start_time\n",
    "                self.logger.info(f\"Query returned {len(df):,} rows in {elapsed:.2f}s\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Query failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def read_table_chunks(self, table: str, chunk_size: int = 100000,\n",
    "                         columns: Optional[List[str]] = None,\n",
    "                         where: Optional[str] = None) -> Iterator[pd.DataFrame]:\n",
    "        \"\"\"Read table in chunks for memory-efficient processing.\"\"\"\n",
    "        cols = ', '.join(columns) if columns else '*'\n",
    "        where_clause = f\"WHERE {where}\" if where else \"\"\n",
    "        \n",
    "        query = f\"SELECT {cols} FROM {table} {where_clause}\"\n",
    "        \n",
    "        self.logger.info(f\"Reading table {table} in chunks of {chunk_size:,}\")\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(pd.read_sql(query, self.engine, chunksize=chunk_size), 1):\n",
    "            self.logger.debug(f\"Processing chunk {chunk_num}: {len(chunk):,} rows\")\n",
    "            yield chunk\n",
    "    \n",
    "    def write_dataframe(self, df: pd.DataFrame, table: str, \n",
    "                       if_exists: str = 'append', method: str = 'multi',\n",
    "                       chunksize: int = 10000):\n",
    "        \"\"\"Write DataFrame to Redshift table.\"\"\"\n",
    "        self.logger.info(f\"Writing {len(df):,} rows to {table}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            df.to_sql(\n",
    "                table,\n",
    "                self.engine,\n",
    "                if_exists=if_exists,\n",
    "                index=False,\n",
    "                method=method,\n",
    "                chunksize=chunksize\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            self.logger.info(f\"Write completed in {elapsed:.2f}s ({len(df)/elapsed:.0f} rows/s)\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Write failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def execute_query(self, query: str) -> None:\n",
    "        \"\"\"Execute DDL/DML query without returning results.\"\"\"\n",
    "        self.logger.info(\"Executing query\")\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(query))\n",
    "            conn.commit()\n",
    "        \n",
    "        self.logger.info(\"Query executed successfully\")\n",
    "    \n",
    "    def get_table_stats(self, table: str) -> Dict:\n",
    "        \"\"\"Get statistics about a table.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as row_count,\n",
    "            pg_size_pretty(pg_total_relation_size('{table}')) as total_size,\n",
    "            pg_size_pretty(pg_relation_size('{table}')) as table_size\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.read_query(query).to_dict('records')[0]\n",
    "    \n",
    "    def analyze_query(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Get EXPLAIN plan for a query.\"\"\"\n",
    "        explain_query = f\"EXPLAIN {query}\"\n",
    "        return self.read_query(explain_query)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connections.\"\"\"\n",
    "        if self.engine:\n",
    "            self.engine.dispose()\n",
    "            self.logger.info(\"Engine disposed\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.region.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', 5439)),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing'),\n",
    "    'user': os.getenv('REDSHIFT_USER', 'analyst'),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', 'your-password')\n",
    "}\n",
    "\n",
    "# Initialize manager\n",
    "# rs = RedshiftManager(REDSHIFT_CONFIG)\n",
    "\n",
    "print(\"✓ Redshift manager configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory-Optimized Pandas Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Optimize DataFrame memory usage by downcasting dtypes.\"\"\"\n",
    "    \n",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    logger.info(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "    \n",
    "    # Optimize integer columns\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    # Optimize float columns\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    # Convert object columns to category if beneficial\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        num_unique = df[col].nunique()\n",
    "        num_total = len(df)\n",
    "        \n",
    "        if num_unique / num_total < 0.5:  # Less than 50% unique values\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    reduction = (1 - final_memory / initial_memory) * 100\n",
    "    \n",
    "    logger.info(f\"Final memory usage: {final_memory:.2f} MB\")\n",
    "    logger.info(f\"Memory reduction: {reduction:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Analyze memory usage by column.\"\"\"\n",
    "    \n",
    "    memory_usage = df.memory_usage(deep=True)\n",
    "    \n",
    "    analysis = pd.DataFrame({\n",
    "        'column': memory_usage.index,\n",
    "        'dtype': [df[col].dtype if col != 'Index' else 'Index' for col in memory_usage.index],\n",
    "        'memory_mb': memory_usage.values / 1024**2,\n",
    "        'percent': memory_usage.values / memory_usage.sum() * 100\n",
    "    }).sort_values('memory_mb', ascending=False)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Example\n",
    "sample_df = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 100000, 1000000),\n",
    "    'campaign_id': np.random.randint(1, 1000, 1000000),\n",
    "    'channel': np.random.choice(['email', 'social', 'search'], 1000000),\n",
    "    'revenue': np.random.uniform(0, 1000, 1000000)\n",
    "})\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(analyze_dataframe_memory(sample_df).head(10))\n",
    "print()\n",
    "\n",
    "optimized_df = optimize_dataframe_dtypes(sample_df)\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(analyze_dataframe_memory(optimized_df).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunked Processing Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedProcessor:\n",
    "    \"\"\"Process large datasets in chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 100000):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def aggregate_chunks(self, query: str, engine, \n",
    "                        agg_func: callable) -> pd.DataFrame:\n",
    "        \"\"\"Aggregate data from chunks.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Starting chunked aggregation\")\n",
    "        results = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(pd.read_sql(query, engine, chunksize=self.chunk_size), 1):\n",
    "            chunk_result = agg_func(chunk)\n",
    "            results.append(chunk_result)\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            if chunk_num % 10 == 0:\n",
    "                self.logger.info(f\"Processed {total_rows:,} rows\")\n",
    "                gc.collect()\n",
    "        \n",
    "        # Combine results\n",
    "        final_result = pd.concat(results).groupby(level=0).sum()\n",
    "        \n",
    "        self.logger.info(f\"Aggregation complete: {total_rows:,} rows processed\")\n",
    "        return final_result\n",
    "    \n",
    "    def filter_and_transform(self, query: str, engine,\n",
    "                            filter_func: callable,\n",
    "                            transform_func: callable) -> pd.DataFrame:\n",
    "        \"\"\"Filter and transform data in chunks.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Starting chunked filter and transform\")\n",
    "        results = []\n",
    "        \n",
    "        for chunk in pd.read_sql(query, engine, chunksize=self.chunk_size):\n",
    "            # Filter\n",
    "            filtered = chunk[filter_func(chunk)]\n",
    "            \n",
    "            if len(filtered) > 0:\n",
    "                # Transform\n",
    "                transformed = transform_func(filtered)\n",
    "                results.append(transformed)\n",
    "        \n",
    "        if results:\n",
    "            return pd.concat(results, ignore_index=True)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def write_chunks_to_redshift(self, df: pd.DataFrame, table: str,\n",
    "                                 engine, chunk_size: int = 10000):\n",
    "        \"\"\"Write large DataFrame to Redshift in chunks.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Writing {len(df):,} rows in chunks of {chunk_size:,}\")\n",
    "        \n",
    "        total_chunks = (len(df) - 1) // chunk_size + 1\n",
    "        \n",
    "        for i in range(total_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, len(df))\n",
    "            chunk = df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            if_exists = 'replace' if i == 0 else 'append'\n",
    "            \n",
    "            chunk.to_sql(\n",
    "                table,\n",
    "                engine,\n",
    "                if_exists=if_exists,\n",
    "                index=False,\n",
    "                method='multi'\n",
    "            )\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                self.logger.info(f\"Written {end_idx:,} rows\")\n",
    "        \n",
    "        self.logger.info(\"Write complete\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "processor = ChunkedProcessor(chunk_size=100000)\n",
    "\n",
    "# Define aggregation function\n",
    "def campaign_aggregation(chunk):\n",
    "    return chunk.groupby('campaign_id').agg({\n",
    "        'revenue': 'sum',\n",
    "        'user_id': 'nunique',\n",
    "        'event_id': 'count'\n",
    "    })\n",
    "\n",
    "# Execute chunked aggregation\n",
    "# query = \"SELECT * FROM marketing_events WHERE date >= '2024-01-01'\"\n",
    "# results = processor.aggregate_chunks(query, rs.engine, campaign_aggregation)\n",
    "\n",
    "print(\"✓ Chunked processor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Optimization for Large Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryOptimizer:\n",
    "    \"\"\"Optimize queries for Redshift.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_limit_offset_pagination(base_query: str, page_size: int, page: int) -> str:\n",
    "        \"\"\"Add pagination to query.\"\"\"\n",
    "        offset = page * page_size\n",
    "        return f\"{base_query} LIMIT {page_size} OFFSET {offset}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def optimize_column_selection(table: str, required_columns: List[str]) -> str:\n",
    "        \"\"\"Select only required columns.\"\"\"\n",
    "        cols = ', '.join(required_columns)\n",
    "        return f\"SELECT {cols} FROM {table}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_date_partition(query: str, date_column: str,\n",
    "                          start_date: str, end_date: str) -> str:\n",
    "        \"\"\"Add date partitioning for efficient scanning.\"\"\"\n",
    "        return f\"\"\"\n",
    "        {query}\n",
    "        WHERE {date_column} >= '{start_date}'\n",
    "          AND {date_column} < '{end_date}'\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_incremental_load_query(table: str, last_updated_column: str,\n",
    "                                     last_loaded_timestamp: str) -> str:\n",
    "        \"\"\"Create query for incremental loading.\"\"\"\n",
    "        return f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table}\n",
    "        WHERE {last_updated_column} > '{last_loaded_timestamp}'\n",
    "        ORDER BY {last_updated_column}\n",
    "        \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sampling_query(table: str, sample_rate: float) -> str:\n",
    "        \"\"\"Create query to sample data.\"\"\"\n",
    "        return f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table}\n",
    "        WHERE RANDOM() < {sample_rate}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "# Example optimization\n",
    "optimizer = QueryOptimizer()\n",
    "\n",
    "# Instead of SELECT *\n",
    "inefficient_query = \"SELECT * FROM marketing_events\"\n",
    "\n",
    "# Use column selection\n",
    "efficient_query = optimizer.optimize_column_selection(\n",
    "    'marketing_events',\n",
    "    ['event_id', 'user_id', 'campaign_id', 'revenue', 'timestamp']\n",
    ")\n",
    "\n",
    "# Add date partition\n",
    "partitioned_query = optimizer.add_date_partition(\n",
    "    efficient_query,\n",
    "    'timestamp',\n",
    "    '2024-01-01',\n",
    "    '2024-02-01'\n",
    ")\n",
    "\n",
    "print(\"Optimized query:\")\n",
    "print(partitioned_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dask for Out-of-Memory Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask client\n",
    "# client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "# print(client)\n",
    "\n",
    "\n",
    "def process_large_dataset_with_dask(file_pattern: str) -> dd.DataFrame:\n",
    "    \"\"\"Process large CSV files with Dask.\"\"\"\n",
    "    \n",
    "    logger.info(f\"Loading data from {file_pattern}\")\n",
    "    \n",
    "    # Read CSV files (lazy loading)\n",
    "    ddf = dd.read_csv(\n",
    "        file_pattern,\n",
    "        blocksize='64MB',  # Size of each partition\n",
    "        dtype={\n",
    "            'user_id': 'int32',\n",
    "            'campaign_id': 'int16',\n",
    "            'revenue': 'float32'\n",
    "        },\n",
    "        parse_dates=['timestamp']\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Partitions: {ddf.npartitions}\")\n",
    "    \n",
    "    return ddf\n",
    "\n",
    "\n",
    "def dask_aggregation_example(ddf: dd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Perform aggregations with Dask.\"\"\"\n",
    "    \n",
    "    logger.info(\"Computing aggregations\")\n",
    "    \n",
    "    # Perform aggregations (lazy)\n",
    "    result = ddf.groupby('campaign_id').agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'user_id': 'nunique'\n",
    "    })\n",
    "    \n",
    "    # Compute (triggers execution)\n",
    "    result_df = result.compute()\n",
    "    \n",
    "    logger.info(\"Aggregation complete\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def dask_to_redshift(ddf: dd.DataFrame, table: str, engine,\n",
    "                    chunksize: int = 10000):\n",
    "    \"\"\"Write Dask DataFrame to Redshift.\"\"\"\n",
    "    \n",
    "    logger.info(f\"Writing Dask DataFrame to {table}\")\n",
    "    \n",
    "    # Convert to pandas in chunks and write\n",
    "    for i, partition in enumerate(ddf.to_delayed()):\n",
    "        df_partition = partition.compute()\n",
    "        \n",
    "        if_exists = 'replace' if i == 0 else 'append'\n",
    "        \n",
    "        df_partition.to_sql(\n",
    "            table,\n",
    "            engine,\n",
    "            if_exists=if_exists,\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=chunksize\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Written partition {i + 1}\")\n",
    "\n",
    "\n",
    "# Example: Process large file with Dask\n",
    "# ddf = process_large_dataset_with_dask('data/marketing_*.csv')\n",
    "# result = dask_aggregation_example(ddf)\n",
    "\n",
    "print(\"✓ Dask processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(description: str):\n",
    "    \"\"\"Context manager for timing operations.\"\"\"\n",
    "    start = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - start\n",
    "    logger.info(f\"{description}: {elapsed:.2f}s\")\n",
    "\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Benchmark different approaches.\"\"\"\n",
    "    \n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.results = []\n",
    "    \n",
    "    def benchmark_read_methods(self, query: str, n_rows: int):\n",
    "        \"\"\"Compare different read methods.\"\"\"\n",
    "        \n",
    "        # Method 1: Full load\n",
    "        with timer(\"Full load\"):\n",
    "            df1 = pd.read_sql(query, self.engine)\n",
    "            mem1 = df1.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        self.results.append({\n",
    "            'method': 'full_load',\n",
    "            'rows': len(df1),\n",
    "            'memory_mb': mem1\n",
    "        })\n",
    "        \n",
    "        # Method 2: Chunked processing\n",
    "        chunks_processed = 0\n",
    "        with timer(\"Chunked processing\"):\n",
    "            for chunk in pd.read_sql(query, self.engine, chunksize=10000):\n",
    "                chunks_processed += len(chunk)\n",
    "        \n",
    "        self.results.append({\n",
    "            'method': 'chunked',\n",
    "            'rows': chunks_processed,\n",
    "            'memory_mb': 'streaming'\n",
    "        })\n",
    "        \n",
    "        # Method 3: Column optimization\n",
    "        with timer(\"Optimized dtypes\"):\n",
    "            df3 = pd.read_sql(query, self.engine)\n",
    "            df3 = optimize_dataframe_dtypes(df3)\n",
    "            mem3 = df3.memory_usage(deep=True).sum() / 1024**2\n",
    "        \n",
    "        self.results.append({\n",
    "            'method': 'optimized_dtypes',\n",
    "            'rows': len(df3),\n",
    "            'memory_mb': mem3\n",
    "        })\n",
    "        \n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def benchmark_aggregations(self, df: pd.DataFrame):\n",
    "        \"\"\"Benchmark different aggregation methods.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Method 1: Standard groupby\n",
    "        with timer(\"Standard groupby\"):\n",
    "            result1 = df.groupby('campaign_id')['revenue'].sum()\n",
    "        \n",
    "        # Method 2: NumPy for simple aggregations\n",
    "        with timer(\"NumPy-based\"):\n",
    "            unique_campaigns = df['campaign_id'].unique()\n",
    "            result2 = {}\n",
    "            for cid in unique_campaigns:\n",
    "                result2[cid] = df[df['campaign_id'] == cid]['revenue'].sum()\n",
    "        \n",
    "        # Method 3: Dask (if very large)\n",
    "        # with timer(\"Dask groupby\"):\n",
    "        #     ddf = dd.from_pandas(df, npartitions=4)\n",
    "        #     result3 = ddf.groupby('campaign_id')['revenue'].sum().compute()\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Example benchmark\n",
    "# benchmark = PerformanceBenchmark(rs.engine)\n",
    "# query = \"SELECT * FROM marketing_events LIMIT 100000\"\n",
    "# results = benchmark.benchmark_read_methods(query, 100000)\n",
    "# print(results)\n",
    "\n",
    "print(\"✓ Benchmarking tools defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDataPipeline:\n",
    "    \"\"\"Production-ready data pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, rs_manager: RedshiftManager):\n",
    "        self.rs = rs_manager\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def extract_incremental(self, table: str, timestamp_col: str,\n",
    "                          last_loaded: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data incrementally.\"\"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table}\n",
    "        WHERE {timestamp_col} > '{last_loaded}'\n",
    "        ORDER BY {timestamp_col}\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Extracting incremental data since {last_loaded}\")\n",
    "        \n",
    "        df = self.rs.read_query(query)\n",
    "        \n",
    "        self.metrics['extract_rows'] = len(df)\n",
    "        self.metrics['extract_time'] = datetime.now()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply transformations.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Applying transformations\")\n",
    "        \n",
    "        # Optimize dtypes\n",
    "        df = optimize_dataframe_dtypes(df)\n",
    "        \n",
    "        # Data quality checks\n",
    "        initial_rows = len(df)\n",
    "        df = df.dropna(subset=['user_id', 'timestamp'])\n",
    "        df = df[df['revenue'] >= 0]\n",
    "        \n",
    "        rows_removed = initial_rows - len(df)\n",
    "        if rows_removed > 0:\n",
    "            self.logger.warning(f\"Removed {rows_removed} invalid rows\")\n",
    "        \n",
    "        # Add derived columns\n",
    "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "        \n",
    "        self.metrics['transform_rows'] = len(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load(self, df: pd.DataFrame, target_table: str,\n",
    "            load_type: str = 'append'):\n",
    "        \"\"\"Load data to Redshift.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Loading {len(df):,} rows to {target_table}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.rs.write_dataframe(\n",
    "            df,\n",
    "            target_table,\n",
    "            if_exists=load_type,\n",
    "            chunksize=10000\n",
    "        )\n",
    "        \n",
    "        self.metrics['load_rows'] = len(df)\n",
    "        self.metrics['load_time'] = time.time() - start_time\n",
    "        \n",
    "        self.logger.info(f\"Load completed in {self.metrics['load_time']:.2f}s\")\n",
    "    \n",
    "    def run_pipeline(self, source_table: str, target_table: str,\n",
    "                    timestamp_col: str, last_loaded: str):\n",
    "        \"\"\"Run complete ETL pipeline.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Starting ETL pipeline\")\n",
    "        pipeline_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Extract\n",
    "            df = self.extract_incremental(source_table, timestamp_col, last_loaded)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                self.logger.info(\"No new data to process\")\n",
    "                return\n",
    "            \n",
    "            # Transform\n",
    "            df = self.transform(df)\n",
    "            \n",
    "            # Load\n",
    "            self.load(df, target_table)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.metrics['pipeline_time'] = time.time() - pipeline_start\n",
    "            self.metrics['status'] = 'success'\n",
    "            \n",
    "            self.logger.info(f\"Pipeline completed in {self.metrics['pipeline_time']:.2f}s\")\n",
    "            self.logger.info(f\"Metrics: {self.metrics}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {e}\")\n",
    "            self.metrics['status'] = 'failed'\n",
    "            self.metrics['error'] = str(e)\n",
    "            raise\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# pipeline = ProductionDataPipeline(rs)\n",
    "# pipeline.run_pipeline(\n",
    "#     source_table='raw_events',\n",
    "#     target_table='processed_events',\n",
    "#     timestamp_col='created_at',\n",
    "#     last_loaded='2024-01-01 00:00:00'\n",
    "# )\n",
    "\n",
    "print(\"✓ Production pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Project: Analyze 100M Row Marketing Dataset\n",
    "\n",
    "### Project: Multi-Channel Attribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingAttributionAnalyzer:\n",
    "    \"\"\"Analyze large-scale marketing attribution data.\"\"\"\n",
    "    \n",
    "    def __init__(self, rs_manager: RedshiftManager):\n",
    "        self.rs = rs_manager\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def get_user_journeys(self, start_date: str, end_date: str,\n",
    "                         chunk_size: int = 100000) -> pd.DataFrame:\n",
    "        \"\"\"Extract user journeys using chunked processing.\"\"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            campaign_id,\n",
    "            channel,\n",
    "            event_type,\n",
    "            revenue,\n",
    "            timestamp\n",
    "        FROM marketing_events\n",
    "        WHERE date >= '{start_date}'\n",
    "          AND date < '{end_date}'\n",
    "        ORDER BY user_id, timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Extracting journeys for {start_date} to {end_date}\")\n",
    "        \n",
    "        # Process in chunks\n",
    "        journey_stats = []\n",
    "        \n",
    "        for chunk in pd.read_sql(query, self.rs.engine, chunksize=chunk_size):\n",
    "            # Analyze chunk\n",
    "            chunk_stats = self._analyze_journey_chunk(chunk)\n",
    "            journey_stats.append(chunk_stats)\n",
    "        \n",
    "        # Combine results\n",
    "        final_stats = pd.concat(journey_stats).groupby('channel').sum()\n",
    "        \n",
    "        return final_stats\n",
    "    \n",
    "    def _analyze_journey_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Analyze a chunk of journey data.\"\"\"\n",
    "        \n",
    "        # Calculate metrics by channel\n",
    "        stats = chunk.groupby('channel').agg({\n",
    "            'user_id': 'nunique',\n",
    "            'event_type': 'count',\n",
    "            'revenue': 'sum'\n",
    "        }).rename(columns={\n",
    "            'user_id': 'unique_users',\n",
    "            'event_type': 'total_events',\n",
    "            'revenue': 'total_revenue'\n",
    "        })\n",
    "        \n",
    "        # Count conversions\n",
    "        conversions = chunk[chunk['event_type'] == 'conversion'].groupby('channel').size()\n",
    "        stats['conversions'] = conversions\n",
    "        stats['conversions'] = stats['conversions'].fillna(0)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def calculate_attribution(self, start_date: str, end_date: str,\n",
    "                            model: str = 'last_touch') -> pd.DataFrame:\n",
    "        \"\"\"Calculate attribution using specified model.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Calculating {model} attribution\")\n",
    "        \n",
    "        if model == 'last_touch':\n",
    "            return self._last_touch_attribution(start_date, end_date)\n",
    "        elif model == 'first_touch':\n",
    "            return self._first_touch_attribution(start_date, end_date)\n",
    "        elif model == 'linear':\n",
    "            return self._linear_attribution(start_date, end_date)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model}\")\n",
    "    \n",
    "    def _last_touch_attribution(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Last-touch attribution model.\"\"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        WITH conversions AS (\n",
    "            SELECT \n",
    "                user_id,\n",
    "                revenue,\n",
    "                timestamp as conversion_time\n",
    "            FROM marketing_events\n",
    "            WHERE event_type = 'conversion'\n",
    "              AND date >= '{start_date}'\n",
    "              AND date < '{end_date}'\n",
    "        ),\n",
    "        last_touch AS (\n",
    "            SELECT DISTINCT\n",
    "                e.user_id,\n",
    "                LAST_VALUE(e.channel) OVER (\n",
    "                    PARTITION BY e.user_id \n",
    "                    ORDER BY e.timestamp\n",
    "                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "                ) as last_channel,\n",
    "                c.revenue\n",
    "            FROM marketing_events e\n",
    "            INNER JOIN conversions c ON e.user_id = c.user_id\n",
    "            WHERE e.timestamp <= c.conversion_time\n",
    "        )\n",
    "        SELECT \n",
    "            last_channel as channel,\n",
    "            COUNT(*) as attributed_conversions,\n",
    "            SUM(revenue) as attributed_revenue\n",
    "        FROM last_touch\n",
    "        GROUP BY last_channel\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.rs.read_query(query)\n",
    "    \n",
    "    def generate_performance_report(self, start_date: str, end_date: str) -> Dict:\n",
    "        \"\"\"Generate comprehensive performance report.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Generating performance report\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_events,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            SUM(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) as conversions,\n",
    "            SUM(revenue) as total_revenue,\n",
    "            AVG(revenue) as avg_revenue\n",
    "        FROM marketing_events\n",
    "        WHERE date >= '{start_date}'\n",
    "          AND date < '{end_date}'\n",
    "        \"\"\"\n",
    "        \n",
    "        overall = self.rs.read_query(overall_query).to_dict('records')[0]\n",
    "        \n",
    "        # Channel performance\n",
    "        channel_query = f\"\"\"\n",
    "        SELECT \n",
    "            channel,\n",
    "            COUNT(*) as events,\n",
    "            COUNT(DISTINCT user_id) as users,\n",
    "            SUM(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) as conversions,\n",
    "            SUM(revenue) as revenue\n",
    "        FROM marketing_events\n",
    "        WHERE date >= '{start_date}'\n",
    "          AND date < '{end_date}'\n",
    "        GROUP BY channel\n",
    "        \"\"\"\n",
    "        \n",
    "        channel_perf = self.rs.read_query(channel_query)\n",
    "        \n",
    "        # Attribution\n",
    "        attribution = self.calculate_attribution(start_date, end_date, 'last_touch')\n",
    "        \n",
    "        return {\n",
    "            'overall_metrics': overall,\n",
    "            'channel_performance': channel_perf,\n",
    "            'attribution': attribution,\n",
    "            'date_range': {'start': start_date, 'end': end_date}\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# analyzer = MarketingAttributionAnalyzer(rs)\n",
    "# report = analyzer.generate_performance_report('2024-01-01', '2024-02-01')\n",
    "# print(report['overall_metrics'])\n",
    "# print(report['channel_performance'])\n",
    "\n",
    "print(\"✓ Attribution analyzer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices Summary\n",
    "\n",
    "### Memory Optimization\n",
    "1. Use appropriate dtypes (downcast integers and floats)\n",
    "2. Convert low-cardinality objects to categories\n",
    "3. Process data in chunks for large datasets\n",
    "4. Use Dask for truly out-of-memory datasets\n",
    "5. Monitor memory usage with `memory_profiler`\n",
    "\n",
    "### Query Optimization\n",
    "1. Select only required columns (avoid SELECT *)\n",
    "2. Use date partitioning for large tables\n",
    "3. Implement incremental loading\n",
    "4. Use EXPLAIN to analyze query plans\n",
    "5. Leverage distribution and sort keys\n",
    "\n",
    "### Redshift Best Practices\n",
    "1. Use connection pooling\n",
    "2. Read data in chunks with `chunksize`\n",
    "3. Use COPY command for bulk loads\n",
    "4. Vacuum and analyze tables regularly\n",
    "5. Monitor query performance with system tables\n",
    "\n",
    "### Performance\n",
    "1. Benchmark different approaches\n",
    "2. Use vectorized operations\n",
    "3. Avoid loops when possible\n",
    "4. Use appropriate aggregation methods\n",
    "5. Profile code to identify bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercises\n",
    "\n",
    "### Exercise 1: Memory Optimization\n",
    "Download the \"Online Retail\" dataset from Kaggle and:\n",
    "1. Load the data and measure initial memory usage\n",
    "2. Optimize dtypes to reduce memory by at least 50%\n",
    "3. Compare processing time before and after optimization\n",
    "\n",
    "### Exercise 2: Chunked Processing\n",
    "1. Create a large CSV file (10M+ rows)\n",
    "2. Process it in chunks to calculate aggregated metrics\n",
    "3. Compare memory usage with full-load approach\n",
    "4. Measure processing time and throughput\n",
    "\n",
    "### Exercise 3: Dask Integration\n",
    "1. Load a dataset too large for memory using Dask\n",
    "2. Perform complex aggregations\n",
    "3. Write results back to disk in partitioned format\n",
    "4. Compare performance with pandas\n",
    "\n",
    "### Exercise 4: Production Pipeline\n",
    "Build an end-to-end ETL pipeline that:\n",
    "1. Extracts data incrementally from Redshift\n",
    "2. Applies transformations and data quality checks\n",
    "3. Loads results back to Redshift\n",
    "4. Logs metrics and handles errors gracefully\n",
    "5. Runs on a schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "### Documentation\n",
    "- [Pandas Performance](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)\n",
    "- [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html)\n",
    "- [Redshift Best Practices](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)\n",
    "- [SQLAlchemy Core](https://docs.sqlalchemy.org/en/14/core/)\n",
    "\n",
    "### Kaggle Datasets\n",
    "- Online Retail Dataset\n",
    "- Marketing Analytics\n",
    "- E-commerce Data\n",
    "- Customer Transactions\n",
    "\n",
    "### Tools\n",
    "- memory_profiler: Memory usage profiling\n",
    "- Dask: Parallel computing library\n",
    "- AWS Redshift: Cloud data warehouse\n",
    "- psycopg2: PostgreSQL adapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
