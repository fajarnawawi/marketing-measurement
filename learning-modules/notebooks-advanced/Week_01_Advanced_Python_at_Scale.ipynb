{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Advanced Python at Scale\n",
    "\n",
    "## Learning Objectives\n",
    "- Work with large datasets (millions of rows) efficiently\n",
    "- Master memory-efficient techniques (generators, chunking)\n",
    "- Implement parallel processing with multiprocessing\n",
    "- Connect to AWS Redshift from Python\n",
    "- Optimize Python code for production performance\n",
    "- Apply production-ready code patterns\n",
    "- Leverage advanced data structures\n",
    "- Implement robust error handling and logging\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install psycopg2-binary sqlalchemy pandas numpy boto3 memory_profiler\n",
    "pip install pyarrow fastparquet dask[complete]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter, deque\n",
    "from itertools import islice, groupby, chain\n",
    "from functools import lru_cache, partial\n",
    "from typing import Iterator, Generator, List, Dict, Tuple\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import gc\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Database connectivity\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "\n",
    "# Performance monitoring\n",
    "from memory_profiler import profile\n",
    "import psutil\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('advanced_python.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"CPU cores: {mp.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Connection Setup\n",
    "\n",
    "### Production-Ready Connection Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftConnection:\n",
    "    \"\"\"Production-ready Redshift connection manager with pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, str]):\n",
    "        self.config = config\n",
    "        self.connection_pool = None\n",
    "        self.engine = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "    def create_connection_pool(self, min_conn: int = 1, max_conn: int = 10):\n",
    "        \"\"\"Create a connection pool for efficient connection management.\"\"\"\n",
    "        try:\n",
    "            self.connection_pool = psycopg2.pool.ThreadedConnectionPool(\n",
    "                min_conn,\n",
    "                max_conn,\n",
    "                host=self.config['host'],\n",
    "                port=self.config.get('port', 5439),\n",
    "                database=self.config['database'],\n",
    "                user=self.config['user'],\n",
    "                password=self.config['password'],\n",
    "                connect_timeout=10,\n",
    "                options='-c statement_timeout=300000'  # 5 minute timeout\n",
    "            )\n",
    "            self.logger.info(f\"Connection pool created: {min_conn}-{max_conn} connections\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create connection pool: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_sqlalchemy_engine(self):\n",
    "        \"\"\"Create SQLAlchemy engine for pandas integration.\"\"\"\n",
    "        connection_string = (\n",
    "            f\"postgresql+psycopg2://{self.config['user']}:{self.config['password']}\"\n",
    "            f\"@{self.config['host']}:{self.config.get('port', 5439)}/{self.config['database']}\"\n",
    "        )\n",
    "        self.engine = create_engine(\n",
    "            connection_string,\n",
    "            pool_size=10,\n",
    "            max_overflow=20,\n",
    "            pool_pre_ping=True,  # Verify connections before using\n",
    "            pool_recycle=3600,   # Recycle connections after 1 hour\n",
    "            echo=False\n",
    "        )\n",
    "        self.logger.info(\"SQLAlchemy engine created\")\n",
    "        return self.engine\n",
    "    \n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a connection from the pool.\"\"\"\n",
    "        if not self.connection_pool:\n",
    "            self.create_connection_pool()\n",
    "        return self.connection_pool.getconn()\n",
    "    \n",
    "    def return_connection(self, conn):\n",
    "        \"\"\"Return a connection to the pool.\"\"\"\n",
    "        if self.connection_pool:\n",
    "            self.connection_pool.putconn(conn)\n",
    "    \n",
    "    def close_all(self):\n",
    "        \"\"\"Close all connections.\"\"\"\n",
    "        if self.connection_pool:\n",
    "            self.connection_pool.closeall()\n",
    "            self.logger.info(\"All connections closed\")\n",
    "        if self.engine:\n",
    "            self.engine.dispose()\n",
    "\n",
    "\n",
    "# Example configuration (use environment variables in production)\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.region.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', 5439)),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing'),\n",
    "    'user': os.getenv('REDSHIFT_USER', 'analyst'),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', 'your-password')\n",
    "}\n",
    "\n",
    "# Initialize connection manager\n",
    "# rs_conn = RedshiftConnection(REDSHIFT_CONFIG)\n",
    "# engine = rs_conn.create_sqlalchemy_engine()\n",
    "\n",
    "print(\"✓ Connection manager configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory-Efficient Data Processing\n",
    "\n",
    "### 3.1 Generators for Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_file_reader(file_path: str, chunk_size: int = 100000) -> Generator:\n",
    "    \"\"\"Read large CSV files in chunks using generators.\"\"\"\n",
    "    logger.info(f\"Reading file in chunks of {chunk_size} rows\")\n",
    "    \n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def chunked_redshift_reader(query: str, engine, chunk_size: int = 50000) -> Generator:\n",
    "    \"\"\"Read large Redshift query results in chunks.\"\"\"\n",
    "    logger.info(f\"Executing query with chunk size {chunk_size}\")\n",
    "    \n",
    "    for chunk in pd.read_sql(query, engine, chunksize=chunk_size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Process a single chunk and return aggregated results.\"\"\"\n",
    "    return {\n",
    "        'rows': len(chunk),\n",
    "        'revenue': chunk.get('revenue', pd.Series([0])).sum(),\n",
    "        'unique_users': chunk.get('user_id', pd.Series()).nunique()\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Process large file without loading into memory\n",
    "def process_large_file_streaming(file_path: str) -> Dict:\n",
    "    \"\"\"Process large file using streaming approach.\"\"\"\n",
    "    results = {\n",
    "        'total_rows': 0,\n",
    "        'total_revenue': 0,\n",
    "        'unique_users': set()\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(chunked_file_reader(file_path, chunk_size=100000), 1):\n",
    "        results['total_rows'] += len(chunk)\n",
    "        results['total_revenue'] += chunk.get('revenue', 0).sum()\n",
    "        results['unique_users'].update(chunk.get('user_id', pd.Series()).unique())\n",
    "        \n",
    "        if chunk_num % 10 == 0:\n",
    "            logger.info(f\"Processed {chunk_num} chunks, {results['total_rows']:,} rows\")\n",
    "            gc.collect()  # Force garbage collection\n",
    "    \n",
    "    results['unique_users'] = len(results['unique_users'])\n",
    "    results['processing_time'] = time.time() - start_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Streaming processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    return {\n",
    "        'rss_mb': mem_info.rss / 1024 / 1024,\n",
    "        'vms_mb': mem_info.vms / 1024 / 1024,\n",
    "        'percent': process.memory_percent()\n",
    "    }\n",
    "\n",
    "\n",
    "def memory_efficient_dataframe(size: int = 1000000):\n",
    "    \"\"\"Create memory-efficient DataFrames using appropriate dtypes.\"\"\"\n",
    "    \n",
    "    # Inefficient approach\n",
    "    print(\"Creating inefficient DataFrame...\")\n",
    "    mem_before = get_memory_usage()['rss_mb']\n",
    "    \n",
    "    df_inefficient = pd.DataFrame({\n",
    "        'user_id': np.random.randint(1, 100000, size),\n",
    "        'campaign_id': np.random.randint(1, 1000, size),\n",
    "        'revenue': np.random.uniform(0, 1000, size),\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=size, freq='1s')\n",
    "    })\n",
    "    \n",
    "    mem_after_inefficient = get_memory_usage()['rss_mb']\n",
    "    print(f\"Inefficient DataFrame memory: {df_inefficient.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Process memory increased by: {mem_after_inefficient - mem_before:.2f} MB\\n\")\n",
    "    \n",
    "    # Efficient approach\n",
    "    print(\"Creating efficient DataFrame...\")\n",
    "    mem_before = get_memory_usage()['rss_mb']\n",
    "    \n",
    "    df_efficient = pd.DataFrame({\n",
    "        'user_id': np.random.randint(1, 100000, size, dtype=np.int32),\n",
    "        'campaign_id': np.random.randint(1, 1000, size, dtype=np.int16),\n",
    "        'revenue': np.random.uniform(0, 1000, size).astype(np.float32),\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=size, freq='1s')\n",
    "    })\n",
    "    \n",
    "    mem_after_efficient = get_memory_usage()['rss_mb']\n",
    "    print(f\"Efficient DataFrame memory: {df_efficient.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Process memory increased by: {mem_after_efficient - mem_before:.2f} MB\\n\")\n",
    "    \n",
    "    print(f\"Memory savings: {(df_inefficient.memory_usage(deep=True).sum() - df_efficient.memory_usage(deep=True).sum()) / df_inefficient.memory_usage(deep=True).sum() * 100:.1f}%\")\n",
    "    \n",
    "    return df_efficient\n",
    "\n",
    "\n",
    "# Test memory optimization\n",
    "# df = memory_efficient_dataframe(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parallel Processing\n",
    "\n",
    "### 4.1 Multiprocessing for CPU-Intensive Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(data_chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process a partition of data (CPU-intensive operations).\"\"\"\n",
    "    # Simulate complex processing\n",
    "    result = data_chunk.copy()\n",
    "    \n",
    "    # Calculate rolling metrics\n",
    "    result['revenue_ma_7d'] = result.groupby('user_id')['revenue'].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Calculate user segments\n",
    "    result['user_segment'] = pd.cut(\n",
    "        result['revenue'],\n",
    "        bins=[0, 100, 500, 1000, float('inf')],\n",
    "        labels=['low', 'medium', 'high', 'vip']\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def parallel_process_dataframe(df: pd.DataFrame, n_workers: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Process DataFrame in parallel using multiprocessing.\"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, mp.cpu_count() - 1)\n",
    "    \n",
    "    logger.info(f\"Processing with {n_workers} workers\")\n",
    "    \n",
    "    # Split dataframe into chunks\n",
    "    chunks = np.array_split(df, n_workers)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process in parallel\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        results = list(executor.map(process_partition, chunks))\n",
    "    \n",
    "    # Combine results\n",
    "    final_result = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Parallel processing completed in {elapsed:.2f}s\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "\n",
    "# Example: Compare serial vs parallel processing\n",
    "def benchmark_parallel_processing(size: int = 1000000):\n",
    "    \"\"\"Benchmark serial vs parallel processing.\"\"\"\n",
    "    # Generate test data\n",
    "    df = pd.DataFrame({\n",
    "        'user_id': np.random.randint(1, 10000, size),\n",
    "        'revenue': np.random.uniform(0, 1000, size),\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=size, freq='1min')\n",
    "    })\n",
    "    \n",
    "    print(f\"Dataset: {len(df):,} rows\\n\")\n",
    "    \n",
    "    # Serial processing\n",
    "    print(\"Serial processing...\")\n",
    "    start = time.time()\n",
    "    result_serial = process_partition(df)\n",
    "    serial_time = time.time() - start\n",
    "    print(f\"Time: {serial_time:.2f}s\\n\")\n",
    "    \n",
    "    # Parallel processing\n",
    "    print(\"Parallel processing...\")\n",
    "    start = time.time()\n",
    "    result_parallel = parallel_process_dataframe(df, n_workers=4)\n",
    "    parallel_time = time.time() - start\n",
    "    print(f\"Time: {parallel_time:.2f}s\\n\")\n",
    "    \n",
    "    speedup = serial_time / parallel_time\n",
    "    print(f\"Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    return result_parallel\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "# result = benchmark_parallel_processing(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Thread Pool for I/O-Bound Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_campaign_data(campaign_id: int, engine) -> pd.DataFrame:\n",
    "    \"\"\"Fetch data for a single campaign (I/O-bound).\"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM marketing_events\n",
    "    WHERE campaign_id = {campaign_id}\n",
    "    LIMIT 10000\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "def parallel_fetch_campaigns(campaign_ids: List[int], engine, max_workers: int = 10) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"Fetch multiple campaigns in parallel using thread pool.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_campaign = {\n",
    "            executor.submit(fetch_campaign_data, cid, engine): cid \n",
    "            for cid in campaign_ids\n",
    "        }\n",
    "        \n",
    "        # Collect results\n",
    "        for future in future_to_campaign:\n",
    "            campaign_id = future_to_campaign[future]\n",
    "            try:\n",
    "                results[campaign_id] = future.result(timeout=30)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Campaign {campaign_id} failed: {e}\")\n",
    "                results[campaign_id] = pd.DataFrame()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Fetched {len(campaign_ids)} campaigns in {elapsed:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✓ Parallel I/O functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Data Structures\n",
    "\n",
    "### 5.1 Collections for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, deque, namedtuple\n",
    "from itertools import groupby, islice\n",
    "\n",
    "# Named tuples for clarity and memory efficiency\n",
    "Event = namedtuple('Event', ['user_id', 'event_type', 'timestamp', 'value'])\n",
    "\n",
    "\n",
    "def analyze_user_journey(events: List[Event]) -> Dict:\n",
    "    \"\"\"Analyze user journey using advanced data structures.\"\"\"\n",
    "    \n",
    "    # defaultdict for automatic initialization\n",
    "    user_paths = defaultdict(list)\n",
    "    event_counts = Counter()\n",
    "    \n",
    "    # Group events by user\n",
    "    events_sorted = sorted(events, key=lambda x: (x.user_id, x.timestamp))\n",
    "    \n",
    "    for user_id, user_events in groupby(events_sorted, key=lambda x: x.user_id):\n",
    "        user_events_list = list(user_events)\n",
    "        \n",
    "        # Store journey path\n",
    "        user_paths[user_id] = [e.event_type for e in user_events_list]\n",
    "        \n",
    "        # Count event types\n",
    "        event_counts.update([e.event_type for e in user_events_list])\n",
    "    \n",
    "    return {\n",
    "        'total_users': len(user_paths),\n",
    "        'avg_journey_length': np.mean([len(p) for p in user_paths.values()]),\n",
    "        'most_common_events': event_counts.most_common(5),\n",
    "        'sample_journey': list(islice(user_paths.items(), 3))\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Generate sample events\n",
    "def generate_sample_events(n: int = 100000) -> List[Event]:\n",
    "    \"\"\"Generate sample events for testing.\"\"\"\n",
    "    event_types = ['page_view', 'click', 'conversion', 'cart_add', 'purchase']\n",
    "    \n",
    "    events = [\n",
    "        Event(\n",
    "            user_id=np.random.randint(1, 10000),\n",
    "            event_type=np.random.choice(event_types),\n",
    "            timestamp=datetime.now() - timedelta(seconds=np.random.randint(0, 86400)),\n",
    "            value=np.random.uniform(0, 100)\n",
    "        )\n",
    "        for _ in range(n)\n",
    "    ]\n",
    "    \n",
    "    return events\n",
    "\n",
    "\n",
    "# Test\n",
    "events = generate_sample_events(10000)\n",
    "analysis = analyze_user_journey(events)\n",
    "print(f\"Analyzed {analysis['total_users']} users\")\n",
    "print(f\"Average journey length: {analysis['avg_journey_length']:.1f}\")\n",
    "print(f\"Most common events: {analysis['most_common_events']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LRU Cache for Expensive Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_user_segment(user_id: int, revenue: float) -> str:\n",
    "    \"\"\"Expensive user segmentation logic (cached).\"\"\"\n",
    "    # Simulate expensive computation\n",
    "    time.sleep(0.001)\n",
    "    \n",
    "    if revenue < 100:\n",
    "        return 'low'\n",
    "    elif revenue < 500:\n",
    "        return 'medium'\n",
    "    elif revenue < 1000:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'vip'\n",
    "\n",
    "\n",
    "def benchmark_caching(n_calls: int = 10000):\n",
    "    \"\"\"Benchmark caching performance.\"\"\"\n",
    "    user_ids = np.random.randint(1, 100, n_calls)  # Limited user IDs for cache hits\n",
    "    revenues = np.random.uniform(0, 1000, n_calls)\n",
    "    \n",
    "    # With cache\n",
    "    get_user_segment.cache_clear()\n",
    "    start = time.time()\n",
    "    results = [get_user_segment(uid, rev) for uid, rev in zip(user_ids, revenues)]\n",
    "    cached_time = time.time() - start\n",
    "    \n",
    "    cache_info = get_user_segment.cache_info()\n",
    "    print(f\"Cached execution time: {cached_time:.3f}s\")\n",
    "    print(f\"Cache hits: {cache_info.hits}, misses: {cache_info.misses}\")\n",
    "    print(f\"Hit rate: {cache_info.hits / (cache_info.hits + cache_info.misses) * 100:.1f}%\")\n",
    "\n",
    "\n",
    "benchmark_caching(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production-Ready Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "class DataProcessingError(Exception):\n",
    "    \"\"\"Custom exception for data processing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def error_handler(operation_name: str, raise_on_error: bool = False):\n",
    "    \"\"\"Context manager for consistent error handling.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting: {operation_name}\")\n",
    "        yield\n",
    "        logger.info(f\"Completed: {operation_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in {operation_name}: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        if raise_on_error:\n",
    "            raise\n",
    "\n",
    "\n",
    "def retry_with_backoff(func, max_retries: int = 3, backoff_factor: int = 2):\n",
    "    \"\"\"Retry function with exponential backoff.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"Failed after {max_retries} attempts\")\n",
    "                raise\n",
    "            \n",
    "            wait_time = backoff_factor ** attempt\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s: {e}\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "\n",
    "def safe_divide(a: float, b: float, default: float = 0.0) -> float:\n",
    "    \"\"\"Safely divide two numbers.\"\"\"\n",
    "    try:\n",
    "        return a / b if b != 0 else default\n",
    "    except (TypeError, ZeroDivisionError) as e:\n",
    "        logger.warning(f\"Division error: {e}, returning {default}\")\n",
    "        return default\n",
    "\n",
    "\n",
    "# Example usage\n",
    "with error_handler(\"Data processing\"):\n",
    "    # Your code here\n",
    "    result = safe_divide(100, 0)\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "def benchmark_operations():\n",
    "    \"\"\"Benchmark different approaches to common operations.\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    data = list(range(1000000))\n",
    "    \n",
    "    print(\"Benchmarking list operations...\\n\")\n",
    "    \n",
    "    # List comprehension vs map\n",
    "    time_listcomp = timeit.timeit(\n",
    "        lambda: [x * 2 for x in range(100000)],\n",
    "        number=10\n",
    "    )\n",
    "    \n",
    "    time_map = timeit.timeit(\n",
    "        lambda: list(map(lambda x: x * 2, range(100000))),\n",
    "        number=10\n",
    "    )\n",
    "    \n",
    "    print(f\"List comprehension: {time_listcomp:.4f}s\")\n",
    "    print(f\"Map function: {time_map:.4f}s\")\n",
    "    print(f\"Winner: {'List comprehension' if time_listcomp < time_map else 'Map'}\\n\")\n",
    "    \n",
    "    # NumPy vectorization vs list comprehension\n",
    "    arr = np.arange(1000000)\n",
    "    \n",
    "    time_numpy = timeit.timeit(\n",
    "        lambda: arr * 2,\n",
    "        number=10\n",
    "    )\n",
    "    \n",
    "    time_list = timeit.timeit(\n",
    "        lambda: [x * 2 for x in range(1000000)],\n",
    "        number=10\n",
    "    )\n",
    "    \n",
    "    print(f\"NumPy vectorization: {time_numpy:.4f}s\")\n",
    "    print(f\"List comprehension: {time_list:.4f}s\")\n",
    "    print(f\"NumPy speedup: {time_list / time_numpy:.1f}x\")\n",
    "\n",
    "\n",
    "benchmark_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Project: Process 10M Row Dataset\n",
    "\n",
    "### Project: Marketing Campaign Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingDataProcessor:\n",
    "    \"\"\"Production-ready marketing data processor.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 100000, n_workers: int = None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.n_workers = n_workers or max(1, mp.cpu_count() - 1)\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.metrics = defaultdict(float)\n",
    "    \n",
    "    def generate_sample_data(self, output_file: str, n_rows: int = 10000000):\n",
    "        \"\"\"Generate sample marketing data.\"\"\"\n",
    "        self.logger.info(f\"Generating {n_rows:,} rows of sample data\")\n",
    "        \n",
    "        chunks_to_write = n_rows // self.chunk_size\n",
    "        \n",
    "        for i in range(chunks_to_write):\n",
    "            chunk = pd.DataFrame({\n",
    "                'event_id': range(i * self.chunk_size, (i + 1) * self.chunk_size),\n",
    "                'user_id': np.random.randint(1, 1000000, self.chunk_size),\n",
    "                'campaign_id': np.random.randint(1, 10000, self.chunk_size),\n",
    "                'channel': np.random.choice(['email', 'social', 'search', 'display'], self.chunk_size),\n",
    "                'event_type': np.random.choice(['impression', 'click', 'conversion'], self.chunk_size, p=[0.8, 0.15, 0.05]),\n",
    "                'revenue': np.random.exponential(50, self.chunk_size),\n",
    "                'timestamp': pd.date_range('2024-01-01', periods=self.chunk_size, freq='1s')\n",
    "            })\n",
    "            \n",
    "            mode = 'w' if i == 0 else 'a'\n",
    "            header = i == 0\n",
    "            chunk.to_csv(output_file, mode=mode, header=header, index=False)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                self.logger.info(f\"Written {(i + 1) * self.chunk_size:,} rows\")\n",
    "        \n",
    "        self.logger.info(f\"Data generation complete: {output_file}\")\n",
    "    \n",
    "    def process_chunk_metrics(self, chunk: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate metrics for a chunk.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Campaign metrics\n",
    "        campaign_stats = chunk.groupby('campaign_id').agg({\n",
    "            'event_id': 'count',\n",
    "            'revenue': 'sum',\n",
    "            'user_id': 'nunique'\n",
    "        }).rename(columns={\n",
    "            'event_id': 'impressions',\n",
    "            'revenue': 'revenue',\n",
    "            'user_id': 'unique_users'\n",
    "        })\n",
    "        \n",
    "        # Channel performance\n",
    "        channel_stats = chunk.groupby('channel').agg({\n",
    "            'revenue': 'sum',\n",
    "            'event_id': 'count'\n",
    "        })\n",
    "        \n",
    "        # Conversion metrics\n",
    "        conversions = chunk[chunk['event_type'] == 'conversion']\n",
    "        \n",
    "        return {\n",
    "            'campaign_stats': campaign_stats,\n",
    "            'channel_stats': channel_stats,\n",
    "            'total_revenue': chunk['revenue'].sum(),\n",
    "            'total_events': len(chunk),\n",
    "            'conversions': len(conversions)\n",
    "        }\n",
    "    \n",
    "    def process_file_streaming(self, input_file: str) -> Dict:\n",
    "        \"\"\"Process large file using streaming.\"\"\"\n",
    "        self.logger.info(f\"Processing file: {input_file}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Accumulated results\n",
    "        campaign_stats_list = []\n",
    "        channel_stats_list = []\n",
    "        total_revenue = 0\n",
    "        total_events = 0\n",
    "        total_conversions = 0\n",
    "        \n",
    "        # Process in chunks\n",
    "        for chunk_num, chunk in enumerate(pd.read_csv(input_file, chunksize=self.chunk_size), 1):\n",
    "            metrics = self.process_chunk_metrics(chunk)\n",
    "            \n",
    "            campaign_stats_list.append(metrics['campaign_stats'])\n",
    "            channel_stats_list.append(metrics['channel_stats'])\n",
    "            total_revenue += metrics['total_revenue']\n",
    "            total_events += metrics['total_events']\n",
    "            total_conversions += metrics['conversions']\n",
    "            \n",
    "            if chunk_num % 10 == 0:\n",
    "                self.logger.info(f\"Processed {total_events:,} events\")\n",
    "                gc.collect()\n",
    "        \n",
    "        # Combine results\n",
    "        final_campaign_stats = pd.concat(campaign_stats_list).groupby(level=0).sum()\n",
    "        final_channel_stats = pd.concat(channel_stats_list).groupby(level=0).sum()\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'campaign_stats': final_campaign_stats,\n",
    "            'channel_stats': final_channel_stats,\n",
    "            'total_revenue': total_revenue,\n",
    "            'total_events': total_events,\n",
    "            'total_conversions': total_conversions,\n",
    "            'conversion_rate': total_conversions / total_events * 100,\n",
    "            'processing_time': processing_time,\n",
    "            'throughput': total_events / processing_time\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "processor = MarketingDataProcessor(chunk_size=100000)\n",
    "\n",
    "# Generate sample data (smaller for demo)\n",
    "# processor.generate_sample_data('marketing_data_10m.csv', n_rows=1000000)\n",
    "\n",
    "# Process the data\n",
    "# results = processor.process_file_streaming('marketing_data_10m.csv')\n",
    "# print(f\"\\nProcessing Summary:\")\n",
    "# print(f\"Total events: {results['total_events']:,}\")\n",
    "# print(f\"Total revenue: ${results['total_revenue']:,.2f}\")\n",
    "# print(f\"Conversion rate: {results['conversion_rate']:.2f}%\")\n",
    "# print(f\"Processing time: {results['processing_time']:.2f}s\")\n",
    "# print(f\"Throughput: {results['throughput']:,.0f} events/second\")\n",
    "\n",
    "print(\"✓ Marketing data processor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices Summary\n",
    "\n",
    "### Memory Management\n",
    "1. Use appropriate data types (int32 instead of int64)\n",
    "2. Process data in chunks for large datasets\n",
    "3. Use generators instead of lists when possible\n",
    "4. Call `gc.collect()` after processing large chunks\n",
    "5. Use `del` to explicitly free memory\n",
    "\n",
    "### Performance\n",
    "1. Use NumPy vectorization over loops\n",
    "2. Use multiprocessing for CPU-bound tasks\n",
    "3. Use threading for I/O-bound tasks\n",
    "4. Cache expensive function results with `@lru_cache`\n",
    "5. Profile code to identify bottlenecks\n",
    "\n",
    "### Production Patterns\n",
    "1. Implement connection pooling for databases\n",
    "2. Add comprehensive error handling\n",
    "3. Use structured logging\n",
    "4. Implement retry logic with backoff\n",
    "5. Monitor memory and CPU usage\n",
    "6. Use context managers for resource cleanup\n",
    "\n",
    "### Redshift Optimization\n",
    "1. Use appropriate distribution and sort keys\n",
    "2. Fetch data in chunks with `chunksize`\n",
    "3. Use connection pooling\n",
    "4. Set appropriate timeouts\n",
    "5. Use UNLOAD for large exports\n",
    "6. Minimize data transfer with projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Memory-Efficient Processing\n",
    "Download a large CSV from Kaggle (e.g., \"Google Analytics Customer Revenue Prediction\") and:\n",
    "1. Calculate total revenue without loading entire file into memory\n",
    "2. Find top 10 customers by revenue using streaming approach\n",
    "3. Compare memory usage with full-load approach\n",
    "\n",
    "### Exercise 2: Parallel Processing\n",
    "1. Create a CPU-intensive transformation function\n",
    "2. Benchmark serial vs parallel execution\n",
    "3. Find optimal number of workers\n",
    "4. Measure speedup and efficiency\n",
    "\n",
    "### Exercise 3: Production Code\n",
    "Refactor a data processing script to include:\n",
    "1. Proper error handling\n",
    "2. Logging at appropriate levels\n",
    "3. Performance metrics\n",
    "4. Configuration management\n",
    "5. Unit tests\n",
    "\n",
    "### Exercise 4: Redshift Integration\n",
    "1. Create a connection pool manager\n",
    "2. Implement chunked data loading from Redshift\n",
    "3. Add retry logic for transient failures\n",
    "4. Monitor query performance\n",
    "5. Implement efficient data upload to Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "### Documentation\n",
    "- [Python Performance Tips](https://wiki.python.org/moin/PythonSpeed/PerformanceTips)\n",
    "- [Psycopg2 Documentation](https://www.psycopg.org/docs/)\n",
    "- [AWS Redshift Best Practices](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)\n",
    "- [Multiprocessing Guide](https://docs.python.org/3/library/multiprocessing.html)\n",
    "\n",
    "### Kaggle Datasets\n",
    "- Google Analytics Customer Revenue Prediction\n",
    "- Retail Data Analytics\n",
    "- E-commerce Dataset\n",
    "- Marketing Analytics Dataset\n",
    "\n",
    "### Tools\n",
    "- memory_profiler: Profile memory usage\n",
    "- line_profiler: Line-by-line performance profiling\n",
    "- py-spy: Sampling profiler\n",
    "- psutil: System and process utilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
