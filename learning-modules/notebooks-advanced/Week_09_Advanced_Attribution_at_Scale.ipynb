{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Advanced Attribution at Scale\n",
    "\n",
    "## Enterprise-Scale Multi-Touch Attribution (100M+ Touchpoints)\n",
    "\n",
    "### Learning Objectives\n",
    "- Implement attribution models on 100M+ touchpoints\n",
    "- Use efficient graph algorithms for customer journey analysis\n",
    "- Apply Markov chains with sparse matrices\n",
    "- Approximate Shapley values for large datasets\n",
    "- Build distributed attribution computation systems\n",
    "- Create attribution dashboards with Redshift backend\n",
    "\n",
    "### Prerequisites\n",
    "- AWS Redshift cluster access\n",
    "- Python 3.8+\n",
    "- Understanding of attribution modeling concepts\n",
    "- Familiarity with distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install psycopg2-binary pandas numpy scipy scikit-learn \\\n",
    "    networkx matplotlib seaborn plotly dash dask[complete] \\\n",
    "    sqlalchemy pyarrow fastparquet joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, linalg as sparse_linalg\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sqlalchemy import create_engine\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Connection and Data Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftConnection:\n",
    "    \"\"\"\n",
    "    Production-ready Redshift connection manager with connection pooling\n",
    "    and error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host, port, database, user, password):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.engine = None\n",
    "        self.conn = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Redshift\"\"\"\n",
    "        try:\n",
    "            # SQLAlchemy engine for pandas integration\n",
    "            conn_string = f\"postgresql+psycopg2://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "            self.engine = create_engine(conn_string, pool_size=10, max_overflow=20)\n",
    "            \n",
    "            # Direct psycopg2 connection for raw queries\n",
    "            self.conn = psycopg2.connect(\n",
    "                host=self.host,\n",
    "                port=self.port,\n",
    "                database=self.database,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"✓ Connected to Redshift: {self.database}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query, return_df=True):\n",
    "        \"\"\"Execute query and optionally return DataFrame\"\"\"\n",
    "        if return_df:\n",
    "            return pd.read_sql(query, self.engine)\n",
    "        else:\n",
    "            cursor = self.conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            self.conn.commit()\n",
    "            cursor.close()\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close connections\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "        if self.engine:\n",
    "            self.engine.dispose()\n",
    "        print(\"✓ Connections closed\")\n",
    "\n",
    "# Initialize connection (replace with your credentials)\n",
    "rs = RedshiftConnection(\n",
    "    host='your-cluster.region.redshift.amazonaws.com',\n",
    "    port=5439,\n",
    "    database='marketing_db',\n",
    "    user='your_user',\n",
    "    password='your_password'\n",
    ")\n",
    "\n",
    "# Uncomment to connect\n",
    "# rs.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Design for Attribution at Scale\n",
    "\n",
    "### Optimized Redshift Tables for 100M+ Touchpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized Redshift tables for attribution\n",
    "create_tables_sql = \"\"\"\n",
    "-- Touchpoint events table (100M+ rows)\n",
    "-- Distribution key: user_id for efficient joins\n",
    "-- Sort key: timestamp for time-series queries\n",
    "CREATE TABLE IF NOT EXISTS touchpoints (\n",
    "    touchpoint_id BIGINT IDENTITY(1,1),\n",
    "    user_id VARCHAR(64) NOT NULL,\n",
    "    session_id VARCHAR(64),\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    channel VARCHAR(50) NOT NULL,\n",
    "    campaign VARCHAR(100),\n",
    "    device VARCHAR(20),\n",
    "    revenue DECIMAL(10,2) DEFAULT 0,\n",
    "    converted BOOLEAN DEFAULT FALSE,\n",
    "    PRIMARY KEY (touchpoint_id)\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(timestamp);\n",
    "\n",
    "-- Pre-aggregated customer journeys\n",
    "-- Stores journey paths for faster attribution computation\n",
    "CREATE TABLE IF NOT EXISTS customer_journeys (\n",
    "    journey_id BIGINT IDENTITY(1,1),\n",
    "    user_id VARCHAR(64) NOT NULL,\n",
    "    journey_path VARCHAR(4096),  -- Comma-separated channel sequence\n",
    "    conversion_value DECIMAL(10,2),\n",
    "    touchpoint_count INTEGER,\n",
    "    journey_duration_hours INTEGER,\n",
    "    first_touch_date DATE,\n",
    "    conversion_date DATE,\n",
    "    PRIMARY KEY (journey_id)\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(conversion_date);\n",
    "\n",
    "-- Attribution results table\n",
    "CREATE TABLE IF NOT EXISTS attribution_results (\n",
    "    result_id BIGINT IDENTITY(1,1),\n",
    "    user_id VARCHAR(64),\n",
    "    channel VARCHAR(50),\n",
    "    model_type VARCHAR(50),\n",
    "    attribution_credit DECIMAL(10,4),\n",
    "    attributed_revenue DECIMAL(10,2),\n",
    "    computation_date TIMESTAMP DEFAULT GETDATE(),\n",
    "    PRIMARY KEY (result_id)\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(computation_date, channel);\n",
    "\n",
    "-- Markov chain transition matrices\n",
    "CREATE TABLE IF NOT EXISTS markov_transitions (\n",
    "    from_state VARCHAR(50),\n",
    "    to_state VARCHAR(50),\n",
    "    transition_count BIGINT,\n",
    "    transition_probability DECIMAL(10,8),\n",
    "    model_version VARCHAR(20),\n",
    "    computation_date DATE,\n",
    "    PRIMARY KEY (from_state, to_state, model_version)\n",
    ")\n",
    "DISTKEY(from_state)\n",
    "SORTKEY(model_version, computation_date);\n",
    "\n",
    "-- Channel performance summary (for dashboards)\n",
    "CREATE TABLE IF NOT EXISTS channel_performance (\n",
    "    channel VARCHAR(50),\n",
    "    date DATE,\n",
    "    total_touchpoints BIGINT,\n",
    "    conversions INTEGER,\n",
    "    revenue DECIMAL(12,2),\n",
    "    first_touch_credit DECIMAL(12,2),\n",
    "    last_touch_credit DECIMAL(12,2),\n",
    "    linear_credit DECIMAL(12,2),\n",
    "    markov_credit DECIMAL(12,2),\n",
    "    shapley_credit DECIMAL(12,2),\n",
    "    PRIMARY KEY (channel, date)\n",
    ")\n",
    "DISTKEY(channel)\n",
    "SORTKEY(date);\n",
    "\"\"\"\n",
    "\n",
    "# Execute table creation\n",
    "# rs.execute_query(create_tables_sql, return_df=False)\n",
    "print(\"✓ Table schemas defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Efficient Data Loading from Redshift\n",
    "\n",
    "### Chunked Loading for Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributionDataLoader:\n",
    "    \"\"\"\n",
    "    Efficient data loader for large-scale attribution analysis.\n",
    "    Uses chunked loading and Dask for datasets that don't fit in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rs_connection):\n",
    "        self.rs = rs_connection\n",
    "        \n",
    "    def load_touchpoints_chunked(self, start_date, end_date, chunk_size=1_000_000):\n",
    "        \"\"\"\n",
    "        Load touchpoints in chunks to handle 100M+ rows.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            session_id,\n",
    "            timestamp,\n",
    "            channel,\n",
    "            campaign,\n",
    "            device,\n",
    "            revenue,\n",
    "            converted\n",
    "        FROM touchpoints\n",
    "        WHERE timestamp BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ORDER BY user_id, timestamp\n",
    "        \"\"\"\n",
    "        \n",
    "        chunks = []\n",
    "        for chunk in pd.read_sql(query, self.rs.engine, chunksize=chunk_size):\n",
    "            chunks.append(chunk)\n",
    "            print(f\"Loaded chunk: {len(chunk):,} rows\")\n",
    "        \n",
    "        return pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    def load_journeys_optimized(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Load pre-aggregated journeys for faster processing.\n",
    "        This query runs entirely in Redshift for efficiency.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        WITH journey_aggregation AS (\n",
    "            SELECT \n",
    "                user_id,\n",
    "                LISTAGG(channel, ' > ') WITHIN GROUP (ORDER BY timestamp) as journey_path,\n",
    "                MAX(CASE WHEN converted THEN revenue ELSE 0 END) as conversion_value,\n",
    "                COUNT(*) as touchpoint_count,\n",
    "                DATEDIFF(hour, MIN(timestamp), MAX(timestamp)) as journey_duration_hours,\n",
    "                DATE(MIN(timestamp)) as first_touch_date,\n",
    "                DATE(MAX(timestamp)) as conversion_date,\n",
    "                MAX(converted::INT) as converted\n",
    "            FROM touchpoints\n",
    "            WHERE timestamp BETWEEN '{start_date}' AND '{end_date}'\n",
    "            GROUP BY user_id\n",
    "        )\n",
    "        SELECT *\n",
    "        FROM journey_aggregation\n",
    "        WHERE converted = 1  -- Only conversion journeys for attribution\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql(query, self.rs.engine)\n",
    "    \n",
    "    def get_channel_summary(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Get channel-level summary statistics.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            channel,\n",
    "            COUNT(*) as total_touchpoints,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            SUM(CASE WHEN converted THEN 1 ELSE 0 END) as conversions,\n",
    "            SUM(revenue) as total_revenue\n",
    "        FROM touchpoints\n",
    "        WHERE timestamp BETWEEN '{start_date}' AND '{end_date}'\n",
    "        GROUP BY channel\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql(query, self.rs.engine)\n",
    "\n",
    "# Example usage (uncomment when connected)\n",
    "# loader = AttributionDataLoader(rs)\n",
    "# journeys_df = loader.load_journeys_optimized('2024-01-01', '2024-12-31')\n",
    "print(\"✓ Data loader configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulated Large-Scale Dataset\n",
    "\n",
    "### Generate 10M Customer Journeys for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_large_scale_journeys(n_journeys=10_000_000, save_to_file=True):\n",
    "    \"\"\"\n",
    "    Generate large-scale synthetic journey data for testing.\n",
    "    Uses memory-efficient generation with Dask.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_journeys:,} customer journeys...\")\n",
    "    \n",
    "    channels = ['Paid Search', 'Organic Search', 'Social', 'Display', \n",
    "                'Email', 'Direct', 'Affiliate', 'Video']\n",
    "    \n",
    "    # Journey patterns (realistic probabilities)\n",
    "    journey_templates = [\n",
    "        ['Paid Search', 'Direct'],  # 30%\n",
    "        ['Organic Search', 'Direct'],  # 25%\n",
    "        ['Social', 'Paid Search', 'Direct'],  # 15%\n",
    "        ['Display', 'Organic Search', 'Direct'],  # 10%\n",
    "        ['Email', 'Direct'],  # 10%\n",
    "        ['Social', 'Email', 'Paid Search', 'Direct'],  # 5%\n",
    "        ['Display', 'Social', 'Organic Search', 'Direct'],  # 3%\n",
    "        ['Affiliate', 'Paid Search', 'Direct'],  # 2%\n",
    "    ]\n",
    "    \n",
    "    template_probs = [0.30, 0.25, 0.15, 0.10, 0.10, 0.05, 0.03, 0.02]\n",
    "    \n",
    "    def generate_batch(batch_size=100000):\n",
    "        \"\"\"Generate a batch of journeys\"\"\"\n",
    "        template_indices = np.random.choice(\n",
    "            len(journey_templates), \n",
    "            size=batch_size, \n",
    "            p=template_probs\n",
    "        )\n",
    "        \n",
    "        batch_journeys = []\n",
    "        for idx in template_indices:\n",
    "            template = journey_templates[idx].copy()\n",
    "            # Add some randomness\n",
    "            if np.random.random() < 0.3:\n",
    "                # Insert random channel\n",
    "                insert_pos = np.random.randint(0, len(template))\n",
    "                template.insert(insert_pos, np.random.choice(channels))\n",
    "            \n",
    "            batch_journeys.append(' > '.join(template))\n",
    "        \n",
    "        # Generate conversion values (log-normal distribution)\n",
    "        conversion_values = np.random.lognormal(mean=4.0, sigma=1.0, size=batch_size)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'user_id': [f'user_{i}' for i in range(batch_size)],\n",
    "            'journey_path': batch_journeys,\n",
    "            'conversion_value': conversion_values,\n",
    "            'touchpoint_count': [len(j.split(' > ')) for j in batch_journeys]\n",
    "        })\n",
    "    \n",
    "    # Generate in batches to manage memory\n",
    "    batch_size = 100000\n",
    "    n_batches = n_journeys // batch_size\n",
    "    \n",
    "    all_batches = []\n",
    "    for i in tqdm(range(n_batches), desc=\"Generating batches\"):\n",
    "        batch_df = generate_batch(batch_size)\n",
    "        all_batches.append(batch_df)\n",
    "    \n",
    "    # Handle remainder\n",
    "    remainder = n_journeys % batch_size\n",
    "    if remainder > 0:\n",
    "        batch_df = generate_batch(remainder)\n",
    "        all_batches.append(batch_df)\n",
    "    \n",
    "    journeys_df = pd.concat(all_batches, ignore_index=True)\n",
    "    journeys_df['user_id'] = [f'user_{i}' for i in range(len(journeys_df))]\n",
    "    \n",
    "    if save_to_file:\n",
    "        # Save in chunks using parquet for efficiency\n",
    "        journeys_df.to_parquet('large_scale_journeys.parquet', compression='snappy')\n",
    "        print(f\"✓ Saved {len(journeys_df):,} journeys to large_scale_journeys.parquet\")\n",
    "    \n",
    "    return journeys_df\n",
    "\n",
    "# Generate sample dataset (reduce size for demo)\n",
    "print(\"Generating sample dataset (100K journeys for demo)...\")\n",
    "journeys_df = generate_large_scale_journeys(n_journeys=100000, save_to_file=False)\n",
    "\n",
    "print(f\"\\n✓ Dataset shape: {journeys_df.shape}\")\n",
    "print(f\"✓ Total conversion value: ${journeys_df['conversion_value'].sum():,.2f}\")\n",
    "print(f\"\\nSample journeys:\")\n",
    "print(journeys_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Markov Chain Attribution with Sparse Matrices\n",
    "\n",
    "### Memory-Efficient Implementation for Large-Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMarkovAttribution:\n",
    "    \"\"\"\n",
    "    Efficient Markov Chain attribution using sparse matrices.\n",
    "    Handles millions of journeys with minimal memory footprint.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transition_matrix = None\n",
    "        self.removal_effects = {}\n",
    "        self.channels = None\n",
    "        self.state_to_idx = {}\n",
    "        self.idx_to_state = {}\n",
    "        \n",
    "    def build_transition_matrix(self, journeys_df):\n",
    "        \"\"\"\n",
    "        Build sparse transition matrix from journey paths.\n",
    "        \"\"\"\n",
    "        print(\"Building transition matrix...\")\n",
    "        \n",
    "        # Extract all unique channels\n",
    "        all_channels = set()\n",
    "        for path in journeys_df['journey_path']:\n",
    "            channels = path.split(' > ')\n",
    "            all_channels.update(channels)\n",
    "        \n",
    "        # Add special states\n",
    "        states = ['START'] + sorted(list(all_channels)) + ['CONVERSION', 'NULL']\n",
    "        self.channels = sorted(list(all_channels))\n",
    "        \n",
    "        # Create state mappings\n",
    "        self.state_to_idx = {state: idx for idx, state in enumerate(states)}\n",
    "        self.idx_to_state = {idx: state for state, idx in self.state_to_idx.items()}\n",
    "        \n",
    "        n_states = len(states)\n",
    "        \n",
    "        # Count transitions using dictionary (memory efficient)\n",
    "        transition_counts = defaultdict(int)\n",
    "        \n",
    "        for _, row in tqdm(journeys_df.iterrows(), total=len(journeys_df), desc=\"Counting transitions\"):\n",
    "            path = row['journey_path'].split(' > ')\n",
    "            value = row['conversion_value']\n",
    "            \n",
    "            # START to first channel\n",
    "            transition_counts[(self.state_to_idx['START'], \n",
    "                             self.state_to_idx[path[0]])] += value\n",
    "            \n",
    "            # Channel to channel transitions\n",
    "            for i in range(len(path) - 1):\n",
    "                from_idx = self.state_to_idx[path[i]]\n",
    "                to_idx = self.state_to_idx[path[i + 1]]\n",
    "                transition_counts[(from_idx, to_idx)] += value\n",
    "            \n",
    "            # Last channel to CONVERSION\n",
    "            transition_counts[(self.state_to_idx[path[-1]], \n",
    "                             self.state_to_idx['CONVERSION'])] += value\n",
    "        \n",
    "        # Build sparse matrix\n",
    "        row_indices = []\n",
    "        col_indices = []\n",
    "        data = []\n",
    "        \n",
    "        for (from_idx, to_idx), count in transition_counts.items():\n",
    "            row_indices.append(from_idx)\n",
    "            col_indices.append(to_idx)\n",
    "            data.append(count)\n",
    "        \n",
    "        # Create sparse matrix\n",
    "        self.transition_matrix = csr_matrix(\n",
    "            (data, (row_indices, col_indices)),\n",
    "            shape=(n_states, n_states)\n",
    "        )\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        row_sums = np.array(self.transition_matrix.sum(axis=1)).flatten()\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        \n",
    "        # Normalize each row\n",
    "        self.transition_matrix = self.transition_matrix.multiply(\n",
    "            1.0 / row_sums[:, np.newaxis]\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Transition matrix built: {n_states} states, {len(data):,} transitions\")\n",
    "        print(f\"✓ Matrix density: {self.transition_matrix.nnz / (n_states ** 2):.4%}\")\n",
    "        \n",
    "    def calculate_conversion_probability(self):\n",
    "        \"\"\"\n",
    "        Calculate base conversion probability from START to CONVERSION.\n",
    "        \"\"\"\n",
    "        start_idx = self.state_to_idx['START']\n",
    "        conversion_idx = self.state_to_idx['CONVERSION']\n",
    "        \n",
    "        # Use matrix exponentiation to find long-term probabilities\n",
    "        # P(conversion) = sum of all paths from START to CONVERSION\n",
    "        n_states = self.transition_matrix.shape[0]\n",
    "        \n",
    "        # Identity matrix\n",
    "        I = sparse.eye(n_states, format='csr')\n",
    "        \n",
    "        # (I - Q)^(-1) where Q is the transient states submatrix\n",
    "        # For simplicity, we'll use iterative approach\n",
    "        \n",
    "        # Start with initial probability vector\n",
    "        prob_vector = np.zeros(n_states)\n",
    "        prob_vector[start_idx] = 1.0\n",
    "        \n",
    "        # Iterate until convergence\n",
    "        for _ in range(100):  # Max iterations\n",
    "            prob_vector = self.transition_matrix.T.dot(prob_vector)\n",
    "        \n",
    "        return prob_vector[conversion_idx]\n",
    "    \n",
    "    def calculate_removal_effects(self):\n",
    "        \"\"\"\n",
    "        Calculate removal effect for each channel.\n",
    "        This shows the impact of removing each channel from the model.\n",
    "        \"\"\"\n",
    "        print(\"\\nCalculating removal effects...\")\n",
    "        \n",
    "        base_conversion_prob = self.calculate_conversion_probability()\n",
    "        \n",
    "        for channel in tqdm(self.channels, desc=\"Processing channels\"):\n",
    "            # Create modified matrix with channel removed\n",
    "            modified_matrix = self.transition_matrix.copy()\n",
    "            \n",
    "            channel_idx = self.state_to_idx[channel]\n",
    "            \n",
    "            # Redirect transitions through removed channel to NULL state\n",
    "            null_idx = self.state_to_idx['NULL']\n",
    "            \n",
    "            # Zero out the channel's row and column\n",
    "            modified_matrix[channel_idx, :] = 0\n",
    "            modified_matrix[:, channel_idx] = 0\n",
    "            \n",
    "            # Redirect to NULL\n",
    "            for i in range(modified_matrix.shape[0]):\n",
    "                if self.transition_matrix[i, channel_idx] > 0:\n",
    "                    modified_matrix[i, null_idx] = self.transition_matrix[i, channel_idx]\n",
    "            \n",
    "            # Renormalize\n",
    "            row_sums = np.array(modified_matrix.sum(axis=1)).flatten()\n",
    "            row_sums[row_sums == 0] = 1\n",
    "            modified_matrix = modified_matrix.multiply(1.0 / row_sums[:, np.newaxis])\n",
    "            \n",
    "            # Calculate new conversion probability\n",
    "            start_idx = self.state_to_idx['START']\n",
    "            conversion_idx = self.state_to_idx['CONVERSION']\n",
    "            \n",
    "            prob_vector = np.zeros(modified_matrix.shape[0])\n",
    "            prob_vector[start_idx] = 1.0\n",
    "            \n",
    "            for _ in range(100):\n",
    "                prob_vector = modified_matrix.T.dot(prob_vector)\n",
    "            \n",
    "            new_conversion_prob = prob_vector[conversion_idx]\n",
    "            \n",
    "            # Removal effect\n",
    "            removal_effect = 1 - (new_conversion_prob / base_conversion_prob)\n",
    "            self.removal_effects[channel] = removal_effect\n",
    "        \n",
    "        print(\"✓ Removal effects calculated\")\n",
    "        \n",
    "    def attribute_conversions(self, total_conversions, total_revenue):\n",
    "        \"\"\"\n",
    "        Attribute conversions and revenue to channels based on removal effects.\n",
    "        \"\"\"\n",
    "        # Normalize removal effects\n",
    "        total_removal = sum(self.removal_effects.values())\n",
    "        \n",
    "        if total_removal == 0:\n",
    "            print(\"Warning: Total removal effect is zero\")\n",
    "            return {}\n",
    "        \n",
    "        attribution = {}\n",
    "        for channel, effect in self.removal_effects.items():\n",
    "            attribution[channel] = {\n",
    "                'removal_effect': effect,\n",
    "                'attribution_weight': effect / total_removal,\n",
    "                'attributed_conversions': (effect / total_removal) * total_conversions,\n",
    "                'attributed_revenue': (effect / total_removal) * total_revenue\n",
    "            }\n",
    "        \n",
    "        return attribution\n",
    "\n",
    "# Test with sample data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MARKOV CHAIN ATTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "markov_model = SparseMarkovAttribution()\n",
    "markov_model.build_transition_matrix(journeys_df)\n",
    "markov_model.calculate_removal_effects()\n",
    "\n",
    "total_revenue = journeys_df['conversion_value'].sum()\n",
    "total_conversions = len(journeys_df)\n",
    "\n",
    "markov_attribution = markov_model.attribute_conversions(total_conversions, total_revenue)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nMarkov Attribution Results:\")\n",
    "results_df = pd.DataFrame(markov_attribution).T\n",
    "results_df = results_df.sort_values('attributed_revenue', ascending=False)\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Shapley Value Approximation for Large Datasets\n",
    "\n",
    "### Monte Carlo Sampling for Efficient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproximateShapleyAttribution:\n",
    "    \"\"\"\n",
    "    Approximate Shapley values using Monte Carlo sampling.\n",
    "    Exact Shapley computation is O(2^n) - infeasible for many channels.\n",
    "    This approximation is O(n * m) where m is number of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_samples=10000):\n",
    "        self.n_samples = n_samples\n",
    "        self.shapley_values = {}\n",
    "        \n",
    "    def calculate_coalition_value(self, coalition, journeys_df):\n",
    "        \"\"\"\n",
    "        Calculate the value (conversions) for a coalition of channels.\n",
    "        \"\"\"\n",
    "        if not coalition:\n",
    "            return 0\n",
    "        \n",
    "        coalition_set = set(coalition)\n",
    "        total_value = 0\n",
    "        \n",
    "        for _, row in journeys_df.iterrows():\n",
    "            journey_channels = set(row['journey_path'].split(' > '))\n",
    "            # Journey succeeds if it only uses channels in the coalition\n",
    "            if journey_channels.issubset(coalition_set):\n",
    "                total_value += row['conversion_value']\n",
    "        \n",
    "        return total_value\n",
    "    \n",
    "    def approximate_shapley(self, journeys_df, channels=None):\n",
    "        \"\"\"\n",
    "        Approximate Shapley values using Monte Carlo sampling.\n",
    "        \"\"\"\n",
    "        if channels is None:\n",
    "            # Extract all unique channels\n",
    "            all_channels = set()\n",
    "            for path in journeys_df['journey_path']:\n",
    "                all_channels.update(path.split(' > '))\n",
    "            channels = sorted(list(all_channels))\n",
    "        \n",
    "        n_channels = len(channels)\n",
    "        print(f\"Approximating Shapley values for {n_channels} channels...\")\n",
    "        print(f\"Using {self.n_samples:,} Monte Carlo samples\")\n",
    "        \n",
    "        # Initialize marginal contribution sums\n",
    "        marginal_contribs = {channel: [] for channel in channels}\n",
    "        \n",
    "        # Monte Carlo sampling\n",
    "        for sample in tqdm(range(self.n_samples), desc=\"MC Sampling\"):\n",
    "            # Random permutation of channels\n",
    "            permutation = np.random.permutation(channels)\n",
    "            \n",
    "            # For each channel in permutation, calculate marginal contribution\n",
    "            for i, channel in enumerate(permutation):\n",
    "                # Coalition before adding this channel\n",
    "                coalition_before = permutation[:i].tolist()\n",
    "                # Coalition after adding this channel\n",
    "                coalition_after = permutation[:i+1].tolist()\n",
    "                \n",
    "                # Calculate marginal contribution\n",
    "                value_before = self.calculate_coalition_value(coalition_before, journeys_df)\n",
    "                value_after = self.calculate_coalition_value(coalition_after, journeys_df)\n",
    "                \n",
    "                marginal_contrib = value_after - value_before\n",
    "                marginal_contribs[channel].append(marginal_contrib)\n",
    "        \n",
    "        # Average marginal contributions = Shapley value\n",
    "        for channel in channels:\n",
    "            self.shapley_values[channel] = np.mean(marginal_contribs[channel])\n",
    "        \n",
    "        print(\"✓ Shapley values calculated\")\n",
    "        return self.shapley_values\n",
    "    \n",
    "    def get_attribution(self, total_conversions, total_revenue):\n",
    "        \"\"\"\n",
    "        Convert Shapley values to attribution percentages.\n",
    "        \"\"\"\n",
    "        total_shapley = sum(self.shapley_values.values())\n",
    "        \n",
    "        if total_shapley == 0:\n",
    "            print(\"Warning: Total Shapley value is zero\")\n",
    "            return {}\n",
    "        \n",
    "        attribution = {}\n",
    "        for channel, value in self.shapley_values.items():\n",
    "            attribution[channel] = {\n",
    "                'shapley_value': value,\n",
    "                'attribution_weight': value / total_shapley,\n",
    "                'attributed_conversions': (value / total_shapley) * total_conversions,\n",
    "                'attributed_revenue': (value / total_shapley) * total_revenue\n",
    "            }\n",
    "        \n",
    "        return attribution\n",
    "\n",
    "# Test with smaller sample for demo (use more samples in production)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHAPLEY VALUE ATTRIBUTION (APPROXIMATE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use subset for faster computation\n",
    "sample_journeys = journeys_df.sample(n=min(1000, len(journeys_df)), random_state=42)\n",
    "\n",
    "shapley_model = ApproximateShapleyAttribution(n_samples=100)  # Reduce for demo\n",
    "shapley_values = shapley_model.approximate_shapley(sample_journeys)\n",
    "\n",
    "total_revenue_sample = sample_journeys['conversion_value'].sum()\n",
    "total_conversions_sample = len(sample_journeys)\n",
    "\n",
    "shapley_attribution = shapley_model.get_attribution(total_conversions_sample, total_revenue_sample)\n",
    "\n",
    "print(\"\\nShapley Attribution Results:\")\n",
    "shapley_df = pd.DataFrame(shapley_attribution).T\n",
    "shapley_df = shapley_df.sort_values('attributed_revenue', ascending=False)\n",
    "print(shapley_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Distributed Attribution with Dask\n",
    "\n",
    "### Parallel Processing for 100M+ Touchpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedAttribution:\n",
    "    \"\"\"\n",
    "    Distributed attribution computation using Dask.\n",
    "    Processes data in parallel across multiple workers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_workers=4):\n",
    "        self.client = Client(n_workers=n_workers, threads_per_worker=2)\n",
    "        print(f\"✓ Dask client started: {self.client.dashboard_link}\")\n",
    "        \n",
    "    def parallel_journey_processing(self, journeys_df):\n",
    "        \"\"\"\n",
    "        Process journeys in parallel using Dask.\n",
    "        \"\"\"\n",
    "        # Convert to Dask DataFrame\n",
    "        ddf = dd.from_pandas(journeys_df, npartitions=self.client.ncores())\n",
    "        \n",
    "        # Define processing function\n",
    "        def process_journey(row):\n",
    "            channels = row['journey_path'].split(' > ')\n",
    "            n_touchpoints = len(channels)\n",
    "            value = row['conversion_value']\n",
    "            \n",
    "            # Calculate linear attribution\n",
    "            linear_credit = value / n_touchpoints\n",
    "            \n",
    "            # Calculate time decay attribution\n",
    "            decay_factor = 0.5\n",
    "            weights = [decay_factor ** (n_touchpoints - i - 1) for i in range(n_touchpoints)]\n",
    "            total_weight = sum(weights)\n",
    "            \n",
    "            results = []\n",
    "            for i, channel in enumerate(channels):\n",
    "                results.append({\n",
    "                    'user_id': row['user_id'],\n",
    "                    'channel': channel,\n",
    "                    'position': i,\n",
    "                    'linear_credit': linear_credit,\n",
    "                    'time_decay_credit': value * weights[i] / total_weight,\n",
    "                    'first_touch_credit': value if i == 0 else 0,\n",
    "                    'last_touch_credit': value if i == n_touchpoints - 1 else 0,\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        # Apply processing in parallel\n",
    "        print(\"Processing journeys in parallel...\")\n",
    "        \n",
    "        # Use map_partitions for parallel processing\n",
    "        def process_partition(df):\n",
    "            all_results = []\n",
    "            for _, row in df.iterrows():\n",
    "                all_results.extend(process_journey(row))\n",
    "            return pd.DataFrame(all_results)\n",
    "        \n",
    "        results_ddf = ddf.map_partitions(process_partition, meta={\n",
    "            'user_id': 'str',\n",
    "            'channel': 'str',\n",
    "            'position': 'int',\n",
    "            'linear_credit': 'float',\n",
    "            'time_decay_credit': 'float',\n",
    "            'first_touch_credit': 'float',\n",
    "            'last_touch_credit': 'float'\n",
    "        })\n",
    "        \n",
    "        # Compute results\n",
    "        results_df = results_ddf.compute()\n",
    "        \n",
    "        print(f\"✓ Processed {len(results_df):,} touchpoint attributions\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def aggregate_attribution(self, results_df):\n",
    "        \"\"\"\n",
    "        Aggregate attribution results by channel.\n",
    "        \"\"\"\n",
    "        aggregated = results_df.groupby('channel').agg({\n",
    "            'linear_credit': 'sum',\n",
    "            'time_decay_credit': 'sum',\n",
    "            'first_touch_credit': 'sum',\n",
    "            'last_touch_credit': 'sum'\n",
    "        }).reset_index()\n",
    "        \n",
    "        aggregated.columns = ['channel', 'linear_revenue', 'time_decay_revenue', \n",
    "                             'first_touch_revenue', 'last_touch_revenue']\n",
    "        \n",
    "        return aggregated.sort_values('linear_revenue', ascending=False)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Dask client\"\"\"\n",
    "        self.client.close()\n",
    "        print(\"✓ Dask client closed\")\n",
    "\n",
    "# Test distributed processing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTRIBUTED ATTRIBUTION COMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dist_attr = DistributedAttribution(n_workers=2)\n",
    "distributed_results = dist_attr.parallel_journey_processing(journeys_df)\n",
    "aggregated_results = dist_attr.aggregate_attribution(distributed_results)\n",
    "\n",
    "print(\"\\nAggregated Attribution Results:\")\n",
    "print(aggregated_results.to_string(index=False))\n",
    "\n",
    "dist_attr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Graph-Based Journey Analysis\n",
    "\n",
    "### Network Analysis for Customer Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JourneyGraphAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze customer journeys using graph theory.\n",
    "    Identify key paths, bottlenecks, and channel relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.journey_paths = []\n",
    "        \n",
    "    def build_journey_graph(self, journeys_df, min_edge_weight=10):\n",
    "        \"\"\"\n",
    "        Build directed graph from customer journeys.\n",
    "        \"\"\"\n",
    "        print(\"Building journey graph...\")\n",
    "        \n",
    "        edge_weights = defaultdict(float)\n",
    "        \n",
    "        for _, row in journeys_df.iterrows():\n",
    "            channels = row['journey_path'].split(' > ')\n",
    "            value = row['conversion_value']\n",
    "            \n",
    "            # Add edges with weights\n",
    "            for i in range(len(channels) - 1):\n",
    "                edge = (channels[i], channels[i + 1])\n",
    "                edge_weights[edge] += value\n",
    "        \n",
    "        # Add edges to graph (filter by minimum weight)\n",
    "        for (source, target), weight in edge_weights.items():\n",
    "            if weight >= min_edge_weight:\n",
    "                self.graph.add_edge(source, target, weight=weight)\n",
    "        \n",
    "        print(f\"✓ Graph built: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges\")\n",
    "    \n",
    "    def calculate_centrality_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate various centrality metrics for channels.\n",
    "        \"\"\"\n",
    "        print(\"\\nCalculating centrality metrics...\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # PageRank (importance in the network)\n",
    "        pagerank = nx.pagerank(self.graph, weight='weight')\n",
    "        \n",
    "        # Betweenness centrality (bridge between other channels)\n",
    "        betweenness = nx.betweenness_centrality(self.graph, weight='weight')\n",
    "        \n",
    "        # In-degree and out-degree\n",
    "        in_degree = dict(self.graph.in_degree(weight='weight'))\n",
    "        out_degree = dict(self.graph.out_degree(weight='weight'))\n",
    "        \n",
    "        # Combine metrics\n",
    "        for node in self.graph.nodes():\n",
    "            metrics[node] = {\n",
    "                'pagerank': pagerank.get(node, 0),\n",
    "                'betweenness': betweenness.get(node, 0),\n",
    "                'in_degree': in_degree.get(node, 0),\n",
    "                'out_degree': out_degree.get(node, 0),\n",
    "                'total_degree': in_degree.get(node, 0) + out_degree.get(node, 0)\n",
    "            }\n",
    "        \n",
    "        return pd.DataFrame(metrics).T.sort_values('pagerank', ascending=False)\n",
    "    \n",
    "    def find_critical_paths(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Identify most valuable paths through the journey graph.\n",
    "        \"\"\"\n",
    "        print(f\"\\nFinding top {top_n} critical paths...\")\n",
    "        \n",
    "        # Get all simple paths (up to length 5 to avoid explosion)\n",
    "        all_paths = []\n",
    "        \n",
    "        # Find source nodes (high out-degree, low in-degree)\n",
    "        source_candidates = [\n",
    "            node for node in self.graph.nodes()\n",
    "            if self.graph.in_degree(node) < self.graph.out_degree(node)\n",
    "        ]\n",
    "        \n",
    "        # Find sink nodes (high in-degree, low out-degree)\n",
    "        sink_candidates = [\n",
    "            node for node in self.graph.nodes()\n",
    "            if self.graph.out_degree(node) < self.graph.in_degree(node)\n",
    "        ]\n",
    "        \n",
    "        for source in source_candidates[:5]:  # Limit sources\n",
    "            for sink in sink_candidates[:5]:  # Limit sinks\n",
    "                if source != sink:\n",
    "                    try:\n",
    "                        for path in nx.all_simple_paths(self.graph, source, sink, cutoff=5):\n",
    "                            # Calculate path value\n",
    "                            path_value = sum(\n",
    "                                self.graph[path[i]][path[i+1]]['weight']\n",
    "                                for i in range(len(path) - 1)\n",
    "                            )\n",
    "                            all_paths.append({\n",
    "                                'path': ' > '.join(path),\n",
    "                                'length': len(path),\n",
    "                                'total_value': path_value\n",
    "                            })\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "        \n",
    "        if all_paths:\n",
    "            paths_df = pd.DataFrame(all_paths)\n",
    "            return paths_df.sort_values('total_value', ascending=False).head(top_n)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def visualize_journey_graph(self, top_n_edges=50):\n",
    "        \"\"\"\n",
    "        Visualize the journey graph.\n",
    "        \"\"\"\n",
    "        # Get top N edges by weight\n",
    "        edges_with_weights = [\n",
    "            (u, v, d['weight']) \n",
    "            for u, v, d in self.graph.edges(data=True)\n",
    "        ]\n",
    "        edges_with_weights.sort(key=lambda x: x[2], reverse=True)\n",
    "        top_edges = edges_with_weights[:top_n_edges]\n",
    "        \n",
    "        # Create subgraph with top edges\n",
    "        subgraph = nx.DiGraph()\n",
    "        for u, v, w in top_edges:\n",
    "            subgraph.add_edge(u, v, weight=w)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # Use spring layout\n",
    "        pos = nx.spring_layout(subgraph, k=2, iterations=50)\n",
    "        \n",
    "        # Draw nodes\n",
    "        node_sizes = [\n",
    "            subgraph.degree(node, weight='weight') * 2\n",
    "            for node in subgraph.nodes()\n",
    "        ]\n",
    "        \n",
    "        nx.draw_networkx_nodes(\n",
    "            subgraph, pos,\n",
    "            node_size=node_sizes,\n",
    "            node_color='lightblue',\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        # Draw edges\n",
    "        edge_weights = [subgraph[u][v]['weight'] for u, v in subgraph.edges()]\n",
    "        max_weight = max(edge_weights)\n",
    "        edge_widths = [5 * w / max_weight for w in edge_weights]\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            subgraph, pos,\n",
    "            width=edge_widths,\n",
    "            alpha=0.5,\n",
    "            edge_color='gray',\n",
    "            arrows=True,\n",
    "            arrowsize=20\n",
    "        )\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(\n",
    "            subgraph, pos,\n",
    "            font_size=10,\n",
    "            font_weight='bold'\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"Customer Journey Graph (Top {top_n_edges} Transitions)\", \n",
    "                 fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test graph analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GRAPH-BASED JOURNEY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "graph_analyzer = JourneyGraphAnalyzer()\n",
    "graph_analyzer.build_journey_graph(journeys_df, min_edge_weight=100)\n",
    "\n",
    "centrality_metrics = graph_analyzer.calculate_centrality_metrics()\n",
    "print(\"\\nChannel Centrality Metrics:\")\n",
    "print(centrality_metrics.to_string())\n",
    "\n",
    "critical_paths = graph_analyzer.find_critical_paths(top_n=10)\n",
    "if not critical_paths.empty:\n",
    "    print(\"\\nTop Critical Paths:\")\n",
    "    print(critical_paths.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "graph_analyzer.visualize_journey_graph(top_n_edges=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-World Project: Multi-Touch Attribution System for 10M Customers\n",
    "\n",
    "### Complete Enterprise Attribution Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseAttributionPlatform:\n",
    "    \"\"\"\n",
    "    Complete attribution platform integrating all models.\n",
    "    Production-ready with Redshift backend and caching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rs_connection):\n",
    "        self.rs = rs_connection\n",
    "        self.cache = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def run_all_models(self, start_date, end_date, use_cache=True):\n",
    "        \"\"\"\n",
    "        Run all attribution models and compare results.\n",
    "        \"\"\"\n",
    "        cache_key = f\"{start_date}_{end_date}\"\n",
    "        \n",
    "        if use_cache and cache_key in self.cache:\n",
    "            print(\"Using cached results\")\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        print(f\"\\nRunning enterprise attribution for {start_date} to {end_date}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Load data\n",
    "        loader = AttributionDataLoader(self.rs)\n",
    "        journeys_df = loader.load_journeys_optimized(start_date, end_date)\n",
    "        \n",
    "        total_conversions = len(journeys_df)\n",
    "        total_revenue = journeys_df['conversion_value'].sum()\n",
    "        \n",
    "        print(f\"\\nDataset: {total_conversions:,} conversions, ${total_revenue:,.2f} revenue\")\n",
    "        \n",
    "        # 1. Simple models (fast)\n",
    "        print(\"\\n1. Computing simple attribution models...\")\n",
    "        simple_results = self._compute_simple_models(journeys_df)\n",
    "        \n",
    "        # 2. Markov Chain (medium)\n",
    "        print(\"\\n2. Computing Markov Chain attribution...\")\n",
    "        markov_model = SparseMarkovAttribution()\n",
    "        markov_model.build_transition_matrix(journeys_df)\n",
    "        markov_model.calculate_removal_effects()\n",
    "        markov_results = markov_model.attribute_conversions(total_conversions, total_revenue)\n",
    "        \n",
    "        # 3. Shapley values (slow - use sampling)\n",
    "        print(\"\\n3. Computing Shapley value attribution...\")\n",
    "        sample_size = min(10000, len(journeys_df))\n",
    "        shapley_sample = journeys_df.sample(n=sample_size, random_state=42)\n",
    "        shapley_model = ApproximateShapleyAttribution(n_samples=1000)\n",
    "        shapley_model.approximate_shapley(shapley_sample)\n",
    "        shapley_results = shapley_model.get_attribution(\n",
    "            total_conversions, total_revenue\n",
    "        )\n",
    "        \n",
    "        # Combine results\n",
    "        combined_results = self._combine_model_results(\n",
    "            simple_results, markov_results, shapley_results\n",
    "        )\n",
    "        \n",
    "        # Cache results\n",
    "        self.cache[cache_key] = combined_results\n",
    "        self.results = combined_results\n",
    "        \n",
    "        return combined_results\n",
    "    \n",
    "    def _compute_simple_models(self, journeys_df):\n",
    "        \"\"\"\n",
    "        Compute first-touch, last-touch, and linear attribution.\n",
    "        \"\"\"\n",
    "        results = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        for _, row in journeys_df.iterrows():\n",
    "            channels = row['journey_path'].split(' > ')\n",
    "            value = row['conversion_value']\n",
    "            n_channels = len(channels)\n",
    "            \n",
    "            # First touch\n",
    "            results[channels[0]]['first_touch'] += value\n",
    "            \n",
    "            # Last touch\n",
    "            results[channels[-1]]['last_touch'] += value\n",
    "            \n",
    "            # Linear\n",
    "            linear_credit = value / n_channels\n",
    "            for channel in channels:\n",
    "                results[channel]['linear'] += linear_credit\n",
    "            \n",
    "            # Time decay\n",
    "            decay_factor = 0.5\n",
    "            weights = [decay_factor ** (n_channels - i - 1) for i in range(n_channels)]\n",
    "            total_weight = sum(weights)\n",
    "            for i, channel in enumerate(channels):\n",
    "                results[channel]['time_decay'] += value * weights[i] / total_weight\n",
    "        \n",
    "        return dict(results)\n",
    "    \n",
    "    def _combine_model_results(self, simple, markov, shapley):\n",
    "        \"\"\"\n",
    "        Combine results from all models into unified DataFrame.\n",
    "        \"\"\"\n",
    "        all_channels = set()\n",
    "        all_channels.update(simple.keys())\n",
    "        all_channels.update(markov.keys())\n",
    "        all_channels.update(shapley.keys())\n",
    "        \n",
    "        combined = []\n",
    "        for channel in sorted(all_channels):\n",
    "            row = {'channel': channel}\n",
    "            \n",
    "            # Simple models\n",
    "            if channel in simple:\n",
    "                row['first_touch_revenue'] = simple[channel].get('first_touch', 0)\n",
    "                row['last_touch_revenue'] = simple[channel].get('last_touch', 0)\n",
    "                row['linear_revenue'] = simple[channel].get('linear', 0)\n",
    "                row['time_decay_revenue'] = simple[channel].get('time_decay', 0)\n",
    "            \n",
    "            # Markov\n",
    "            if channel in markov:\n",
    "                row['markov_revenue'] = markov[channel]['attributed_revenue']\n",
    "                row['markov_removal_effect'] = markov[channel]['removal_effect']\n",
    "            \n",
    "            # Shapley\n",
    "            if channel in shapley:\n",
    "                row['shapley_revenue'] = shapley[channel]['attributed_revenue']\n",
    "                row['shapley_value'] = shapley[channel]['shapley_value']\n",
    "            \n",
    "            combined.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(combined).fillna(0)\n",
    "        \n",
    "        # Add ensemble model (average of data-driven models)\n",
    "        df['ensemble_revenue'] = df[['markov_revenue', 'shapley_revenue', 'linear_revenue']].mean(axis=1)\n",
    "        \n",
    "        return df.sort_values('ensemble_revenue', ascending=False)\n",
    "    \n",
    "    def save_to_redshift(self, table_name='attribution_results'):\n",
    "        \"\"\"\n",
    "        Save attribution results to Redshift.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to save. Run attribution first.\")\n",
    "            return\n",
    "        \n",
    "        # Add metadata\n",
    "        self.results['computation_date'] = datetime.now()\n",
    "        self.results['model_version'] = 'v1.0'\n",
    "        \n",
    "        # Save to Redshift\n",
    "        self.results.to_sql(\n",
    "            table_name,\n",
    "            self.rs.engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Results saved to {table_name}\")\n",
    "    \n",
    "    def generate_comparison_report(self):\n",
    "        \"\"\"\n",
    "        Generate visual comparison of all attribution models.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'results') or self.results is None:\n",
    "            print(\"No results available. Run attribution first.\")\n",
    "            return\n",
    "        \n",
    "        df = self.results\n",
    "        \n",
    "        # Create comparison visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # 1. Model comparison by channel\n",
    "        ax1 = axes[0, 0]\n",
    "        model_cols = ['first_touch_revenue', 'last_touch_revenue', 'linear_revenue', \n",
    "                     'markov_revenue', 'shapley_revenue']\n",
    "        df[['channel'] + model_cols].set_index('channel').plot(kind='bar', ax=ax1)\n",
    "        ax1.set_title('Attribution by Model', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Channel')\n",
    "        ax1.set_ylabel('Attributed Revenue ($)')\n",
    "        ax1.legend(title='Model', bbox_to_anchor=(1.05, 1))\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Model correlation heatmap\n",
    "        ax2 = axes[0, 1]\n",
    "        correlation = df[model_cols].corr()\n",
    "        sns.heatmap(correlation, annot=True, fmt='.2f', cmap='coolwarm', ax=ax2)\n",
    "        ax2.set_title('Model Correlation', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 3. Distribution comparison\n",
    "        ax3 = axes[1, 0]\n",
    "        ensemble_data = df[['channel', 'ensemble_revenue']].sort_values('ensemble_revenue', ascending=False)\n",
    "        ax3.pie(ensemble_data['ensemble_revenue'], labels=ensemble_data['channel'], autopct='%1.1f%%')\n",
    "        ax3.set_title('Ensemble Model Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 4. Markov removal effect\n",
    "        ax4 = axes[1, 1]\n",
    "        markov_data = df[['channel', 'markov_removal_effect']].sort_values('markov_removal_effect', ascending=False)\n",
    "        ax4.barh(markov_data['channel'], markov_data['markov_removal_effect'])\n",
    "        ax4.set_title('Markov Removal Effects', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Removal Effect')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ATTRIBUTION MODEL COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "print(\"✓ Enterprise Attribution Platform configured\")\n",
    "print(\"\\nReady to process 10M+ customer journeys with:\")\n",
    "print(\"  - Multiple attribution models (First/Last touch, Linear, Markov, Shapley)\")\n",
    "print(\"  - Distributed processing with Dask\")\n",
    "print(\"  - Redshift integration for data storage\")\n",
    "print(\"  - Graph-based journey analysis\")\n",
    "print(\"  - Production-ready caching and optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Monitoring and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "import psutil\n",
    "\n",
    "def performance_monitor(func):\n",
    "    \"\"\"\n",
    "    Decorator to monitor function performance.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Memory before\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Time execution\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Memory after\n",
    "        mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"\\n[Performance] {func.__name__}:\")\n",
    "        print(f\"  Execution time: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"  Memory before: {mem_before:.2f} MB\")\n",
    "        print(f\"  Memory after: {mem_after:.2f} MB\")\n",
    "        print(f\"  Memory delta: {mem_after - mem_before:.2f} MB\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Example usage\n",
    "@performance_monitor\n",
    "def test_large_scale_processing():\n",
    "    \"\"\"Test processing performance\"\"\"\n",
    "    test_df = generate_large_scale_journeys(n_journeys=10000, save_to_file=False)\n",
    "    return len(test_df)\n",
    "\n",
    "n_journeys = test_large_scale_processing()\n",
    "print(f\"\\n✓ Processed {n_journeys:,} journeys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Data Architecture**\n",
    "   - Use Redshift DISTKEY and SORTKEY for optimal performance\n",
    "   - Pre-aggregate journeys when possible\n",
    "   - Partition large tables by date\n",
    "\n",
    "2. **Algorithm Selection**\n",
    "   - Use sparse matrices for Markov chains (100x memory reduction)\n",
    "   - Approximate Shapley values with Monte Carlo sampling\n",
    "   - Distribute computation with Dask for 100M+ rows\n",
    "\n",
    "3. **Performance Optimization**\n",
    "   - Load data in chunks\n",
    "   - Use columnar formats (Parquet)\n",
    "   - Implement caching for repeated queries\n",
    "   - Profile and monitor memory usage\n",
    "\n",
    "4. **Production Deployment**\n",
    "   - Implement connection pooling\n",
    "   - Add error handling and retries\n",
    "   - Schedule batch processing during off-hours\n",
    "   - Monitor query performance in Redshift\n",
    "\n",
    "### Next Steps\n",
    "- Implement real-time attribution streaming\n",
    "- Build attribution API with FastAPI\n",
    "- Create automated reporting dashboards\n",
    "- Integrate with marketing automation platforms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
