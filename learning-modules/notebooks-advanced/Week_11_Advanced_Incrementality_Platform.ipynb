{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Advanced Incrementality Measurement Platform\n",
    "\n",
    "## Scalable Geo-Lift and Synthetic Control at Enterprise Scale\n",
    "\n",
    "### Learning Objectives\n",
    "- Build scalable geo-lift analysis systems\n",
    "- Implement automated matched market selection\n",
    "- Deploy synthetic control at scale\n",
    "- Analyze causal impact with large time series\n",
    "- Create incrementality monitoring dashboards\n",
    "- Master production deployment patterns\n",
    "\n",
    "### Prerequisites\n",
    "- AWS Redshift cluster access\n",
    "- Python 3.8+\n",
    "- Understanding of causal inference\n",
    "- Familiarity with time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install causalimpact pandas numpy scipy scikit-learn statsmodels \\\n",
    "    matplotlib seaborn plotly psycopg2-binary sqlalchemy \\\n",
    "    prophet pymc3 arviz optuna shap joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from causalimpact import CausalImpact\n",
    "from prophet import Prophet\n",
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Data Architecture for Incrementality Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftIncrementalityData:\n",
    "    \"\"\"\n",
    "    Data management for incrementality testing at scale.\n",
    "    Handles geo-level data, experiments, and results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host, port, database, user, password):\n",
    "        conn_string = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\"\n",
    "        self.engine = create_engine(conn_string, pool_size=10)\n",
    "        print(f\"✓ Connected to Redshift: {database}\")\n",
    "    \n",
    "    def create_incrementality_tables(self):\n",
    "        \"\"\"\n",
    "        Create optimized tables for incrementality testing.\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "        -- Geo-level daily metrics\n",
    "        CREATE TABLE IF NOT EXISTS geo_metrics (\n",
    "            date DATE NOT NULL,\n",
    "            geo_id VARCHAR(50) NOT NULL,\n",
    "            geo_name VARCHAR(100),\n",
    "            region VARCHAR(50),\n",
    "            population INTEGER,\n",
    "            revenue DECIMAL(12,2),\n",
    "            conversions INTEGER,\n",
    "            sessions INTEGER,\n",
    "            marketing_spend DECIMAL(12,2),\n",
    "            organic_traffic INTEGER,\n",
    "            PRIMARY KEY (date, geo_id)\n",
    "        )\n",
    "        DISTKEY(geo_id)\n",
    "        SORTKEY(date, geo_id);\n",
    "        \n",
    "        -- Experiment configurations\n",
    "        CREATE TABLE IF NOT EXISTS experiments (\n",
    "            experiment_id VARCHAR(100) PRIMARY KEY,\n",
    "            experiment_name VARCHAR(200),\n",
    "            experiment_type VARCHAR(50),  -- geo_lift, synthetic_control, etc.\n",
    "            start_date DATE,\n",
    "            end_date DATE,\n",
    "            treatment_geos VARCHAR(1000),  -- JSON array\n",
    "            control_geos VARCHAR(1000),    -- JSON array\n",
    "            status VARCHAR(20),\n",
    "            created_at TIMESTAMP DEFAULT GETDATE()\n",
    "        )\n",
    "        DISTKEY(experiment_id);\n",
    "        \n",
    "        -- Experiment results\n",
    "        CREATE TABLE IF NOT EXISTS experiment_results (\n",
    "            experiment_id VARCHAR(100),\n",
    "            metric_name VARCHAR(50),\n",
    "            treatment_mean DECIMAL(12,2),\n",
    "            control_mean DECIMAL(12,2),\n",
    "            lift_absolute DECIMAL(12,2),\n",
    "            lift_percentage DECIMAL(10,2),\n",
    "            p_value DECIMAL(10,6),\n",
    "            ci_lower DECIMAL(12,2),\n",
    "            ci_upper DECIMAL(12,2),\n",
    "            is_significant BOOLEAN,\n",
    "            computation_date TIMESTAMP DEFAULT GETDATE(),\n",
    "            PRIMARY KEY (experiment_id, metric_name)\n",
    "        )\n",
    "        DISTKEY(experiment_id);\n",
    "        \n",
    "        -- Daily incrementality estimates\n",
    "        CREATE TABLE IF NOT EXISTS daily_incrementality (\n",
    "            date DATE,\n",
    "            experiment_id VARCHAR(100),\n",
    "            geo_id VARCHAR(50),\n",
    "            metric_name VARCHAR(50),\n",
    "            observed_value DECIMAL(12,2),\n",
    "            predicted_value DECIMAL(12,2),\n",
    "            incremental_value DECIMAL(12,2),\n",
    "            PRIMARY KEY (date, experiment_id, geo_id, metric_name)\n",
    "        )\n",
    "        DISTKEY(experiment_id)\n",
    "        SORTKEY(date);\n",
    "        \n",
    "        -- Matched markets (for reuse)\n",
    "        CREATE TABLE IF NOT EXISTS matched_markets (\n",
    "            match_id VARCHAR(100) PRIMARY KEY,\n",
    "            treatment_geo VARCHAR(50),\n",
    "            control_geos VARCHAR(1000),  -- JSON array with weights\n",
    "            match_score DECIMAL(6,4),\n",
    "            matching_method VARCHAR(50),\n",
    "            pre_period_start DATE,\n",
    "            pre_period_end DATE,\n",
    "            created_at TIMESTAMP DEFAULT GETDATE()\n",
    "        )\n",
    "        DISTKEY(treatment_geo);\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(sql)\n",
    "        print(\"✓ Incrementality tables created\")\n",
    "    \n",
    "    def load_geo_data(self, start_date, end_date, geos=None):\n",
    "        \"\"\"\n",
    "        Load geo-level time series data.\n",
    "        \"\"\"\n",
    "        geo_filter = f\"AND geo_id IN ({','.join([f\"'{g}'\" for g in geos])})\" if geos else \"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM geo_metrics\n",
    "        WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        {geo_filter}\n",
    "        ORDER BY date, geo_id\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def save_experiment(self, experiment_config):\n",
    "        \"\"\"\n",
    "        Save experiment configuration.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame([experiment_config])\n",
    "        df.to_sql('experiments', self.engine, if_exists='append', index=False)\n",
    "        print(f\"✓ Experiment saved: {experiment_config['experiment_id']}\")\n",
    "    \n",
    "    def save_results(self, results_df, experiment_id):\n",
    "        \"\"\"\n",
    "        Save experiment results.\n",
    "        \"\"\"\n",
    "        results_df['experiment_id'] = experiment_id\n",
    "        results_df.to_sql(\n",
    "            'experiment_results',\n",
    "            self.engine,\n",
    "            if_exists='append',\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"✓ Results saved for experiment: {experiment_id}\")\n",
    "\n",
    "# Initialize (update with your credentials)\n",
    "# rs_incr = RedshiftIncrementalityData(\n",
    "#     host='your-cluster.region.redshift.amazonaws.com',\n",
    "#     port=5439,\n",
    "#     database='marketing_db',\n",
    "#     user='your_user',\n",
    "#     password='your_password'\n",
    "# )\n",
    "\n",
    "print(\"✓ Redshift incrementality data loader configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Geo-Level Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_geo_dataset(n_geos=50, n_days=365, treatment_effect=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate realistic geo-level time series data with treatment effect.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_geos: Number of geographies\n",
    "    - n_days: Number of days\n",
    "    - treatment_effect: True incremental lift (e.g., 0.15 = 15% lift)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate dates\n",
    "    dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "    \n",
    "    # Generate geo characteristics\n",
    "    geos = []\n",
    "    for i in range(n_geos):\n",
    "        geos.append({\n",
    "            'geo_id': f'GEO_{i:03d}',\n",
    "            'geo_name': f'Market {i+1}',\n",
    "            'region': f'Region_{i % 5}',\n",
    "            'population': int(np.random.lognormal(12, 0.5)),  # Population size\n",
    "            'base_revenue': np.random.lognormal(10, 0.3)     # Base daily revenue\n",
    "        })\n",
    "    \n",
    "    geos_df = pd.DataFrame(geos)\n",
    "    \n",
    "    # Select treatment geos (20% of total)\n",
    "    n_treatment = max(1, n_geos // 5)\n",
    "    treatment_geos = np.random.choice(geos_df['geo_id'].values, n_treatment, replace=False)\n",
    "    \n",
    "    # Treatment period: last 30% of time series\n",
    "    treatment_start_idx = int(n_days * 0.7)\n",
    "    treatment_start_date = dates[treatment_start_idx]\n",
    "    \n",
    "    # Generate time series for each geo\n",
    "    all_data = []\n",
    "    \n",
    "    for _, geo in geos_df.iterrows():\n",
    "        geo_id = geo['geo_id']\n",
    "        base_revenue = geo['base_revenue']\n",
    "        \n",
    "        # Trend component\n",
    "        trend = 1 + 0.0005 * np.arange(n_days)\n",
    "        \n",
    "        # Seasonal component (weekly)\n",
    "        day_of_week = np.array([d.dayofweek for d in dates])\n",
    "        weekend_effect = np.where(day_of_week >= 5, 1.2, 1.0)\n",
    "        \n",
    "        # Annual seasonality\n",
    "        day_of_year = np.array([d.dayofyear for d in dates])\n",
    "        annual_seasonality = 1 + 0.2 * np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "        \n",
    "        # Base revenue with trend and seasonality\n",
    "        revenue = base_revenue * trend * weekend_effect * annual_seasonality\n",
    "        \n",
    "        # Add treatment effect\n",
    "        is_treatment = geo_id in treatment_geos\n",
    "        if is_treatment:\n",
    "            treatment_mask = np.arange(n_days) >= treatment_start_idx\n",
    "            # Gradual ramp-up of treatment effect\n",
    "            days_since_treatment = np.maximum(0, np.arange(n_days) - treatment_start_idx)\n",
    "            ramp_up = np.minimum(1.0, days_since_treatment / 7)  # Full effect after 7 days\n",
    "            treatment_lift = treatment_mask * treatment_effect * ramp_up\n",
    "            revenue = revenue * (1 + treatment_lift)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, base_revenue * 0.1, n_days)\n",
    "        revenue += noise\n",
    "        revenue = np.maximum(revenue, 0)\n",
    "        \n",
    "        # Generate related metrics\n",
    "        conversions = (revenue / 50 + np.random.normal(0, 5, n_days)).astype(int)\n",
    "        conversions = np.maximum(conversions, 0)\n",
    "        \n",
    "        sessions = (conversions * 20 + np.random.normal(0, 100, n_days)).astype(int)\n",
    "        sessions = np.maximum(sessions, conversions)\n",
    "        \n",
    "        # Create records\n",
    "        for i, date in enumerate(dates):\n",
    "            all_data.append({\n",
    "                'date': date,\n",
    "                'geo_id': geo_id,\n",
    "                'geo_name': geo['geo_name'],\n",
    "                'region': geo['region'],\n",
    "                'population': geo['population'],\n",
    "                'revenue': revenue[i],\n",
    "                'conversions': conversions[i],\n",
    "                'sessions': sessions[i],\n",
    "                'is_treatment': is_treatment\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    print(f\"✓ Generated geo dataset: {n_geos} geos, {n_days} days\")\n",
    "    print(f\"  Treatment geos: {len(treatment_geos)}\")\n",
    "    print(f\"  Control geos: {n_geos - len(treatment_geos)}\")\n",
    "    print(f\"  Treatment start: {treatment_start_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  True treatment effect: {treatment_effect:.1%}\")\n",
    "    \n",
    "    metadata = {\n",
    "        'treatment_geos': treatment_geos.tolist(),\n",
    "        'treatment_start_date': treatment_start_date,\n",
    "        'treatment_effect': treatment_effect,\n",
    "        'pre_period_days': treatment_start_idx,\n",
    "        'post_period_days': n_days - treatment_start_idx\n",
    "    }\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "# Generate dataset\n",
    "geo_data, metadata = generate_geo_dataset(n_geos=50, n_days=365, treatment_effect=0.15)\n",
    "\n",
    "print(\"\\nDataset preview:\")\n",
    "print(geo_data.head())\n",
    "\n",
    "print(\"\\nSummary by treatment group:\")\n",
    "print(geo_data.groupby('is_treatment')[['revenue', 'conversions', 'sessions']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automated Matched Market Selection\n",
    "\n",
    "### Find Similar Control Markets for Treatment Geos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedMarketSelector:\n",
    "    \"\"\"\n",
    "    Automated selection of matched control markets.\n",
    "    Uses multiple methods: correlation, distance, and synthetic matching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.matches = {}\n",
    "        \n",
    "    def prepare_features(self, df, pre_period_end, metric='revenue'):\n",
    "        \"\"\"\n",
    "        Prepare geo-level features for matching.\n",
    "        \"\"\"\n",
    "        # Filter pre-period data\n",
    "        pre_data = df[df['date'] <= pre_period_end].copy()\n",
    "        \n",
    "        # Create pivot table: geos as columns, dates as rows\n",
    "        pivot = pre_data.pivot(index='date', columns='geo_id', values=metric)\n",
    "        \n",
    "        # Calculate features for each geo\n",
    "        features = []\n",
    "        \n",
    "        for geo_id in pivot.columns:\n",
    "            ts = pivot[geo_id].values\n",
    "            \n",
    "            features.append({\n",
    "                'geo_id': geo_id,\n",
    "                'mean': np.mean(ts),\n",
    "                'std': np.std(ts),\n",
    "                'cv': np.std(ts) / np.mean(ts) if np.mean(ts) > 0 else 0,\n",
    "                'trend': self._calculate_trend(ts),\n",
    "                'seasonality': self._calculate_seasonality(ts),\n",
    "                'autocorr': self._calculate_autocorr(ts)\n",
    "            })\n",
    "        \n",
    "        features_df = pd.DataFrame(features).set_index('geo_id')\n",
    "        \n",
    "        return pivot, features_df\n",
    "    \n",
    "    def _calculate_trend(self, ts):\n",
    "        \"\"\"Calculate linear trend\"\"\"\n",
    "        x = np.arange(len(ts))\n",
    "        slope, _ = np.polyfit(x, ts, 1)\n",
    "        return slope\n",
    "    \n",
    "    def _calculate_seasonality(self, ts, period=7):\n",
    "        \"\"\"Calculate seasonality strength\"\"\"\n",
    "        if len(ts) < period * 2:\n",
    "            return 0\n",
    "        \n",
    "        # Simple measure: correlation with lagged series\n",
    "        return np.corrcoef(ts[:-period], ts[period:])[0, 1]\n",
    "    \n",
    "    def _calculate_autocorr(self, ts, lag=1):\n",
    "        \"\"\"Calculate autocorrelation\"\"\"\n",
    "        if len(ts) <= lag:\n",
    "            return 0\n",
    "        return np.corrcoef(ts[:-lag], ts[lag:])[0, 1]\n",
    "    \n",
    "    def correlation_matching(self, pivot, treatment_geo, n_matches=5):\n",
    "        \"\"\"\n",
    "        Match based on time series correlation.\n",
    "        \"\"\"\n",
    "        treatment_ts = pivot[treatment_geo].values\n",
    "        \n",
    "        correlations = {}\n",
    "        for geo in pivot.columns:\n",
    "            if geo != treatment_geo:\n",
    "                control_ts = pivot[geo].values\n",
    "                corr = np.corrcoef(treatment_ts, control_ts)[0, 1]\n",
    "                correlations[geo] = corr\n",
    "        \n",
    "        # Sort by correlation (descending)\n",
    "        sorted_geos = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Top N matches\n",
    "        matches = sorted_geos[:n_matches]\n",
    "        \n",
    "        return {\n",
    "            'method': 'correlation',\n",
    "            'matches': matches,\n",
    "            'match_score': np.mean([score for _, score in matches])\n",
    "        }\n",
    "    \n",
    "    def distance_matching(self, features_df, treatment_geo, n_matches=5):\n",
    "        \"\"\"\n",
    "        Match based on feature distance.\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features_df)\n",
    "        features_scaled_df = pd.DataFrame(\n",
    "            features_scaled,\n",
    "            index=features_df.index,\n",
    "            columns=features_df.columns\n",
    "        )\n",
    "        \n",
    "        treatment_features = features_scaled_df.loc[treatment_geo].values\n",
    "        \n",
    "        distances = {}\n",
    "        for geo in features_scaled_df.index:\n",
    "            if geo != treatment_geo:\n",
    "                control_features = features_scaled_df.loc[geo].values\n",
    "                dist = euclidean(treatment_features, control_features)\n",
    "                distances[geo] = dist\n",
    "        \n",
    "        # Sort by distance (ascending)\n",
    "        sorted_geos = sorted(distances.items(), key=lambda x: x[1])\n",
    "        \n",
    "        # Top N matches (convert distance to similarity score)\n",
    "        matches = [(geo, 1 / (1 + dist)) for geo, dist in sorted_geos[:n_matches]]\n",
    "        \n",
    "        return {\n",
    "            'method': 'distance',\n",
    "            'matches': matches,\n",
    "            'match_score': np.mean([score for _, score in matches])\n",
    "        }\n",
    "    \n",
    "    def synthetic_matching(self, pivot, treatment_geo):\n",
    "        \"\"\"\n",
    "        Find optimal weighted combination of control geos.\n",
    "        Uses constrained optimization to minimize pre-period error.\n",
    "        \"\"\"\n",
    "        treatment_ts = pivot[treatment_geo].values\n",
    "        control_geos = [g for g in pivot.columns if g != treatment_geo]\n",
    "        control_data = pivot[control_geos].values\n",
    "        \n",
    "        n_controls = len(control_geos)\n",
    "        \n",
    "        # Objective: minimize squared error\n",
    "        def objective(weights):\n",
    "            synthetic_control = control_data @ weights\n",
    "            return np.sum((treatment_ts - synthetic_control) ** 2)\n",
    "        \n",
    "        # Constraints: weights sum to 1, all non-negative\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "        ]\n",
    "        bounds = [(0, 1) for _ in range(n_controls)]\n",
    "        \n",
    "        # Initial guess: equal weights\n",
    "        x0 = np.ones(n_controls) / n_controls\n",
    "        \n",
    "        # Optimize\n",
    "        result = optimize.minimize(\n",
    "            objective,\n",
    "            x0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints\n",
    "        )\n",
    "        \n",
    "        weights = result.x\n",
    "        \n",
    "        # Get top contributors (weight > 0.01)\n",
    "        significant_controls = [\n",
    "            (control_geos[i], weights[i])\n",
    "            for i in range(n_controls)\n",
    "            if weights[i] > 0.01\n",
    "        ]\n",
    "        \n",
    "        # Calculate match quality (R²)\n",
    "        synthetic_control = control_data @ weights\n",
    "        ss_res = np.sum((treatment_ts - synthetic_control) ** 2)\n",
    "        ss_tot = np.sum((treatment_ts - np.mean(treatment_ts)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return {\n",
    "            'method': 'synthetic',\n",
    "            'matches': significant_controls,\n",
    "            'all_weights': dict(zip(control_geos, weights)),\n",
    "            'match_score': r2\n",
    "        }\n",
    "    \n",
    "    def find_best_matches(self, df, treatment_geos, pre_period_end, metric='revenue'):\n",
    "        \"\"\"\n",
    "        Find best matches for all treatment geos using multiple methods.\n",
    "        \"\"\"\n",
    "        print(f\"Finding matches for {len(treatment_geos)} treatment geos...\\n\")\n",
    "        \n",
    "        # Prepare data\n",
    "        pivot, features_df = self.prepare_features(df, pre_period_end, metric)\n",
    "        \n",
    "        all_matches = {}\n",
    "        \n",
    "        for treatment_geo in tqdm(treatment_geos, desc=\"Matching geos\"):\n",
    "            # Try all methods\n",
    "            corr_match = self.correlation_matching(pivot, treatment_geo, n_matches=5)\n",
    "            dist_match = self.distance_matching(features_df, treatment_geo, n_matches=5)\n",
    "            synth_match = self.synthetic_matching(pivot, treatment_geo)\n",
    "            \n",
    "            # Select best method based on match score\n",
    "            methods = [corr_match, dist_match, synth_match]\n",
    "            best_method = max(methods, key=lambda x: x['match_score'])\n",
    "            \n",
    "            all_matches[treatment_geo] = {\n",
    "                'best_method': best_method['method'],\n",
    "                'match_score': best_method['match_score'],\n",
    "                'matches': best_method['matches'],\n",
    "                'all_methods': {\n",
    "                    'correlation': corr_match,\n",
    "                    'distance': dist_match,\n",
    "                    'synthetic': synth_match\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        self.matches = all_matches\n",
    "        \n",
    "        print(f\"\\n✓ Matching complete\")\n",
    "        self._print_match_summary()\n",
    "        \n",
    "        return all_matches\n",
    "    \n",
    "    def _print_match_summary(self):\n",
    "        \"\"\"Print summary of matching results\"\"\"\n",
    "        print(\"\\nMatch Summary:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for treatment_geo, match_info in self.matches.items():\n",
    "            print(f\"\\n{treatment_geo}:\")\n",
    "            print(f\"  Method: {match_info['best_method']}\")\n",
    "            print(f\"  Match Score: {match_info['match_score']:.4f}\")\n",
    "            print(f\"  Top Matches:\")\n",
    "            for geo, score in match_info['matches'][:3]:\n",
    "                print(f\"    - {geo}: {score:.4f}\")\n",
    "\n",
    "# Test matching\n",
    "matcher = MatchedMarketSelector()\n",
    "\n",
    "treatment_geos = metadata['treatment_geos']\n",
    "pre_period_end = metadata['treatment_start_date'] - timedelta(days=1)\n",
    "\n",
    "matches = matcher.find_best_matches(\n",
    "    geo_data,\n",
    "    treatment_geos,\n",
    "    pre_period_end,\n",
    "    metric='revenue'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Synthetic Control Method at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticControlAnalysis:\n",
    "    \"\"\"\n",
    "    Scalable synthetic control implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def run_synthetic_control(self, df, treatment_geo, control_geos, \n",
    "                             treatment_start, metric='revenue'):\n",
    "        \"\"\"\n",
    "        Run synthetic control analysis for a single treatment geo.\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        pivot = df.pivot(index='date', columns='geo_id', values=metric)\n",
    "        \n",
    "        # Split pre/post periods\n",
    "        pre_period = pivot[pivot.index < treatment_start]\n",
    "        post_period = pivot[pivot.index >= treatment_start]\n",
    "        \n",
    "        # Get treatment time series\n",
    "        treatment_pre = pre_period[treatment_geo].values\n",
    "        treatment_post = post_period[treatment_geo].values\n",
    "        \n",
    "        # Get control time series\n",
    "        control_pre = pre_period[control_geos].values\n",
    "        control_post = post_period[control_geos].values\n",
    "        \n",
    "        # Find optimal weights\n",
    "        weights = self._optimize_weights(treatment_pre, control_pre)\n",
    "        \n",
    "        # Create synthetic control\n",
    "        synthetic_pre = control_pre @ weights\n",
    "        synthetic_post = control_post @ weights\n",
    "        \n",
    "        # Calculate treatment effect\n",
    "        treatment_effect = treatment_post - synthetic_post\n",
    "        \n",
    "        # Statistical inference via placebo tests\n",
    "        p_value, placebo_effects = self._placebo_test(\n",
    "            df, control_geos, treatment_start, metric, treatment_effect\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'treatment_geo': treatment_geo,\n",
    "            'control_geos': control_geos,\n",
    "            'weights': dict(zip(control_geos, weights)),\n",
    "            'treatment_pre': treatment_pre,\n",
    "            'treatment_post': treatment_post,\n",
    "            'synthetic_pre': synthetic_pre,\n",
    "            'synthetic_post': synthetic_post,\n",
    "            'treatment_effect': treatment_effect,\n",
    "            'avg_effect': np.mean(treatment_effect),\n",
    "            'cumulative_effect': np.sum(treatment_effect),\n",
    "            'relative_effect': np.mean(treatment_effect) / np.mean(synthetic_post),\n",
    "            'p_value': p_value,\n",
    "            'pre_period_fit': self._calculate_fit(treatment_pre, synthetic_pre),\n",
    "            'dates': {\n",
    "                'pre': pre_period.index,\n",
    "                'post': post_period.index\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.results[treatment_geo] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _optimize_weights(self, treatment, controls):\n",
    "        \"\"\"\n",
    "        Find optimal weights for synthetic control.\n",
    "        \"\"\"\n",
    "        n_controls = controls.shape[1]\n",
    "        \n",
    "        def objective(weights):\n",
    "            synthetic = controls @ weights\n",
    "            return np.sum((treatment - synthetic) ** 2)\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(n_controls)]\n",
    "        x0 = np.ones(n_controls) / n_controls\n",
    "        \n",
    "        result = optimize.minimize(\n",
    "            objective, x0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints\n",
    "        )\n",
    "        \n",
    "        return result.x\n",
    "    \n",
    "    def _calculate_fit(self, actual, predicted):\n",
    "        \"\"\"\n",
    "        Calculate goodness of fit (R²).\n",
    "        \"\"\"\n",
    "        ss_res = np.sum((actual - predicted) ** 2)\n",
    "        ss_tot = np.sum((actual - np.mean(actual)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    def _placebo_test(self, df, control_geos, treatment_start, metric, true_effect):\n",
    "        \"\"\"\n",
    "        Perform placebo tests for statistical inference.\n",
    "        \"\"\"\n",
    "        placebo_effects = []\n",
    "        \n",
    "        # Run synthetic control on each control geo (as if it were treated)\n",
    "        for placebo_geo in control_geos[:20]:  # Limit for speed\n",
    "            other_controls = [g for g in control_geos if g != placebo_geo]\n",
    "            \n",
    "            pivot = df.pivot(index='date', columns='geo_id', values=metric)\n",
    "            pre_period = pivot[pivot.index < treatment_start]\n",
    "            post_period = pivot[pivot.index >= treatment_start]\n",
    "            \n",
    "            placebo_pre = pre_period[placebo_geo].values\n",
    "            placebo_post = post_period[placebo_geo].values\n",
    "            \n",
    "            control_pre = pre_period[other_controls].values\n",
    "            control_post = post_period[other_controls].values\n",
    "            \n",
    "            weights = self._optimize_weights(placebo_pre, control_pre)\n",
    "            synthetic_post = control_post @ weights\n",
    "            \n",
    "            placebo_effect = placebo_post - synthetic_post\n",
    "            placebo_effects.append(np.mean(placebo_effect))\n",
    "        \n",
    "        # Calculate p-value\n",
    "        true_effect_mean = np.mean(true_effect)\n",
    "        more_extreme = sum(abs(pe) >= abs(true_effect_mean) for pe in placebo_effects)\n",
    "        p_value = (more_extreme + 1) / (len(placebo_effects) + 1)\n",
    "        \n",
    "        return p_value, placebo_effects\n",
    "    \n",
    "    def visualize_result(self, treatment_geo):\n",
    "        \"\"\"\n",
    "        Visualize synthetic control results.\n",
    "        \"\"\"\n",
    "        if treatment_geo not in self.results:\n",
    "            print(f\"No results for {treatment_geo}\")\n",
    "            return\n",
    "        \n",
    "        result = self.results[treatment_geo]\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=(\n",
    "                f\"Actual vs Synthetic Control: {treatment_geo}\",\n",
    "                \"Treatment Effect Over Time\"\n",
    "            ),\n",
    "            vertical_spacing=0.15\n",
    "        )\n",
    "        \n",
    "        # Combine dates\n",
    "        all_dates = pd.concat([\n",
    "            pd.Series(result['dates']['pre']),\n",
    "            pd.Series(result['dates']['post'])\n",
    "        ])\n",
    "        \n",
    "        # Combine values\n",
    "        actual_values = np.concatenate([result['treatment_pre'], result['treatment_post']])\n",
    "        synthetic_values = np.concatenate([result['synthetic_pre'], result['synthetic_post']])\n",
    "        \n",
    "        # Plot 1: Actual vs Synthetic\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=all_dates, y=actual_values, name=\"Actual\",\n",
    "                      line=dict(color='blue', width=2)),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=all_dates, y=synthetic_values, name=\"Synthetic Control\",\n",
    "                      line=dict(color='red', width=2, dash='dash')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add vertical line at treatment start\n",
    "        treatment_start = result['dates']['post'][0]\n",
    "        fig.add_vline(x=treatment_start, line_dash=\"solid\", line_color=\"green\",\n",
    "                     annotation_text=\"Treatment Start\", row=1, col=1)\n",
    "        \n",
    "        # Plot 2: Treatment Effect\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=result['dates']['post'], y=result['treatment_effect'],\n",
    "                      name=\"Treatment Effect\", fill='tozeroy',\n",
    "                      line=dict(color='green', width=2)),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_hline(y=0, line_dash=\"dot\", line_color=\"gray\", row=2, col=1)\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Metric Value\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Treatment Effect\", row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(height=800, showlegend=True)\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\nSynthetic Control Results: {treatment_geo}\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Pre-period fit (R²): {result['pre_period_fit']:.4f}\")\n",
    "        print(f\"Average treatment effect: {result['avg_effect']:,.2f}\")\n",
    "        print(f\"Cumulative treatment effect: {result['cumulative_effect']:,.2f}\")\n",
    "        print(f\"Relative lift: {result['relative_effect']:.2%}\")\n",
    "        print(f\"P-value: {result['p_value']:.4f}\")\n",
    "        print(f\"Significant: {'Yes' if result['p_value'] < 0.05 else 'No'}\")\n",
    "\n",
    "# Test synthetic control\n",
    "sc_analysis = SyntheticControlAnalysis()\n",
    "\n",
    "# Run for first treatment geo\n",
    "test_treatment_geo = treatment_geos[0]\n",
    "test_controls = [g for g in geo_data['geo_id'].unique() if g not in treatment_geos]\n",
    "\n",
    "result = sc_analysis.run_synthetic_control(\n",
    "    geo_data,\n",
    "    test_treatment_geo,\n",
    "    test_controls[:20],  # Use subset for speed\n",
    "    metadata['treatment_start_date'],\n",
    "    metric='revenue'\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "sc_analysis.visualize_result(test_treatment_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Causal Impact Analysis at Scale\n",
    "\n",
    "### Using Bayesian Structural Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalableCausalImpact:\n",
    "    \"\"\"\n",
    "    Scalable causal impact analysis using CausalImpact library.\n",
    "    Handles multiple treatment geos in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_jobs=-1):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.results = {}\n",
    "        \n",
    "    def run_causal_impact(self, df, treatment_geo, control_geos,\n",
    "                         pre_period, post_period, metric='revenue'):\n",
    "        \"\"\"\n",
    "        Run causal impact analysis for a single geo.\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        pivot = df.pivot(index='date', columns='geo_id', values=metric)\n",
    "        \n",
    "        # Create DataFrame with treatment and controls\n",
    "        data = pd.DataFrame({\n",
    "            'y': pivot[treatment_geo],\n",
    "        })\n",
    "        \n",
    "        # Add control geos as covariates\n",
    "        for i, control_geo in enumerate(control_geos[:10]):  # Limit covariates\n",
    "            data[f'x{i+1}'] = pivot[control_geo]\n",
    "        \n",
    "        # Remove any NaN values\n",
    "        data = data.dropna()\n",
    "        \n",
    "        # Run CausalImpact\n",
    "        try:\n",
    "            ci = CausalImpact(data, pre_period, post_period)\n",
    "            \n",
    "            # Extract summary\n",
    "            summary = ci.summary(output='report')\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'treatment_geo': treatment_geo,\n",
    "                'ci_object': ci,\n",
    "                'summary': ci.summary(),\n",
    "                'report': summary,\n",
    "                'absolute_effect': ci.summary()['abs_effect'][0],\n",
    "                'relative_effect': ci.summary()['rel_effect'][0],\n",
    "                'p_value': ci.p_value,\n",
    "                'significant': ci.p_value < 0.05\n",
    "            }\n",
    "            \n",
    "            self.results[treatment_geo] = result\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error running CausalImpact for {treatment_geo}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_parallel(self, df, treatment_geos, control_geos,\n",
    "                    pre_period, post_period, metric='revenue'):\n",
    "        \"\"\"\n",
    "        Run causal impact for multiple geos in parallel.\n",
    "        \"\"\"\n",
    "        print(f\"Running Causal Impact for {len(treatment_geos)} geos in parallel...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self.run_causal_impact)(\n",
    "                df, treatment_geo, control_geos, pre_period, post_period, metric\n",
    "            )\n",
    "            for treatment_geo in tqdm(treatment_geos, desc=\"Processing geos\")\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        for result in results:\n",
    "            if result:\n",
    "                self.results[result['treatment_geo']] = result\n",
    "        \n",
    "        print(f\"\\n✓ Completed {len(self.results)} analyses\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def get_summary_table(self):\n",
    "        \"\"\"\n",
    "        Create summary table of all results.\n",
    "        \"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for geo, result in self.results.items():\n",
    "            summary_data.append({\n",
    "                'geo': geo,\n",
    "                'absolute_effect': result['absolute_effect'],\n",
    "                'relative_effect': result['relative_effect'],\n",
    "                'p_value': result['p_value'],\n",
    "                'significant': result['significant']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data).sort_values('absolute_effect', ascending=False)\n",
    "    \n",
    "    def visualize_result(self, treatment_geo):\n",
    "        \"\"\"\n",
    "        Visualize causal impact results.\n",
    "        \"\"\"\n",
    "        if treatment_geo not in self.results:\n",
    "            print(f\"No results for {treatment_geo}\")\n",
    "            return\n",
    "        \n",
    "        result = self.results[treatment_geo]\n",
    "        ci = result['ci_object']\n",
    "        \n",
    "        # Use built-in plot\n",
    "        ci.plot()\n",
    "        \n",
    "        # Print report\n",
    "        print(f\"\\nCausal Impact Report: {treatment_geo}\")\n",
    "        print(\"=\" * 70)\n",
    "        print(result['report'])\n",
    "\n",
    "print(\"✓ Scalable Causal Impact configured\")\n",
    "print(\"  Ready for large-scale incrementality testing\")\n",
    "print(\"  Supports parallel processing across multiple geos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-World Project: Enterprise Incrementality Platform\n",
    "\n",
    "### Complete End-to-End System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseIncrementalityPlatform:\n",
    "    \"\"\"\n",
    "    Production-ready incrementality measurement platform.\n",
    "    Integrates:\n",
    "    - Automated market matching\n",
    "    - Synthetic control\n",
    "    - Causal impact\n",
    "    - Geo-lift analysis\n",
    "    - Continuous monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rs_connection=None):\n",
    "        self.rs = rs_connection\n",
    "        self.matcher = MatchedMarketSelector()\n",
    "        self.sc_analyzer = SyntheticControlAnalysis()\n",
    "        self.ci_analyzer = ScalableCausalImpact()\n",
    "        self.experiments = {}\n",
    "        \n",
    "    def setup_experiment(self, experiment_config):\n",
    "        \"\"\"\n",
    "        Setup a new incrementality experiment.\n",
    "        \n",
    "        experiment_config should include:\n",
    "        - experiment_id\n",
    "        - treatment_geos\n",
    "        - start_date\n",
    "        - end_date\n",
    "        - metric\n",
    "        \"\"\"\n",
    "        experiment_id = experiment_config['experiment_id']\n",
    "        \n",
    "        print(f\"\\nSetting up experiment: {experiment_id}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.experiments[experiment_id] = {\n",
    "            'config': experiment_config,\n",
    "            'status': 'setup',\n",
    "            'created_at': datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Save to Redshift if connected\n",
    "        if self.rs:\n",
    "            self.rs.save_experiment(experiment_config)\n",
    "        \n",
    "        print(f\"✓ Experiment {experiment_id} configured\")\n",
    "        \n",
    "        return experiment_id\n",
    "    \n",
    "    def run_complete_analysis(self, experiment_id, df):\n",
    "        \"\"\"\n",
    "        Run complete incrementality analysis pipeline.\n",
    "        \"\"\"\n",
    "        if experiment_id not in self.experiments:\n",
    "            print(f\"Experiment {experiment_id} not found\")\n",
    "            return\n",
    "        \n",
    "        config = self.experiments[experiment_id]['config']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"RUNNING INCREMENTALITY ANALYSIS: {experiment_id}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        treatment_geos = config['treatment_geos']\n",
    "        treatment_start = config['start_date']\n",
    "        treatment_end = config['end_date']\n",
    "        metric = config.get('metric', 'revenue')\n",
    "        \n",
    "        # Step 1: Market Matching\n",
    "        print(\"\\n[1/4] Finding matched control markets...\")\n",
    "        pre_period_end = treatment_start - timedelta(days=1)\n",
    "        matches = self.matcher.find_best_matches(\n",
    "            df, treatment_geos, pre_period_end, metric\n",
    "        )\n",
    "        \n",
    "        # Get all control geos\n",
    "        all_control_geos = set()\n",
    "        for match_info in matches.values():\n",
    "            if match_info['best_method'] == 'synthetic':\n",
    "                all_control_geos.update([g for g, _ in match_info['matches']])\n",
    "            else:\n",
    "                all_control_geos.update([g for g, _ in match_info['matches']])\n",
    "        \n",
    "        all_control_geos = list(all_control_geos)\n",
    "        \n",
    "        # Step 2: Synthetic Control Analysis\n",
    "        print(f\"\\n[2/4] Running Synthetic Control...\")\n",
    "        sc_results = {}\n",
    "        for treatment_geo in tqdm(treatment_geos, desc=\"SC Analysis\"):\n",
    "            result = self.sc_analyzer.run_synthetic_control(\n",
    "                df, treatment_geo, all_control_geos,\n",
    "                treatment_start, metric\n",
    "            )\n",
    "            sc_results[treatment_geo] = result\n",
    "        \n",
    "        # Step 3: Causal Impact Analysis\n",
    "        print(f\"\\n[3/4] Running Causal Impact...\")\n",
    "        \n",
    "        # Define pre and post periods for CausalImpact\n",
    "        all_dates = sorted(df['date'].unique())\n",
    "        treatment_idx = all_dates.index(treatment_start)\n",
    "        \n",
    "        pre_period = [all_dates[0], all_dates[treatment_idx - 1]]\n",
    "        post_period = [all_dates[treatment_idx], all_dates[-1]]\n",
    "        \n",
    "        ci_results = self.ci_analyzer.run_parallel(\n",
    "            df, treatment_geos[:3], all_control_geos,  # Limit for demo\n",
    "            pre_period, post_period, metric\n",
    "        )\n",
    "        \n",
    "        # Step 4: Aggregate Results\n",
    "        print(f\"\\n[4/4] Aggregating results...\")\n",
    "        aggregated = self._aggregate_results(sc_results, ci_results, metric)\n",
    "        \n",
    "        # Store results\n",
    "        self.experiments[experiment_id]['results'] = {\n",
    "            'matches': matches,\n",
    "            'synthetic_control': sc_results,\n",
    "            'causal_impact': ci_results,\n",
    "            'aggregated': aggregated\n",
    "        }\n",
    "        self.experiments[experiment_id]['status'] = 'completed'\n",
    "        self.experiments[experiment_id]['completed_at'] = datetime.now()\n",
    "        \n",
    "        # Save to Redshift\n",
    "        if self.rs:\n",
    "            self.rs.save_results(aggregated, experiment_id)\n",
    "        \n",
    "        print(f\"\\n✓ Analysis complete for {experiment_id}\")\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _aggregate_results(self, sc_results, ci_results, metric):\n",
    "        \"\"\"\n",
    "        Aggregate results from different methods.\n",
    "        \"\"\"\n",
    "        aggregated = []\n",
    "        \n",
    "        for geo in sc_results.keys():\n",
    "            sc_result = sc_results[geo]\n",
    "            \n",
    "            row = {\n",
    "                'geo': geo,\n",
    "                'metric_name': metric,\n",
    "                'sc_avg_effect': sc_result['avg_effect'],\n",
    "                'sc_cumulative_effect': sc_result['cumulative_effect'],\n",
    "                'sc_relative_effect': sc_result['relative_effect'],\n",
    "                'sc_p_value': sc_result['p_value'],\n",
    "                'sc_significant': sc_result['p_value'] < 0.05\n",
    "            }\n",
    "            \n",
    "            # Add CI results if available\n",
    "            if geo in ci_results:\n",
    "                ci_result = ci_results[geo]\n",
    "                row['ci_absolute_effect'] = ci_result['absolute_effect']\n",
    "                row['ci_relative_effect'] = ci_result['relative_effect']\n",
    "                row['ci_p_value'] = ci_result['p_value']\n",
    "                row['ci_significant'] = ci_result['significant']\n",
    "            \n",
    "            aggregated.append(row)\n",
    "        \n",
    "        return pd.DataFrame(aggregated)\n",
    "    \n",
    "    def generate_report(self, experiment_id):\n",
    "        \"\"\"\n",
    "        Generate comprehensive experiment report.\n",
    "        \"\"\"\n",
    "        if experiment_id not in self.experiments:\n",
    "            print(f\"Experiment {experiment_id} not found\")\n",
    "            return\n",
    "        \n",
    "        experiment = self.experiments[experiment_id]\n",
    "        \n",
    "        if experiment['status'] != 'completed':\n",
    "            print(f\"Experiment {experiment_id} not completed\")\n",
    "            return\n",
    "        \n",
    "        config = experiment['config']\n",
    "        results = experiment['results']['aggregated']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"INCREMENTALITY EXPERIMENT REPORT\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\"\\nExperiment ID: {experiment_id}\")\n",
    "        print(f\"Treatment Period: {config['start_date']} to {config['end_date']}\")\n",
    "        print(f\"Treatment Geos: {len(config['treatment_geos'])}\")\n",
    "        print(f\"Metric: {config.get('metric', 'revenue')}\")\n",
    "        \n",
    "        print(f\"\\nOverall Results:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_sc_effect = results['sc_cumulative_effect'].sum()\n",
    "        avg_relative_lift = results['sc_relative_effect'].mean()\n",
    "        n_significant = results['sc_significant'].sum()\n",
    "        \n",
    "        print(f\"Total Incremental {config.get('metric', 'revenue').title()}: {total_sc_effect:,.2f}\")\n",
    "        print(f\"Average Relative Lift: {avg_relative_lift:.2%}\")\n",
    "        print(f\"Significant Results: {n_significant}/{len(results)} geos\")\n",
    "        \n",
    "        print(f\"\\nDetailed Results by Geo:\")\n",
    "        print(results.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_dashboard(self, experiment_id):\n",
    "        \"\"\"\n",
    "        Create interactive dashboard for results.\n",
    "        \"\"\"\n",
    "        if experiment_id not in self.experiments:\n",
    "            print(f\"Experiment {experiment_id} not found\")\n",
    "            return\n",
    "        \n",
    "        results = self.experiments[experiment_id]['results']\n",
    "        aggregated = results['aggregated']\n",
    "        \n",
    "        # Create subplot figure\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Cumulative Effect by Geo',\n",
    "                'Relative Lift Distribution',\n",
    "                'Statistical Significance',\n",
    "                'Effect Comparison: SC vs CI'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Plot 1: Cumulative effect\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=aggregated['geo'], y=aggregated['sc_cumulative_effect'],\n",
    "                  name='Cumulative Effect'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 2: Relative lift histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=aggregated['sc_relative_effect'] * 100,\n",
    "                        name='Relative Lift (%)', nbinsx=20),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Plot 3: Significance\n",
    "        sig_counts = aggregated['sc_significant'].value_counts()\n",
    "        fig.add_trace(\n",
    "            go.Pie(labels=['Not Significant', 'Significant'],\n",
    "                  values=[sig_counts.get(False, 0), sig_counts.get(True, 0)]),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 4: Method comparison (if CI results available)\n",
    "        if 'ci_absolute_effect' in aggregated.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=aggregated['sc_avg_effect'],\n",
    "                          y=aggregated['ci_absolute_effect'],\n",
    "                          mode='markers',\n",
    "                          name='SC vs CI',\n",
    "                          text=aggregated['geo']),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            # Add diagonal line\n",
    "            max_val = max(aggregated['sc_avg_effect'].max(),\n",
    "                         aggregated['ci_absolute_effect'].max())\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[0, max_val], y=[0, max_val],\n",
    "                          mode='lines', line=dict(dash='dash', color='gray'),\n",
    "                          showlegend=False),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=800, showlegend=True,\n",
    "                         title_text=f\"Incrementality Dashboard: {experiment_id}\")\n",
    "        fig.show()\n",
    "\n",
    "print(\"✓ Enterprise Incrementality Platform configured\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - Automated market matching\")\n",
    "print(\"  - Synthetic control analysis\")\n",
    "print(\"  - Causal impact with Bayesian methods\")\n",
    "print(\"  - Parallel processing for scale\")\n",
    "print(\"  - Comprehensive reporting and dashboards\")\n",
    "print(\"  - Redshift integration for data/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Complete Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize platform\n",
    "platform = EnterpriseIncrementalityPlatform()\n",
    "\n",
    "# Setup experiment\n",
    "experiment_config = {\n",
    "    'experiment_id': 'test_experiment_001',\n",
    "    'experiment_name': 'Q4 Marketing Incrementality Test',\n",
    "    'treatment_geos': metadata['treatment_geos'][:3],  # Limit for demo\n",
    "    'start_date': metadata['treatment_start_date'],\n",
    "    'end_date': geo_data['date'].max(),\n",
    "    'metric': 'revenue',\n",
    "    'experiment_type': 'geo_lift'\n",
    "}\n",
    "\n",
    "experiment_id = platform.setup_experiment(experiment_config)\n",
    "\n",
    "# Run complete analysis\n",
    "results = platform.run_complete_analysis(experiment_id, geo_data)\n",
    "\n",
    "# Generate report\n",
    "report = platform.generate_report(experiment_id)\n",
    "\n",
    "# Create dashboard\n",
    "platform.create_dashboard(experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Deployment Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Experiment Design**\n",
    "   - Pre-register experiments and hypotheses\n",
    "   - Ensure sufficient pre-period data (minimum 4 weeks)\n",
    "   - Validate matched markets before treatment\n",
    "   - Document all design decisions\n",
    "\n",
    "2. **Analysis Methods**\n",
    "   - Use multiple methods (Synthetic Control + Causal Impact)\n",
    "   - Always validate pre-period fit (R² > 0.7)\n",
    "   - Conduct placebo tests for inference\n",
    "   - Account for multiple testing if running many experiments\n",
    "\n",
    "3. **Scale and Performance**\n",
    "   - Parallelize across geos for speed\n",
    "   - Cache matched markets for reuse\n",
    "   - Use Redshift for data storage and aggregation\n",
    "   - Monitor computation costs\n",
    "\n",
    "4. **Production Systems**\n",
    "   - Automate experiment setup and monitoring\n",
    "   - Build real-time dashboards for stakeholders\n",
    "   - Version control experiment configurations\n",
    "   - Implement alerting for anomalies\n",
    "\n",
    "### Next Steps\n",
    "- Integrate with experimentation platform\n",
    "- Build automated reporting pipelines\n",
    "- Create self-service experiment setup UI\n",
    "- Implement continuous incrementality monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
