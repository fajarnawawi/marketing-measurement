{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Advanced A/B Testing Platform\n",
    "\n",
    "## Overview\n",
    "Build a production-ready, scalable A/B testing platform using Redshift, Python, and modern statistical methods.\n",
    "\n",
    "## Learning Objectives\n",
    "- Build scalable A/B testing infrastructure\n",
    "- Implement multi-variant testing at scale\n",
    "- Design sequential testing systems\n",
    "- Create automated monitoring and alerting\n",
    "- Generate automated reports\n",
    "- Deploy production A/B testing pipelines\n",
    "\n",
    "## Prerequisites\n",
    "- Redshift cluster access\n",
    "- Strong statistics background\n",
    "- Understanding of A/B testing principles\n",
    "- Python programming skills\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Environment](#setup)\n",
    "2. [A/B Testing Platform Architecture](#architecture)\n",
    "3. [Experiment Configuration System](#config)\n",
    "4. [Multi-Variant Testing at Scale](#multivariant)\n",
    "5. [Sequential Testing Implementation](#sequential)\n",
    "6. [Automated Monitoring System](#monitoring)\n",
    "7. [Automated Reporting](#reporting)\n",
    "8. [Redshift-Based Analytics](#analytics)\n",
    "9. [Real-World Project: Production Platform](#project)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy scipy statsmodels\n",
    "!pip install -q redshift_connector sqlalchemy\n",
    "!pip install -q plotly dash dash-bootstrap-components\n",
    "!pip install -q pyyaml python-dateutil\n",
    "!pip install -q apscheduler  # For scheduling\n",
    "!pip install -q jinja2  # For report templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, beta\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "import redshift_connector\n",
    "import yaml\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', '5439')),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing_db'),\n",
    "    'user': os.getenv('REDSHIFT_USER', input('Redshift username: ')),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', getpass('Redshift password: '))\n",
    "}\n",
    "\n",
    "class DatabaseConnection:\n",
    "    \"\"\"Managed database connection\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.conn = None\n",
    "    \n",
    "    def connect(self):\n",
    "        if not self.conn:\n",
    "            self.conn = redshift_connector.connect(**self.config)\n",
    "        return self.conn\n",
    "    \n",
    "    def query(self, sql, params=None):\n",
    "        \"\"\"Execute query and return DataFrame\"\"\"\n",
    "        conn = self.connect()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if params:\n",
    "            cursor.execute(sql, params)\n",
    "        else:\n",
    "            cursor.execute(sql)\n",
    "        \n",
    "        result = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        return pd.DataFrame(result, columns=columns)\n",
    "    \n",
    "    def execute(self, sql, params=None):\n",
    "        \"\"\"Execute non-query SQL\"\"\"\n",
    "        conn = self.connect()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if params:\n",
    "            cursor.execute(sql, params)\n",
    "        else:\n",
    "            cursor.execute(sql)\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "\n",
    "db = DatabaseConnection(REDSHIFT_CONFIG)\n",
    "print(\"✓ Database connection initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A/B Testing Platform Architecture <a name=\"architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                  A/B Testing Platform                   │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  1. Experiment Configuration                           │\n",
    "│     - Metadata storage                                 │\n",
    "│     - Variant definitions                              │\n",
    "│     - Success metrics                                  │\n",
    "│                                                         │\n",
    "│  2. Data Collection                                    │\n",
    "│     - Event tracking                                   │\n",
    "│     - Assignment logging                               │\n",
    "│     - Metric calculation                               │\n",
    "│                                                         │\n",
    "│  3. Statistical Analysis                               │\n",
    "│     - Frequentist tests                                │\n",
    "│     - Bayesian analysis                                │\n",
    "│     - Sequential testing                               │\n",
    "│                                                         │\n",
    "│  4. Monitoring & Alerting                              │\n",
    "│     - Real-time dashboards                             │\n",
    "│     - Anomaly detection                                │\n",
    "│     - Automated alerts                                 │\n",
    "│                                                         │\n",
    "│  5. Reporting                                          │\n",
    "│     - Automated reports                                │\n",
    "│     - Visualizations                                   │\n",
    "│     - Decision recommendations                         │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ab_testing_schema(db_conn):\n",
    "    \"\"\"\n",
    "    Create database schema for A/B testing platform\n",
    "    \"\"\"\n",
    "    \n",
    "    # Experiments table\n",
    "    db_conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS experiments (\n",
    "        experiment_id VARCHAR(255) PRIMARY KEY,\n",
    "        name VARCHAR(500),\n",
    "        description TEXT,\n",
    "        hypothesis TEXT,\n",
    "        status VARCHAR(50),\n",
    "        start_date TIMESTAMP,\n",
    "        end_date TIMESTAMP,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        created_by VARCHAR(255),\n",
    "        primary_metric VARCHAR(255),\n",
    "        secondary_metrics TEXT,  -- JSON array\n",
    "        target_sample_size INTEGER,\n",
    "        min_detectable_effect FLOAT,\n",
    "        confidence_level FLOAT DEFAULT 0.95,\n",
    "        power FLOAT DEFAULT 0.80\n",
    "    )\n",
    "    DISTSTYLE ALL\n",
    "    \"\"\")\n",
    "    \n",
    "    # Variants table\n",
    "    db_conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS experiment_variants (\n",
    "        variant_id VARCHAR(255) PRIMARY KEY,\n",
    "        experiment_id VARCHAR(255),\n",
    "        variant_name VARCHAR(255),\n",
    "        traffic_allocation FLOAT,\n",
    "        is_control BOOLEAN,\n",
    "        configuration TEXT,  -- JSON\n",
    "        FOREIGN KEY (experiment_id) REFERENCES experiments(experiment_id)\n",
    "    )\n",
    "    DISTSTYLE ALL\n",
    "    \"\"\")\n",
    "    \n",
    "    # Assignments table\n",
    "    db_conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS experiment_assignments (\n",
    "        assignment_id BIGINT IDENTITY(1,1),\n",
    "        experiment_id VARCHAR(255),\n",
    "        variant_id VARCHAR(255),\n",
    "        user_id VARCHAR(255),\n",
    "        session_id VARCHAR(255),\n",
    "        assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (assignment_id),\n",
    "        FOREIGN KEY (experiment_id) REFERENCES experiments(experiment_id),\n",
    "        FOREIGN KEY (variant_id) REFERENCES experiment_variants(variant_id)\n",
    "    )\n",
    "    DISTKEY (user_id)\n",
    "    SORTKEY (assigned_at, experiment_id)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Events table\n",
    "    db_conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS experiment_events (\n",
    "        event_id BIGINT IDENTITY(1,1),\n",
    "        experiment_id VARCHAR(255),\n",
    "        variant_id VARCHAR(255),\n",
    "        user_id VARCHAR(255),\n",
    "        session_id VARCHAR(255),\n",
    "        event_type VARCHAR(255),\n",
    "        event_value FLOAT,\n",
    "        event_metadata TEXT,  -- JSON\n",
    "        occurred_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        PRIMARY KEY (event_id)\n",
    "    )\n",
    "    DISTKEY (user_id)\n",
    "    SORTKEY (occurred_at, experiment_id)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Results table (cached analysis results)\n",
    "    db_conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS experiment_results (\n",
    "        result_id BIGINT IDENTITY(1,1),\n",
    "        experiment_id VARCHAR(255),\n",
    "        analysis_date DATE,\n",
    "        variant_id VARCHAR(255),\n",
    "        metric_name VARCHAR(255),\n",
    "        sample_size INTEGER,\n",
    "        mean_value FLOAT,\n",
    "        std_dev FLOAT,\n",
    "        ci_lower FLOAT,\n",
    "        ci_upper FLOAT,\n",
    "        PRIMARY KEY (result_id)\n",
    "    )\n",
    "    DISTSTYLE ALL\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"✓ A/B testing schema created\")\n",
    "\n",
    "# create_ab_testing_schema(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration System <a name=\"config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentStatus(Enum):\n",
    "    \"\"\"Experiment lifecycle states\"\"\"\n",
    "    DRAFT = \"draft\"\n",
    "    ACTIVE = \"active\"\n",
    "    PAUSED = \"paused\"\n",
    "    COMPLETED = \"completed\"\n",
    "    ARCHIVED = \"archived\"\n",
    "\n",
    "@dataclass\n",
    "class Variant:\n",
    "    \"\"\"Experiment variant configuration\"\"\"\n",
    "    variant_id: str\n",
    "    name: str\n",
    "    traffic_allocation: float\n",
    "    is_control: bool = False\n",
    "    configuration: Dict = field(default_factory=dict)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not 0 <= self.traffic_allocation <= 1:\n",
    "            raise ValueError(\"Traffic allocation must be between 0 and 1\")\n",
    "\n",
    "@dataclass\n",
    "class Metric:\n",
    "    \"\"\"Experiment metric definition\"\"\"\n",
    "    name: str\n",
    "    metric_type: str  # 'conversion', 'revenue', 'engagement'\n",
    "    aggregation: str  # 'mean', 'sum', 'rate'\n",
    "    is_primary: bool = False\n",
    "    \n",
    "@dataclass\n",
    "class Experiment:\n",
    "    \"\"\"Complete experiment configuration\"\"\"\n",
    "    experiment_id: str\n",
    "    name: str\n",
    "    hypothesis: str\n",
    "    variants: List[Variant]\n",
    "    primary_metric: Metric\n",
    "    secondary_metrics: List[Metric] = field(default_factory=list)\n",
    "    status: ExperimentStatus = ExperimentStatus.DRAFT\n",
    "    start_date: Optional[datetime] = None\n",
    "    end_date: Optional[datetime] = None\n",
    "    target_sample_size: int = 10000\n",
    "    min_detectable_effect: float = 0.05\n",
    "    confidence_level: float = 0.95\n",
    "    power: float = 0.80\n",
    "    created_by: str = \"system\"\n",
    "    description: str = \"\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Validate traffic allocation sums to 1\n",
    "        total_traffic = sum(v.traffic_allocation for v in self.variants)\n",
    "        if not np.isclose(total_traffic, 1.0):\n",
    "            raise ValueError(f\"Traffic allocations must sum to 1, got {total_traffic}\")\n",
    "        \n",
    "        # Ensure exactly one control\n",
    "        controls = sum(1 for v in self.variants if v.is_control)\n",
    "        if controls != 1:\n",
    "            raise ValueError(\"Must have exactly one control variant\")\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary for storage\"\"\"\n",
    "        return {\n",
    "            'experiment_id': self.experiment_id,\n",
    "            'name': self.name,\n",
    "            'description': self.description,\n",
    "            'hypothesis': self.hypothesis,\n",
    "            'status': self.status.value,\n",
    "            'start_date': self.start_date,\n",
    "            'end_date': self.end_date,\n",
    "            'created_by': self.created_by,\n",
    "            'primary_metric': self.primary_metric.name,\n",
    "            'secondary_metrics': json.dumps([m.name for m in self.secondary_metrics]),\n",
    "            'target_sample_size': self.target_sample_size,\n",
    "            'min_detectable_effect': self.min_detectable_effect,\n",
    "            'confidence_level': self.confidence_level,\n",
    "            'power': self.power\n",
    "        }\n",
    "\n",
    "class ExperimentManager:\n",
    "    \"\"\"\n",
    "    Manages experiment lifecycle and configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_conn):\n",
    "        self.db = db_conn\n",
    "    \n",
    "    def create_experiment(self, experiment: Experiment) -> str:\n",
    "        \"\"\"\n",
    "        Create new experiment in database\n",
    "        \"\"\"\n",
    "        # Insert experiment\n",
    "        exp_data = experiment.to_dict()\n",
    "        \n",
    "        cols = ', '.join(exp_data.keys())\n",
    "        placeholders = ', '.join(['%s'] * len(exp_data))\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO experiments ({cols})\n",
    "        VALUES ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        self.db.execute(sql, list(exp_data.values()))\n",
    "        \n",
    "        # Insert variants\n",
    "        for variant in experiment.variants:\n",
    "            self.db.execute(\"\"\"\n",
    "            INSERT INTO experiment_variants \n",
    "            (variant_id, experiment_id, variant_name, traffic_allocation, \n",
    "             is_control, configuration)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                variant.variant_id,\n",
    "                experiment.experiment_id,\n",
    "                variant.name,\n",
    "                variant.traffic_allocation,\n",
    "                variant.is_control,\n",
    "                json.dumps(variant.configuration)\n",
    "            ))\n",
    "        \n",
    "        print(f\"✓ Experiment {experiment.experiment_id} created\")\n",
    "        return experiment.experiment_id\n",
    "    \n",
    "    def get_experiment(self, experiment_id: str) -> Experiment:\n",
    "        \"\"\"\n",
    "        Load experiment configuration\n",
    "        \"\"\"\n",
    "        # Load experiment\n",
    "        exp_df = self.db.query(\"\"\"\n",
    "        SELECT * FROM experiments WHERE experiment_id = %s\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        if exp_df.empty:\n",
    "            raise ValueError(f\"Experiment {experiment_id} not found\")\n",
    "        \n",
    "        # Load variants\n",
    "        var_df = self.db.query(\"\"\"\n",
    "        SELECT * FROM experiment_variants WHERE experiment_id = %s\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        # Reconstruct experiment object\n",
    "        # (Implementation details omitted for brevity)\n",
    "        \n",
    "        return None  # Would return reconstructed Experiment object\n",
    "    \n",
    "    def start_experiment(self, experiment_id: str):\n",
    "        \"\"\"\n",
    "        Activate experiment\n",
    "        \"\"\"\n",
    "        self.db.execute(\"\"\"\n",
    "        UPDATE experiments\n",
    "        SET status = %s, start_date = CURRENT_TIMESTAMP\n",
    "        WHERE experiment_id = %s\n",
    "        \"\"\", (ExperimentStatus.ACTIVE.value, experiment_id))\n",
    "        \n",
    "        print(f\"✓ Experiment {experiment_id} started\")\n",
    "    \n",
    "    def stop_experiment(self, experiment_id: str):\n",
    "        \"\"\"\n",
    "        Complete experiment\n",
    "        \"\"\"\n",
    "        self.db.execute(\"\"\"\n",
    "        UPDATE experiments\n",
    "        SET status = %s, end_date = CURRENT_TIMESTAMP\n",
    "        WHERE experiment_id = %s\n",
    "        \"\"\", (ExperimentStatus.COMPLETED.value, experiment_id))\n",
    "        \n",
    "        print(f\"✓ Experiment {experiment_id} stopped\")\n",
    "    \n",
    "    def list_active_experiments(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get all active experiments\n",
    "        \"\"\"\n",
    "        return self.db.query(\"\"\"\n",
    "        SELECT \n",
    "            experiment_id,\n",
    "            name,\n",
    "            status,\n",
    "            start_date,\n",
    "            primary_metric,\n",
    "            target_sample_size\n",
    "        FROM experiments\n",
    "        WHERE status = 'active'\n",
    "        ORDER BY start_date DESC\n",
    "        \"\"\")\n",
    "\n",
    "# Example usage\n",
    "# manager = ExperimentManager(db)\n",
    "# \n",
    "# # Create experiment\n",
    "# experiment = Experiment(\n",
    "#     experiment_id='exp_001',\n",
    "#     name='Homepage CTA Test',\n",
    "#     hypothesis='Changing CTA color to green will increase conversions',\n",
    "#     variants=[\n",
    "#         Variant('control', 'Blue CTA (Control)', 0.5, is_control=True),\n",
    "#         Variant('treatment', 'Green CTA', 0.5)\n",
    "#     ],\n",
    "#     primary_metric=Metric('conversion_rate', 'conversion', 'rate', is_primary=True),\n",
    "#     target_sample_size=10000,\n",
    "#     min_detectable_effect=0.05\n",
    "# )\n",
    "# \n",
    "# manager.create_experiment(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Variant Testing at Scale <a name=\"multivariant\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVariantAnalyzer:\n",
    "    \"\"\"\n",
    "    Statistical analysis for multi-variant experiments\n",
    "    Handles A/B/n testing with multiple comparisons correction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_conn):\n",
    "        self.db = db_conn\n",
    "    \n",
    "    def get_variant_metrics(self, experiment_id: str, metric_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get aggregated metrics for all variants\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        WITH variant_data AS (\n",
    "            SELECT \n",
    "                ev.variant_id,\n",
    "                v.variant_name,\n",
    "                v.is_control,\n",
    "                COUNT(DISTINCT ea.user_id) as users,\n",
    "                COUNT(*) as events,\n",
    "                AVG(ev.event_value) as mean_value,\n",
    "                STDDEV(ev.event_value) as std_value,\n",
    "                SUM(ev.event_value) as total_value\n",
    "            FROM experiment_events ev\n",
    "            JOIN experiment_assignments ea \n",
    "                ON ev.experiment_id = ea.experiment_id \n",
    "                AND ev.user_id = ea.user_id\n",
    "            JOIN experiment_variants v\n",
    "                ON ev.variant_id = v.variant_id\n",
    "            WHERE ev.experiment_id = %s\n",
    "                AND ev.event_type = %s\n",
    "            GROUP BY ev.variant_id, v.variant_name, v.is_control\n",
    "        )\n",
    "        SELECT * FROM variant_data\n",
    "        ORDER BY is_control DESC, variant_name\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.db.query(query, (experiment_id, metric_name))\n",
    "    \n",
    "    def pairwise_comparisons(self, experiment_id: str, metric_name: str,\n",
    "                            correction='bonferroni') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform pairwise comparisons between all variants\n",
    "        \n",
    "        Args:\n",
    "            correction: 'bonferroni', 'holm', or 'none'\n",
    "        \"\"\"\n",
    "        # Get variant metrics\n",
    "        df = self.get_variant_metrics(experiment_id, metric_name)\n",
    "        \n",
    "        # Identify control\n",
    "        control = df[df['is_control'] == True].iloc[0]\n",
    "        treatments = df[df['is_control'] == False]\n",
    "        \n",
    "        # Number of comparisons\n",
    "        n_comparisons = len(treatments)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for idx, treatment in treatments.iterrows():\n",
    "            # T-test\n",
    "            # Note: In production, would load actual data for test\n",
    "            # Here we use summary statistics\n",
    "            \n",
    "            # Calculate t-statistic\n",
    "            mean_diff = treatment['mean_value'] - control['mean_value']\n",
    "            se = np.sqrt(\n",
    "                control['std_value']**2 / control['users'] +\n",
    "                treatment['std_value']**2 / treatment['users']\n",
    "            )\n",
    "            t_stat = mean_diff / se\n",
    "            \n",
    "            # Degrees of freedom (Welch's t-test)\n",
    "            df_welch = (\n",
    "                (control['std_value']**2/control['users'] + \n",
    "                 treatment['std_value']**2/treatment['users'])**2 /\n",
    "                (\n",
    "                    (control['std_value']**2/control['users'])**2/(control['users']-1) +\n",
    "                    (treatment['std_value']**2/treatment['users'])**2/(treatment['users']-1)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # P-value\n",
    "            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df_welch))\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(\n",
    "                (control['std_value']**2 + treatment['std_value']**2) / 2\n",
    "            )\n",
    "            cohens_d = mean_diff / pooled_std\n",
    "            \n",
    "            # Confidence interval\n",
    "            ci_95 = (\n",
    "                mean_diff - 1.96 * se,\n",
    "                mean_diff + 1.96 * se\n",
    "            )\n",
    "            \n",
    "            # Relative lift\n",
    "            relative_lift = mean_diff / control['mean_value'] * 100\n",
    "            \n",
    "            results.append({\n",
    "                'control': control['variant_name'],\n",
    "                'treatment': treatment['variant_name'],\n",
    "                'control_mean': control['mean_value'],\n",
    "                'treatment_mean': treatment['mean_value'],\n",
    "                'mean_diff': mean_diff,\n",
    "                'relative_lift_pct': relative_lift,\n",
    "                'ci_95_lower': ci_95[0],\n",
    "                'ci_95_upper': ci_95[1],\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': cohens_d,\n",
    "                't_statistic': t_stat\n",
    "            })\n",
    "        \n",
    "        # Apply multiple testing correction\n",
    "        if correction == 'bonferroni':\n",
    "            adjusted_alpha = 0.05 / n_comparisons\n",
    "            for r in results:\n",
    "                r['adjusted_p_value'] = min(r['p_value'] * n_comparisons, 1.0)\n",
    "                r['significant_adjusted'] = r['adjusted_p_value'] < 0.05\n",
    "        \n",
    "        elif correction == 'holm':\n",
    "            # Sort by p-value\n",
    "            results_sorted = sorted(results, key=lambda x: x['p_value'])\n",
    "            for i, r in enumerate(results_sorted, 1):\n",
    "                adjusted_alpha = 0.05 / (n_comparisons - i + 1)\n",
    "                r['adjusted_alpha'] = adjusted_alpha\n",
    "                r['significant_adjusted'] = r['p_value'] < adjusted_alpha\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_multivariant_results(self, results: List[Dict]):\n",
    "        \"\"\"\n",
    "        Visualize multi-variant test results\n",
    "        \"\"\"\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Relative Lift vs Control',\n",
    "                'Effect Sizes (Cohen\\'s d)',\n",
    "                'P-values',\n",
    "                'Confidence Intervals'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # 1. Relative lift\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=df_results['treatment'],\n",
    "                y=df_results['relative_lift_pct'],\n",
    "                name='Lift %',\n",
    "                marker_color=['green' if x > 0 else 'red' \n",
    "                             for x in df_results['relative_lift_pct']]\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Effect sizes\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=df_results['treatment'],\n",
    "                y=df_results['cohens_d'],\n",
    "                name=\"Cohen's d\",\n",
    "                marker_color='blue'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. P-values\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=df_results['treatment'],\n",
    "                y=df_results['p_value'],\n",
    "                name='P-value',\n",
    "                marker_color=['green' if x < 0.05 else 'red' \n",
    "                             for x in df_results['p_value']]\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        fig.add_hline(y=0.05, line_dash=\"dash\", row=2, col=1,\n",
    "                     annotation_text=\"α=0.05\")\n",
    "        \n",
    "        # 4. Confidence intervals\n",
    "        for i, row in df_results.iterrows():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[row['treatment'], row['treatment']],\n",
    "                    y=[row['ci_95_lower'], row['ci_95_upper']],\n",
    "                    mode='lines+markers',\n",
    "                    name=row['treatment'],\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        fig.add_hline(y=0, line_dash=\"dash\", row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Multi-Variant Test Results',\n",
    "            height=800,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "# Example usage\n",
    "# mva = MultiVariantAnalyzer(db)\n",
    "# results = mva.pairwise_comparisons('exp_001', 'conversion', correction='bonferroni')\n",
    "# mva.visualize_multivariant_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sequential Testing Implementation <a name=\"sequential\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialTester:\n",
    "    \"\"\"\n",
    "    Sequential testing framework\n",
    "    Allows continuous monitoring with controlled error rates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.05, power=0.80, max_n=100000):\n",
    "        self.alpha = alpha\n",
    "        self.power = power\n",
    "        self.max_n = max_n\n",
    "        self.boundaries = self._calculate_boundaries()\n",
    "    \n",
    "    def _calculate_boundaries(self, n_looks=20):\n",
    "        \"\"\"\n",
    "        Calculate sequential testing boundaries (O'Brien-Fleming)\n",
    "        \"\"\"\n",
    "        # Information fractions\n",
    "        info_fractions = np.linspace(0.05, 1.0, n_looks)\n",
    "        \n",
    "        # O'Brien-Fleming spending function\n",
    "        z_alpha = stats.norm.ppf(1 - self.alpha/2)\n",
    "        \n",
    "        boundaries = []\n",
    "        for t in info_fractions:\n",
    "            # Boundary for this information fraction\n",
    "            z_boundary = z_alpha / np.sqrt(t)\n",
    "            p_boundary = 2 * (1 - stats.norm.cdf(z_boundary))\n",
    "            \n",
    "            boundaries.append({\n",
    "                'info_fraction': t,\n",
    "                'n': int(t * self.max_n),\n",
    "                'z_boundary': z_boundary,\n",
    "                'p_boundary': p_boundary\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(boundaries)\n",
    "    \n",
    "    def check_stopping_condition(self, n_observed, p_value, z_stat):\n",
    "        \"\"\"\n",
    "        Check if experiment can be stopped\n",
    "        \n",
    "        Returns:\n",
    "            dict with decision and reasoning\n",
    "        \"\"\"\n",
    "        # Find closest boundary\n",
    "        info_fraction = n_observed / self.max_n\n",
    "        \n",
    "        # Get appropriate boundary\n",
    "        boundary = self.boundaries[\n",
    "            self.boundaries['info_fraction'] >= info_fraction\n",
    "        ].iloc[0]\n",
    "        \n",
    "        # Check if we can stop\n",
    "        can_stop = abs(z_stat) >= boundary['z_boundary']\n",
    "        \n",
    "        decision = {\n",
    "            'can_stop': can_stop,\n",
    "            'n_observed': n_observed,\n",
    "            'info_fraction': info_fraction,\n",
    "            'p_value': p_value,\n",
    "            'z_stat': z_stat,\n",
    "            'z_boundary': boundary['z_boundary'],\n",
    "            'p_boundary': boundary['p_boundary'],\n",
    "            'reason': self._get_reason(can_stop, z_stat, boundary)\n",
    "        }\n",
    "        \n",
    "        return decision\n",
    "    \n",
    "    def _get_reason(self, can_stop, z_stat, boundary):\n",
    "        \"\"\"Generate human-readable decision reason\"\"\"\n",
    "        if can_stop:\n",
    "            direction = \"positive\" if z_stat > 0 else \"negative\"\n",
    "            return f\"Significant {direction} effect detected. \"\\\n",
    "                   f\"Z-statistic ({z_stat:.3f}) exceeds boundary ({boundary['z_boundary']:.3f}).\"\n",
    "        else:\n",
    "            return f\"Continue testing. \"\\\n",
    "                   f\"Z-statistic ({z_stat:.3f}) has not crossed boundary ({boundary['z_boundary']:.3f}).\"\n",
    "    \n",
    "    def visualize_sequential_test(self, test_history: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Visualize sequential test progress\n",
    "        \n",
    "        Args:\n",
    "            test_history: DataFrame with columns [n, z_stat, p_value]\n",
    "        \"\"\"\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Plot boundaries\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.boundaries['n'],\n",
    "            y=self.boundaries['z_boundary'],\n",
    "            mode='lines',\n",
    "            name='Upper Boundary',\n",
    "            line=dict(color='red', dash='dash')\n",
    "        ))\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=self.boundaries['n'],\n",
    "            y=-self.boundaries['z_boundary'],\n",
    "            mode='lines',\n",
    "            name='Lower Boundary',\n",
    "            line=dict(color='red', dash='dash')\n",
    "        ))\n",
    "        \n",
    "        # Plot test statistic over time\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=test_history['n'],\n",
    "            y=test_history['z_stat'],\n",
    "            mode='lines+markers',\n",
    "            name='Z-statistic',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Sequential Testing Progress',\n",
    "            xaxis_title='Sample Size',\n",
    "            yaxis_title='Z-statistic',\n",
    "            hovermode='x unified',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "# Example usage\n",
    "# seq_tester = SequentialTester(alpha=0.05, power=0.80, max_n=100000)\n",
    "# \n",
    "# # Check at current sample size\n",
    "# decision = seq_tester.check_stopping_condition(\n",
    "#     n_observed=25000,\n",
    "#     p_value=0.03,\n",
    "#     z_stat=2.17\n",
    "# )\n",
    "# \n",
    "# print(f\"Can stop: {decision['can_stop']}\")\n",
    "# print(f\"Reason: {decision['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Automated Monitoring System <a name=\"monitoring\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentMonitor:\n",
    "    \"\"\"\n",
    "    Automated monitoring and alerting for running experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_conn):\n",
    "        self.db = db_conn\n",
    "        self.alerts = []\n",
    "    \n",
    "    def check_all_experiments(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Run health checks on all active experiments\n",
    "        \"\"\"\n",
    "        # Get active experiments\n",
    "        experiments = self.db.query(\"\"\"\n",
    "        SELECT experiment_id, name, start_date, target_sample_size\n",
    "        FROM experiments\n",
    "        WHERE status = 'active'\n",
    "        \"\"\")\n",
    "        \n",
    "        all_checks = []\n",
    "        \n",
    "        for _, exp in experiments.iterrows():\n",
    "            checks = self.check_experiment(exp['experiment_id'])\n",
    "            checks['experiment_name'] = exp['name']\n",
    "            all_checks.append(checks)\n",
    "        \n",
    "        return all_checks\n",
    "    \n",
    "    def check_experiment(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive health check for single experiment\n",
    "        \"\"\"\n",
    "        checks = {\n",
    "            'experiment_id': experiment_id,\n",
    "            'timestamp': datetime.now(),\n",
    "            'issues': [],\n",
    "            'warnings': [],\n",
    "            'health_score': 100\n",
    "        }\n",
    "        \n",
    "        # 1. Check sample size\n",
    "        sample_check = self._check_sample_size(experiment_id)\n",
    "        if sample_check['has_issue']:\n",
    "            checks['issues'].append(sample_check)\n",
    "            checks['health_score'] -= 20\n",
    "        \n",
    "        # 2. Check traffic distribution\n",
    "        traffic_check = self._check_traffic_distribution(experiment_id)\n",
    "        if traffic_check['has_issue']:\n",
    "            checks['issues'].append(traffic_check)\n",
    "            checks['health_score'] -= 15\n",
    "        \n",
    "        # 3. Check metric anomalies\n",
    "        anomaly_check = self._check_metric_anomalies(experiment_id)\n",
    "        if anomaly_check['has_issue']:\n",
    "            checks['warnings'].append(anomaly_check)\n",
    "            checks['health_score'] -= 10\n",
    "        \n",
    "        # 4. Check SRM (Sample Ratio Mismatch)\n",
    "        srm_check = self._check_srm(experiment_id)\n",
    "        if srm_check['has_issue']:\n",
    "            checks['issues'].append(srm_check)\n",
    "            checks['health_score'] -= 25\n",
    "        \n",
    "        # 5. Check data freshness\n",
    "        freshness_check = self._check_data_freshness(experiment_id)\n",
    "        if freshness_check['has_issue']:\n",
    "            checks['warnings'].append(freshness_check)\n",
    "            checks['health_score'] -= 5\n",
    "        \n",
    "        return checks\n",
    "    \n",
    "    def _check_sample_size(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Check if sample size is on track\"\"\"\n",
    "        \n",
    "        # Get target and actual\n",
    "        result = self.db.query(\"\"\"\n",
    "        SELECT \n",
    "            e.target_sample_size,\n",
    "            DATEDIFF(day, e.start_date, CURRENT_DATE) as days_running,\n",
    "            COUNT(DISTINCT ea.user_id) as current_sample\n",
    "        FROM experiments e\n",
    "        LEFT JOIN experiment_assignments ea ON e.experiment_id = ea.experiment_id\n",
    "        WHERE e.experiment_id = %s\n",
    "        GROUP BY e.target_sample_size, e.start_date\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        if result.empty:\n",
    "            return {'has_issue': True, 'message': 'No data found'}\n",
    "        \n",
    "        row = result.iloc[0]\n",
    "        \n",
    "        # Check if on pace\n",
    "        expected_rate = row['target_sample_size'] / 14  # Assume 2-week test\n",
    "        actual_rate = row['current_sample'] / max(row['days_running'], 1)\n",
    "        \n",
    "        if actual_rate < expected_rate * 0.5:\n",
    "            return {\n",
    "                'has_issue': True,\n",
    "                'type': 'sample_size',\n",
    "                'message': f'Sample size growing too slowly. '\n",
    "                          f'Current: {row[\"current_sample\"]}, '\n",
    "                          f'Target: {row[\"target_sample_size\"]}'\n",
    "            }\n",
    "        \n",
    "        return {'has_issue': False}\n",
    "    \n",
    "    def _check_traffic_distribution(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Check if traffic is distributed as expected\"\"\"\n",
    "        \n",
    "        result = self.db.query(\"\"\"\n",
    "        SELECT \n",
    "            v.variant_id,\n",
    "            v.traffic_allocation as expected,\n",
    "            COUNT(DISTINCT ea.user_id) * 1.0 / \n",
    "                SUM(COUNT(DISTINCT ea.user_id)) OVER () as actual\n",
    "        FROM experiment_variants v\n",
    "        LEFT JOIN experiment_assignments ea \n",
    "            ON v.variant_id = ea.variant_id\n",
    "        WHERE v.experiment_id = %s\n",
    "        GROUP BY v.variant_id, v.traffic_allocation\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        # Check if any variant deviates > 5%\n",
    "        max_deviation = (result['actual'] - result['expected']).abs().max()\n",
    "        \n",
    "        if max_deviation > 0.05:\n",
    "            return {\n",
    "                'has_issue': True,\n",
    "                'type': 'traffic_distribution',\n",
    "                'message': f'Traffic distribution off by {max_deviation*100:.1f}%'\n",
    "            }\n",
    "        \n",
    "        return {'has_issue': False}\n",
    "    \n",
    "    def _check_srm(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Check for Sample Ratio Mismatch\n",
    "        Critical data quality issue\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.db.query(\"\"\"\n",
    "        SELECT \n",
    "            v.variant_id,\n",
    "            v.traffic_allocation,\n",
    "            COUNT(DISTINCT ea.user_id) as observed\n",
    "        FROM experiment_variants v\n",
    "        LEFT JOIN experiment_assignments ea \n",
    "            ON v.variant_id = ea.variant_id\n",
    "        WHERE v.experiment_id = %s\n",
    "        GROUP BY v.variant_id, v.traffic_allocation\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        # Chi-square test\n",
    "        total = result['observed'].sum()\n",
    "        expected = result['traffic_allocation'] * total\n",
    "        observed = result['observed']\n",
    "        \n",
    "        chi2_stat = ((observed - expected)**2 / expected).sum()\n",
    "        dof = len(result) - 1\n",
    "        p_value = 1 - stats.chi2.cdf(chi2_stat, dof)\n",
    "        \n",
    "        if p_value < 0.001:  # Strict threshold for SRM\n",
    "            return {\n",
    "                'has_issue': True,\n",
    "                'type': 'srm',\n",
    "                'message': f'Sample Ratio Mismatch detected (p={p_value:.6f}). '\n",
    "                          f'Investigate randomization!',\n",
    "                'severity': 'critical'\n",
    "            }\n",
    "        \n",
    "        return {'has_issue': False}\n",
    "    \n",
    "    def _check_metric_anomalies(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Check for unusual metric values\"\"\"\n",
    "        # Implementation would check for outliers, sudden drops, etc.\n",
    "        return {'has_issue': False}\n",
    "    \n",
    "    def _check_data_freshness(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Check if data is up to date\"\"\"\n",
    "        \n",
    "        result = self.db.query(\"\"\"\n",
    "        SELECT MAX(occurred_at) as latest_event\n",
    "        FROM experiment_events\n",
    "        WHERE experiment_id = %s\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        if result.empty or result['latest_event'].iloc[0] is None:\n",
    "            return {'has_issue': True, 'message': 'No recent events'}\n",
    "        \n",
    "        latest = pd.to_datetime(result['latest_event'].iloc[0])\n",
    "        hours_old = (datetime.now() - latest).total_seconds() / 3600\n",
    "        \n",
    "        if hours_old > 6:\n",
    "            return {\n",
    "                'has_issue': True,\n",
    "                'type': 'data_freshness',\n",
    "                'message': f'No events in {hours_old:.1f} hours'\n",
    "            }\n",
    "        \n",
    "        return {'has_issue': False}\n",
    "    \n",
    "    def send_alert(self, check_result: Dict):\n",
    "        \"\"\"\n",
    "        Send alert for issues\n",
    "        In production: email, Slack, PagerDuty, etc.\n",
    "        \"\"\"\n",
    "        if check_result['health_score'] < 80:\n",
    "            print(f\"⚠️  ALERT: {check_result['experiment_name']}\")\n",
    "            print(f\"   Health Score: {check_result['health_score']}/100\")\n",
    "            for issue in check_result['issues']:\n",
    "                print(f\"   - {issue['message']}\")\n",
    "\n",
    "# Example usage\n",
    "# monitor = ExperimentMonitor(db)\n",
    "# checks = monitor.check_all_experiments()\n",
    "# for check in checks:\n",
    "#     monitor.send_alert(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Reporting <a name=\"reporting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentReporter:\n",
    "    \"\"\"\n",
    "    Generate automated experiment reports\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_conn):\n",
    "        self.db = db_conn\n",
    "    \n",
    "    def generate_report(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate comprehensive experiment report\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'experiment_id': experiment_id,\n",
    "            'generated_at': datetime.now(),\n",
    "            'metadata': self._get_metadata(experiment_id),\n",
    "            'summary': self._get_summary(experiment_id),\n",
    "            'variant_performance': self._get_variant_performance(experiment_id),\n",
    "            'statistical_tests': self._run_statistical_tests(experiment_id),\n",
    "            'visualizations': self._create_visualizations(experiment_id),\n",
    "            'recommendation': self._generate_recommendation(experiment_id)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _get_metadata(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Get experiment configuration\"\"\"\n",
    "        result = self.db.query(\"\"\"\n",
    "        SELECT * FROM experiments WHERE experiment_id = %s\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        return result.to_dict('records')[0] if not result.empty else {}\n",
    "    \n",
    "    def _get_summary(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Get high-level summary statistics\"\"\"\n",
    "        result = self.db.query(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT user_id) as total_users,\n",
    "            COUNT(DISTINCT session_id) as total_sessions,\n",
    "            COUNT(*) as total_events,\n",
    "            MIN(assigned_at) as first_assignment,\n",
    "            MAX(assigned_at) as last_assignment\n",
    "        FROM experiment_assignments\n",
    "        WHERE experiment_id = %s\n",
    "        \"\"\", (experiment_id,))\n",
    "        \n",
    "        return result.to_dict('records')[0] if not result.empty else {}\n",
    "    \n",
    "    def _get_variant_performance(self, experiment_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Get detailed variant metrics\"\"\"\n",
    "        return self.db.query(\"\"\"\n",
    "        SELECT \n",
    "            v.variant_name,\n",
    "            v.is_control,\n",
    "            COUNT(DISTINCT ea.user_id) as users,\n",
    "            COUNT(DISTINCT CASE WHEN ee.event_type = 'conversion' \n",
    "                  THEN ea.user_id END) as conversions,\n",
    "            AVG(CASE WHEN ee.event_type = 'revenue' \n",
    "                THEN ee.event_value END) as avg_revenue,\n",
    "            SUM(CASE WHEN ee.event_type = 'revenue' \n",
    "                THEN ee.event_value ELSE 0 END) as total_revenue\n",
    "        FROM experiment_variants v\n",
    "        JOIN experiment_assignments ea ON v.variant_id = ea.variant_id\n",
    "        LEFT JOIN experiment_events ee \n",
    "            ON ea.user_id = ee.user_id \n",
    "            AND ea.experiment_id = ee.experiment_id\n",
    "        WHERE v.experiment_id = %s\n",
    "        GROUP BY v.variant_name, v.is_control\n",
    "        \"\"\", (experiment_id,))\n",
    "    \n",
    "    def _run_statistical_tests(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Run statistical analysis\"\"\"\n",
    "        # Use MultiVariantAnalyzer\n",
    "        mva = MultiVariantAnalyzer(self.db)\n",
    "        results = mva.pairwise_comparisons(experiment_id, 'conversion')\n",
    "        \n",
    "        return {\n",
    "            'pairwise_comparisons': results\n",
    "        }\n",
    "    \n",
    "    def _create_visualizations(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Create report visualizations\"\"\"\n",
    "        # Implementation would create and save plots\n",
    "        return {}\n",
    "    \n",
    "    def _generate_recommendation(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate data-driven recommendation\n",
    "        \"\"\"\n",
    "        # Get test results\n",
    "        tests = self._run_statistical_tests(experiment_id)\n",
    "        \n",
    "        # Simple decision logic\n",
    "        best_variant = None\n",
    "        max_lift = 0\n",
    "        \n",
    "        for result in tests['pairwise_comparisons']:\n",
    "            if result['p_value'] < 0.05 and result['relative_lift_pct'] > max_lift:\n",
    "                best_variant = result['treatment']\n",
    "                max_lift = result['relative_lift_pct']\n",
    "        \n",
    "        if best_variant:\n",
    "            recommendation = {\n",
    "                'decision': 'ship_treatment',\n",
    "                'variant': best_variant,\n",
    "                'confidence': 'high',\n",
    "                'expected_lift': max_lift,\n",
    "                'reasoning': f'{best_variant} shows {max_lift:.1f}% lift with statistical significance'\n",
    "            }\n",
    "        else:\n",
    "            recommendation = {\n",
    "                'decision': 'keep_control',\n",
    "                'confidence': 'medium',\n",
    "                'reasoning': 'No treatment variant shows significant improvement'\n",
    "            }\n",
    "        \n",
    "        return recommendation\n",
    "    \n",
    "    def export_to_html(self, report: Dict, filepath: str):\n",
    "        \"\"\"\n",
    "        Export report to HTML\n",
    "        \"\"\"\n",
    "        html_template = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Experiment Report: {experiment_id}</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "                h1 {{ color: #2c3e50; }}\n",
    "                .metric {{ padding: 20px; background: #ecf0f1; margin: 10px 0; }}\n",
    "                .recommendation {{ padding: 20px; background: #2ecc71; color: white; }}\n",
    "                table {{ border-collapse: collapse; width: 100%; }}\n",
    "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
    "                th {{ background-color: #3498db; color: white; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Experiment Report</h1>\n",
    "            <h2>{name}</h2>\n",
    "            <p><strong>Generated:</strong> {generated_at}</p>\n",
    "            \n",
    "            <h3>Summary</h3>\n",
    "            <div class=\"metric\">\n",
    "                <p>Total Users: {total_users:,}</p>\n",
    "                <p>Total Events: {total_events:,}</p>\n",
    "            </div>\n",
    "            \n",
    "            <h3>Recommendation</h3>\n",
    "            <div class=\"recommendation\">\n",
    "                <h4>{decision}</h4>\n",
    "                <p>{reasoning}</p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Format template\n",
    "        html = html_template.format(\n",
    "            experiment_id=report['experiment_id'],\n",
    "            name=report['metadata'].get('name', 'Unknown'),\n",
    "            generated_at=report['generated_at'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            total_users=report['summary'].get('total_users', 0),\n",
    "            total_events=report['summary'].get('total_events', 0),\n",
    "            decision=report['recommendation']['decision'].upper(),\n",
    "            reasoning=report['recommendation']['reasoning']\n",
    "        )\n",
    "        \n",
    "        # Write file\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(html)\n",
    "        \n",
    "        print(f\"✓ Report exported to {filepath}\")\n",
    "\n",
    "# Example usage\n",
    "# reporter = ExperimentReporter(db)\n",
    "# report = reporter.generate_report('exp_001')\n",
    "# reporter.export_to_html(report, 'experiment_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Project: Production A/B Testing Pipeline <a name=\"project\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROJECT: Build Production A/B Testing Platform\n",
    "\n",
    "Requirements:\n",
    "1. Complete experiment lifecycle management\n",
    "2. Automated daily analysis and reporting\n",
    "3. Real-time monitoring with alerts\n",
    "4. Sequential testing support\n",
    "5. Multi-variant testing\n",
    "6. Data quality checks (SRM, etc.)\n",
    "7. Automated decision recommendations\n",
    "8. Integration with Redshift\n",
    "\n",
    "Deliverables:\n",
    "- Working platform code\n",
    "- Database schema\n",
    "- Monitoring dashboard\n",
    "- Automated reports\n",
    "- Documentation\n",
    "- Deployment guide\n",
    "\"\"\"\n",
    "\n",
    "class ABTestingPlatform:\n",
    "    \"\"\"\n",
    "    Complete A/B testing platform\n",
    "    Integrates all components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_config: Dict):\n",
    "        self.db = DatabaseConnection(db_config)\n",
    "        self.manager = ExperimentManager(self.db)\n",
    "        self.analyzer = MultiVariantAnalyzer(self.db)\n",
    "        self.monitor = ExperimentMonitor(self.db)\n",
    "        self.reporter = ExperimentReporter(self.db)\n",
    "        self.seq_tester = SequentialTester()\n",
    "    \n",
    "    def initialize_platform(self):\n",
    "        \"\"\"Set up platform infrastructure\"\"\"\n",
    "        print(\"Initializing A/B testing platform...\")\n",
    "        create_ab_testing_schema(self.db)\n",
    "        print(\"✓ Platform initialized\")\n",
    "    \n",
    "    def daily_analysis_job(self):\n",
    "        \"\"\"\n",
    "        Daily automated analysis job\n",
    "        Run via cron/airflow\n",
    "        \"\"\"\n",
    "        print(f\"Starting daily analysis: {datetime.now()}\")\n",
    "        \n",
    "        # Get active experiments\n",
    "        active_experiments = self.manager.list_active_experiments()\n",
    "        \n",
    "        for _, exp in active_experiments.iterrows():\n",
    "            exp_id = exp['experiment_id']\n",
    "            \n",
    "            print(f\"\\nAnalyzing: {exp['name']}\")\n",
    "            \n",
    "            # Run health checks\n",
    "            health = self.monitor.check_experiment(exp_id)\n",
    "            \n",
    "            if health['health_score'] < 80:\n",
    "                self.monitor.send_alert(health)\n",
    "            \n",
    "            # Run statistical analysis\n",
    "            results = self.analyzer.pairwise_comparisons(exp_id, 'conversion')\n",
    "            \n",
    "            # Check if can stop early\n",
    "            if results:\n",
    "                decision = self.seq_tester.check_stopping_condition(\n",
    "                    n_observed=health.get('current_sample', 0),\n",
    "                    p_value=results[0]['p_value'],\n",
    "                    z_stat=results[0]['t_statistic']\n",
    "                )\n",
    "                \n",
    "                if decision['can_stop']:\n",
    "                    print(f\"  → Can stop early: {decision['reason']}\")\n",
    "            \n",
    "            # Generate report\n",
    "            report = self.reporter.generate_report(exp_id)\n",
    "            self.reporter.export_to_html(\n",
    "                report, \n",
    "                f\"reports/{exp_id}_{datetime.now().strftime('%Y%m%d')}.html\"\n",
    "            )\n",
    "        \n",
    "        print(\"\\n✓ Daily analysis complete\")\n",
    "    \n",
    "    def create_and_launch_experiment(\n",
    "        self, \n",
    "        name: str,\n",
    "        hypothesis: str,\n",
    "        variants: List[Variant],\n",
    "        primary_metric: Metric,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        End-to-end experiment creation and launch\n",
    "        \"\"\"\n",
    "        # Create experiment object\n",
    "        experiment = Experiment(\n",
    "            experiment_id=f\"exp_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
    "            name=name,\n",
    "            hypothesis=hypothesis,\n",
    "            variants=variants,\n",
    "            primary_metric=primary_metric,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Create in database\n",
    "        exp_id = self.manager.create_experiment(experiment)\n",
    "        \n",
    "        # Start experiment\n",
    "        self.manager.start_experiment(exp_id)\n",
    "        \n",
    "        print(f\"✓ Experiment {exp_id} launched\")\n",
    "        return exp_id\n",
    "\n",
    "# Example usage\n",
    "# platform = ABTestingPlatform(REDSHIFT_CONFIG)\n",
    "# platform.initialize_platform()\n",
    "# \n",
    "# # Launch experiment\n",
    "# exp_id = platform.create_and_launch_experiment(\n",
    "#     name='CTA Color Test',\n",
    "#     hypothesis='Green CTA increases conversions',\n",
    "#     variants=[\n",
    "#         Variant('control', 'Blue', 0.5, is_control=True),\n",
    "#         Variant('treatment', 'Green', 0.5)\n",
    "#     ],\n",
    "#     primary_metric=Metric('conversion', 'conversion', 'rate', is_primary=True)\n",
    "# )\n",
    "# \n",
    "# # Run daily analysis\n",
    "# platform.daily_analysis_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises <a name=\"exercises\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement SRM Detection\n",
    "\n",
    "**Task:** Build robust SRM (Sample Ratio Mismatch) detection:\n",
    "1. Implement chi-square test for SRM\n",
    "2. Add daily SRM monitoring\n",
    "3. Create alerts for SRM issues\n",
    "4. Build debugging tools to investigate SRM causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Multi-Metric Decision Framework\n",
    "\n",
    "**Task:** Build system to make decisions with multiple metrics:\n",
    "1. Define primary and guardrail metrics\n",
    "2. Implement decision logic considering all metrics\n",
    "3. Handle trade-offs (e.g., higher conversions but lower revenue)\n",
    "4. Generate nuanced recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Experiment Scheduling System\n",
    "\n",
    "**Task:** Build experiment scheduling and conflict detection:\n",
    "1. Detect overlapping experiments\n",
    "2. Identify metric pollution risks\n",
    "3. Recommend optimal scheduling\n",
    "4. Implement traffic allocation across multiple experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Production Deployment\n",
    "\n",
    "**Task:** Deploy platform to production:\n",
    "1. Set up automated daily jobs\n",
    "2. Configure monitoring and alerting\n",
    "3. Create user documentation\n",
    "4. Build admin dashboard\n",
    "5. Implement access controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Platform Architecture**\n",
    "   - Complete system design\n",
    "   - Database schema\n",
    "   - Component integration\n",
    "\n",
    "2. **Experiment Management**\n",
    "   - Configuration system\n",
    "   - Lifecycle management\n",
    "   - Variant allocation\n",
    "\n",
    "3. **Advanced Testing**\n",
    "   - Multi-variant analysis\n",
    "   - Sequential testing\n",
    "   - Multiple comparison correction\n",
    "\n",
    "4. **Automation**\n",
    "   - Automated monitoring\n",
    "   - Health checks\n",
    "   - Alerting systems\n",
    "   - Report generation\n",
    "\n",
    "5. **Production Readiness**\n",
    "   - Error handling\n",
    "   - Data quality checks\n",
    "   - Scalability\n",
    "   - Documentation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Deploy platform to production\n",
    "- Build custom integrations\n",
    "- Add advanced features\n",
    "- Train team on platform usage\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Trustworthy Online Controlled Experiments](https://experimentguide.com/)\n",
    "- [GrowthBook - Open Source Feature Flags](https://www.growthbook.io/)\n",
    "- [Statsig Documentation](https://docs.statsig.com/)\n",
    "- [Sample Ratio Mismatch](https://dl.acm.org/doi/10.1145/3292500.3330722)\n",
    "- [Sequential Testing Methods](https://www.optimizely.com/optimization-glossary/sequential-testing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
