{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Advanced Visualization & Dashboards\n",
    "\n",
    "## Overview\n",
    "Master creating production-ready dashboards and visualizations for large-scale marketing data using Plotly, Dash, and Redshift.\n",
    "\n",
    "## Learning Objectives\n",
    "- Create interactive dashboards with large datasets\n",
    "- Implement efficient aggregation before visualization\n",
    "- Build real-time monitoring dashboards\n",
    "- Optimize visualization performance\n",
    "- Design executive-level dashboard templates\n",
    "- Deploy production dashboards\n",
    "\n",
    "## Prerequisites\n",
    "- Redshift cluster access\n",
    "- Large marketing dataset\n",
    "- Understanding of data aggregation\n",
    "- Basic web development knowledge (helpful)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Environment](#setup)\n",
    "2. [Visualization Best Practices for Large Data](#best-practices)\n",
    "3. [Aggregation Strategies](#aggregation)\n",
    "4. [Interactive Visualizations with Plotly](#plotly)\n",
    "5. [Real-Time Monitoring Dashboards](#realtime)\n",
    "6. [Redshift → Pandas → Visualization Pipeline](#pipeline)\n",
    "7. [Performance Optimization](#performance)\n",
    "8. [Executive Dashboard Templates](#templates)\n",
    "9. [Real-World Project: Live Marketing Dashboard](#project)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q plotly dash pandas numpy redshift_connector\n",
    "!pip install -q dash-bootstrap-components kaleido\n",
    "!pip install -q sqlalchemy psycopg2-binary\n",
    "!pip install -q jupyter-dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output, State\n",
    "import dash_bootstrap_components as dbc\n",
    "from jupyter_dash import JupyterDash\n",
    "import redshift_connector\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Plotly\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Database configuration\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', '5439')),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing_db'),\n",
    "    'user': os.getenv('REDSHIFT_USER', input('Redshift username: ')),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', getpass('Redshift password: '))\n",
    "}\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Efficient data loading for visualizations\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.conn = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection\"\"\"\n",
    "        self.conn = redshift_connector.connect(**self.config)\n",
    "        return self.conn\n",
    "    \n",
    "    def query(self, sql):\n",
    "        \"\"\"Execute query and return DataFrame\"\"\"\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        \n",
    "        result = cursor.fetchall()\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        \n",
    "        return pd.DataFrame(result, columns=columns)\n",
    "    \n",
    "    def load_for_viz(self, table, agg_cols, metrics, filters=None, \n",
    "                     limit=10000, order_by=None):\n",
    "        \"\"\"\n",
    "        Load pre-aggregated data optimized for visualization\n",
    "        \n",
    "        Args:\n",
    "            table: Source table name\n",
    "            agg_cols: Columns to group by\n",
    "            metrics: Dict of {alias: aggregation}\n",
    "            filters: WHERE clause conditions\n",
    "            limit: Maximum rows to return\n",
    "            order_by: Order clause\n",
    "        \"\"\"\n",
    "        # Build aggregations\n",
    "        agg_exprs = [f\"{expr} as {alias}\" for alias, expr in metrics.items()]\n",
    "        \n",
    "        # Build query\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            {', '.join(agg_cols)},\n",
    "            {', '.join(agg_exprs)}\n",
    "        FROM {table}\n",
    "        \"\"\"\n",
    "        \n",
    "        if filters:\n",
    "            query += f\" WHERE {filters}\"\n",
    "        \n",
    "        query += f\" GROUP BY {', '.join(agg_cols)}\"\n",
    "        \n",
    "        if order_by:\n",
    "            query += f\" ORDER BY {order_by}\"\n",
    "        \n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "        \n",
    "        return self.query(query)\n",
    "    \n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "# Initialize data loader\n",
    "loader = DataLoader(REDSHIFT_CONFIG)\n",
    "print(\"✓ Data loader initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization Best Practices for Large Data <a name=\"best-practices\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Principles for Large-Scale Visualizations\n",
    "\n",
    "1. **Aggregate Before Visualizing**\n",
    "   - Never load raw data for visualization\n",
    "   - Pre-aggregate in database (Redshift)\n",
    "   - Keep viz data < 10,000 points\n",
    "\n",
    "2. **Choose Appropriate Chart Types**\n",
    "   - Time series: Line charts with aggregation\n",
    "   - Categories: Bar charts (limit to top N)\n",
    "   - Distributions: Histograms with binning\n",
    "   - Relationships: Scatter plots with sampling\n",
    "\n",
    "3. **Optimize Rendering**\n",
    "   - Use WebGL for large scatter plots\n",
    "   - Implement progressive loading\n",
    "   - Cache aggregated data\n",
    "\n",
    "4. **Ensure Interactivity**\n",
    "   - Drill-down capabilities\n",
    "   - Dynamic filtering\n",
    "   - Responsive design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_aggregation_need():\n",
    "    \"\"\"\n",
    "    Demonstrate why aggregation is critical for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate large dataset\n",
    "    np.random.seed(42)\n",
    "    n_points = 1_000_000\n",
    "    \n",
    "    dates = pd.date_range('2023-01-01', periods=365, freq='D')\n",
    "    df_raw = pd.DataFrame({\n",
    "        'date': np.random.choice(dates, n_points),\n",
    "        'revenue': np.random.lognormal(3, 1, n_points),\n",
    "        'channel': np.random.choice(['A', 'B', 'C'], n_points)\n",
    "    })\n",
    "    \n",
    "    # BAD: Try to plot 1M points (slow, unusable)\n",
    "    # fig = px.scatter(df_raw, x='date', y='revenue')  # DON'T DO THIS!\n",
    "    \n",
    "    # GOOD: Aggregate first\n",
    "    df_agg = df_raw.groupby('date').agg({\n",
    "        'revenue': ['sum', 'mean', 'count']\n",
    "    }).reset_index()\n",
    "    df_agg.columns = ['date', 'total_revenue', 'avg_revenue', 'transactions']\n",
    "    \n",
    "    # Now plot aggregated data (fast, clear)\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('Daily Total Revenue', 'Daily Avg Revenue')\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_agg['date'], y=df_agg['total_revenue'], \n",
    "                   mode='lines', name='Total Revenue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_agg['date'], y=df_agg['avg_revenue'], \n",
    "                   mode='lines', name='Avg Revenue'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=600, showlegend=False)\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Raw data: {len(df_raw):,} points\")\n",
    "    print(f\"Aggregated data: {len(df_agg):,} points\")\n",
    "    print(f\"Reduction: {(1 - len(df_agg)/len(df_raw))*100:.1f}%\")\n",
    "\n",
    "# demonstrate_aggregation_need()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregation Strategies <a name=\"aggregation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationStrategy:\n",
    "    \"\"\"\n",
    "    Smart aggregation strategies for different visualization types\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_series_daily(loader, table, date_col, metric_col, filters=None):\n",
    "        \"\"\"\n",
    "        Aggregate time series data by day\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE({date_col}) as date,\n",
    "            COUNT(*) as events,\n",
    "            SUM({metric_col}) as total,\n",
    "            AVG({metric_col}) as average,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {metric_col}) as median,\n",
    "            STDDEV({metric_col}) as std_dev\n",
    "        FROM {table}\n",
    "        \"\"\"\n",
    "        \n",
    "        if filters:\n",
    "            query += f\" WHERE {filters}\"\n",
    "        \n",
    "        query += f\"\"\"\n",
    "        GROUP BY DATE({date_col})\n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        \n",
    "        return loader.query(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_series_hourly(loader, table, timestamp_col, metric_col, \n",
    "                           last_n_days=7, filters=None):\n",
    "        \"\"\"\n",
    "        Aggregate time series data by hour (for recent data)\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE_TRUNC('hour', {timestamp_col}) as hour,\n",
    "            COUNT(*) as events,\n",
    "            SUM({metric_col}) as total,\n",
    "            AVG({metric_col}) as average\n",
    "        FROM {table}\n",
    "        WHERE {timestamp_col} >= CURRENT_DATE - {last_n_days}\n",
    "        \"\"\"\n",
    "        \n",
    "        if filters:\n",
    "            query += f\" AND {filters}\"\n",
    "        \n",
    "        query += f\"\"\"\n",
    "        GROUP BY DATE_TRUNC('hour', {timestamp_col})\n",
    "        ORDER BY hour\n",
    "        \"\"\"\n",
    "        \n",
    "        return loader.query(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_n_categories(loader, table, category_col, metric_col, \n",
    "                        n=10, filters=None):\n",
    "        \"\"\"\n",
    "        Get top N categories by metric\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            {category_col},\n",
    "            COUNT(*) as count,\n",
    "            SUM({metric_col}) as total,\n",
    "            AVG({metric_col}) as average,\n",
    "            SUM({metric_col}) * 100.0 / SUM(SUM({metric_col})) OVER () as percentage\n",
    "        FROM {table}\n",
    "        \"\"\"\n",
    "        \n",
    "        if filters:\n",
    "            query += f\" WHERE {filters}\"\n",
    "        \n",
    "        query += f\"\"\"\n",
    "        GROUP BY {category_col}\n",
    "        ORDER BY total DESC\n",
    "        LIMIT {n}\n",
    "        \"\"\"\n",
    "        \n",
    "        return loader.query(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def distribution_bins(loader, table, metric_col, bins=30, filters=None):\n",
    "        \"\"\"\n",
    "        Create histogram bins for distribution visualization\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        WITH bounds AS (\n",
    "            SELECT \n",
    "                MIN({metric_col}) as min_val,\n",
    "                MAX({metric_col}) as max_val,\n",
    "                (MAX({metric_col}) - MIN({metric_col})) / {bins} as bin_width\n",
    "            FROM {table}\n",
    "            WHERE {metric_col} IS NOT NULL\n",
    "            {f'AND {filters}' if filters else ''}\n",
    "        ),\n",
    "        binned AS (\n",
    "            SELECT \n",
    "                FLOOR(({metric_col} - b.min_val) / b.bin_width) as bin_num,\n",
    "                b.min_val + FLOOR(({metric_col} - b.min_val) / b.bin_width) * b.bin_width as bin_start\n",
    "            FROM {table}, bounds b\n",
    "            WHERE {metric_col} IS NOT NULL\n",
    "            {f'AND {filters}' if filters else ''}\n",
    "        )\n",
    "        SELECT \n",
    "            bin_start,\n",
    "            COUNT(*) as frequency\n",
    "        FROM binned\n",
    "        GROUP BY bin_start\n",
    "        ORDER BY bin_start\n",
    "        \"\"\"\n",
    "        \n",
    "        return loader.query(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def multi_dimensional(loader, table, dimensions, metrics, filters=None, limit=1000):\n",
    "        \"\"\"\n",
    "        Multi-dimensional aggregation\n",
    "        \n",
    "        Args:\n",
    "            dimensions: List of dimension columns\n",
    "            metrics: Dict of {name: aggregation_expression}\n",
    "        \"\"\"\n",
    "        metric_exprs = [f\"{expr} as {name}\" for name, expr in metrics.items()]\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            {', '.join(dimensions)},\n",
    "            {', '.join(metric_exprs)}\n",
    "        FROM {table}\n",
    "        \"\"\"\n",
    "        \n",
    "        if filters:\n",
    "            query += f\" WHERE {filters}\"\n",
    "        \n",
    "        query += f\"\"\"\n",
    "        GROUP BY {', '.join(dimensions)}\n",
    "        ORDER BY {list(metrics.keys())[0]} DESC\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "        \n",
    "        return loader.query(query)\n",
    "\n",
    "# Example usage\n",
    "# agg = AggregationStrategy()\n",
    "# daily_data = agg.time_series_daily(loader, 'marketing_events', 'timestamp', 'revenue')\n",
    "# top_channels = agg.top_n_categories(loader, 'marketing_events', 'channel', 'revenue', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Visualizations with Plotly <a name=\"plotly\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingVisualizations:\n",
    "    \"\"\"\n",
    "    Library of marketing-specific visualizations\n",
    "    All optimized for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def revenue_trend(df, date_col='date', revenue_col='total_revenue'):\n",
    "        \"\"\"\n",
    "        Revenue trend with moving average\n",
    "        \"\"\"\n",
    "        # Calculate moving average\n",
    "        df = df.copy()\n",
    "        df['ma_7'] = df[revenue_col].rolling(window=7, min_periods=1).mean()\n",
    "        df['ma_30'] = df[revenue_col].rolling(window=30, min_periods=1).mean()\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Daily revenue\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[date_col],\n",
    "            y=df[revenue_col],\n",
    "            mode='lines',\n",
    "            name='Daily Revenue',\n",
    "            line=dict(color='lightgray', width=1),\n",
    "            opacity=0.5\n",
    "        ))\n",
    "        \n",
    "        # 7-day MA\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[date_col],\n",
    "            y=df['ma_7'],\n",
    "            mode='lines',\n",
    "            name='7-Day MA',\n",
    "            line=dict(color='blue', width=2)\n",
    "        ))\n",
    "        \n",
    "        # 30-day MA\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df[date_col],\n",
    "            y=df['ma_30'],\n",
    "            mode='lines',\n",
    "            name='30-Day MA',\n",
    "            line=dict(color='red', width=2)\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Revenue Trend Analysis',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Revenue ($)',\n",
    "            hovermode='x unified',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def channel_performance_comparison(df, channel_col='channel', \n",
    "                                       metrics=['total', 'average', 'count']):\n",
    "        \"\"\"\n",
    "        Multi-metric channel comparison\n",
    "        \"\"\"\n",
    "        n_metrics = len(metrics)\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=n_metrics,\n",
    "            subplot_titles=[m.replace('_', ' ').title() for m in metrics]\n",
    "        )\n",
    "        \n",
    "        for i, metric in enumerate(metrics, 1):\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=df[channel_col],\n",
    "                    y=df[metric],\n",
    "                    name=metric,\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=i\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Channel Performance Comparison',\n",
    "            height=400,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def conversion_funnel(stages_df, stage_col='stage', count_col='count'):\n",
    "        \"\"\"\n",
    "        Conversion funnel visualization\n",
    "        \"\"\"\n",
    "        fig = go.Figure(go.Funnel(\n",
    "            y=stages_df[stage_col],\n",
    "            x=stages_df[count_col],\n",
    "            textinfo=\"value+percent initial\",\n",
    "            textposition=\"inside\"\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Conversion Funnel',\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def cohort_retention_heatmap(cohort_df, cohort_col='cohort', \n",
    "                                 period_col='period', retention_col='retention'):\n",
    "        \"\"\"\n",
    "        Cohort retention heatmap\n",
    "        \"\"\"\n",
    "        # Pivot data for heatmap\n",
    "        pivot_df = cohort_df.pivot(\n",
    "            index=cohort_col,\n",
    "            columns=period_col,\n",
    "            values=retention_col\n",
    "        )\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=pivot_df.values,\n",
    "            x=pivot_df.columns,\n",
    "            y=pivot_df.index,\n",
    "            colorscale='Blues',\n",
    "            text=pivot_df.values,\n",
    "            texttemplate='%{text:.1f}%',\n",
    "            textfont={\"size\": 10},\n",
    "            colorbar=dict(title=\"Retention %\")\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Cohort Retention Analysis',\n",
    "            xaxis_title='Period',\n",
    "            yaxis_title='Cohort',\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def kpi_cards(metrics_dict):\n",
    "        \"\"\"\n",
    "        Create KPI indicator cards\n",
    "        \n",
    "        Args:\n",
    "            metrics_dict: {title: {'value': X, 'delta': Y, 'prefix': '$'}}\n",
    "        \"\"\"\n",
    "        n_metrics = len(metrics_dict)\n",
    "        cols = min(4, n_metrics)\n",
    "        rows = (n_metrics + cols - 1) // cols\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=rows, cols=cols,\n",
    "            specs=[[{'type': 'indicator'}] * cols for _ in range(rows)],\n",
    "            subplot_titles=list(metrics_dict.keys())\n",
    "        )\n",
    "        \n",
    "        for i, (title, data) in enumerate(metrics_dict.items(), 1):\n",
    "            row = (i - 1) // cols + 1\n",
    "            col = (i - 1) % cols + 1\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Indicator(\n",
    "                    mode=\"number+delta\",\n",
    "                    value=data['value'],\n",
    "                    delta={'reference': data.get('delta', data['value']),\n",
    "                           'relative': True},\n",
    "                    number={'prefix': data.get('prefix', '')},\n",
    "                    domain={'x': [0, 1], 'y': [0, 1]}\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=200 * rows)\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Example usage\n",
    "# viz = MarketingVisualizations()\n",
    "# Sample data\n",
    "# df_daily = pd.DataFrame({\n",
    "#     'date': pd.date_range('2023-01-01', periods=90),\n",
    "#     'total_revenue': np.random.lognormal(10, 0.5, 90)\n",
    "# })\n",
    "# fig = viz.revenue_trend(df_daily)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time Monitoring Dashboards <a name=\"realtime\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realtime_dashboard():\n",
    "    \"\"\"\n",
    "    Create a real-time monitoring dashboard using Dash\n",
    "    Updates every 30 seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Dash app\n",
    "    app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "    \n",
    "    # Layout\n",
    "    app.layout = dbc.Container([\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                html.H1(\"Marketing Performance Dashboard\"),\n",
    "                html.P(\"Real-time monitoring\", className=\"text-muted\")\n",
    "            ])\n",
    "        ], className=\"mb-4\"),\n",
    "        \n",
    "        # KPI Cards\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.H4(\"Total Revenue\", className=\"card-title\"),\n",
    "                        html.H2(id=\"revenue-kpi\", children=\"$0\"),\n",
    "                        html.P(id=\"revenue-delta\", className=\"text-success\")\n",
    "                    ])\n",
    "                ])\n",
    "            ], width=3),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.H4(\"Conversions\", className=\"card-title\"),\n",
    "                        html.H2(id=\"conversions-kpi\", children=\"0\"),\n",
    "                        html.P(id=\"conversions-delta\", className=\"text-success\")\n",
    "                    ])\n",
    "                ])\n",
    "            ], width=3),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.H4(\"Active Users\", className=\"card-title\"),\n",
    "                        html.H2(id=\"users-kpi\", children=\"0\"),\n",
    "                        html.P(id=\"users-delta\", className=\"text-info\")\n",
    "                    ])\n",
    "                ])\n",
    "            ], width=3),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.H4(\"Avg Order Value\", className=\"card-title\"),\n",
    "                        html.H2(id=\"aov-kpi\", children=\"$0\"),\n",
    "                        html.P(id=\"aov-delta\", className=\"text-warning\")\n",
    "                    ])\n",
    "                ])\n",
    "            ], width=3),\n",
    "        ], className=\"mb-4\"),\n",
    "        \n",
    "        # Charts\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                dcc.Graph(id=\"revenue-chart\")\n",
    "            ], width=8),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dcc.Graph(id=\"channel-chart\")\n",
    "            ], width=4),\n",
    "        ], className=\"mb-4\"),\n",
    "        \n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                dcc.Graph(id=\"hourly-chart\")\n",
    "            ], width=12),\n",
    "        ]),\n",
    "        \n",
    "        # Auto-refresh interval\n",
    "        dcc.Interval(\n",
    "            id='interval-component',\n",
    "            interval=30*1000,  # 30 seconds\n",
    "            n_intervals=0\n",
    "        )\n",
    "    ], fluid=True)\n",
    "    \n",
    "    # Callbacks for real-time updates\n",
    "    @app.callback(\n",
    "        [\n",
    "            Output('revenue-kpi', 'children'),\n",
    "            Output('conversions-kpi', 'children'),\n",
    "            Output('users-kpi', 'children'),\n",
    "            Output('aov-kpi', 'children'),\n",
    "            Output('revenue-chart', 'figure'),\n",
    "            Output('channel-chart', 'figure'),\n",
    "            Output('hourly-chart', 'figure'),\n",
    "        ],\n",
    "        Input('interval-component', 'n_intervals')\n",
    "    )\n",
    "    def update_dashboard(n):\n",
    "        \"\"\"\n",
    "        Update all dashboard components\n",
    "        In production, this would query Redshift\n",
    "        \"\"\"\n",
    "        \n",
    "        # Simulate data fetch (replace with actual Redshift queries)\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # KPIs\n",
    "        revenue = f\"${np.random.randint(50000, 100000):,}\"\n",
    "        conversions = f\"{np.random.randint(1000, 2000):,}\"\n",
    "        users = f\"{np.random.randint(5000, 10000):,}\"\n",
    "        aov = f\"${np.random.randint(40, 80)}\"\n",
    "        \n",
    "        # Revenue trend (last 24 hours)\n",
    "        hours = pd.date_range(end=current_time, periods=24, freq='H')\n",
    "        revenue_data = pd.DataFrame({\n",
    "            'hour': hours,\n",
    "            'revenue': np.random.lognormal(9, 0.3, 24)\n",
    "        })\n",
    "        \n",
    "        revenue_fig = go.Figure()\n",
    "        revenue_fig.add_trace(go.Scatter(\n",
    "            x=revenue_data['hour'],\n",
    "            y=revenue_data['revenue'],\n",
    "            mode='lines+markers',\n",
    "            fill='tozeroy',\n",
    "            name='Revenue'\n",
    "        ))\n",
    "        revenue_fig.update_layout(\n",
    "            title='Revenue (Last 24 Hours)',\n",
    "            xaxis_title='Time',\n",
    "            yaxis_title='Revenue ($)',\n",
    "            height=300\n",
    "        )\n",
    "        \n",
    "        # Channel distribution\n",
    "        channels = ['Google', 'Facebook', 'Email', 'Organic', 'Direct']\n",
    "        values = np.random.dirichlet(np.ones(5)) * 100\n",
    "        \n",
    "        channel_fig = go.Figure(data=[go.Pie(\n",
    "            labels=channels,\n",
    "            values=values,\n",
    "            hole=.3\n",
    "        )])\n",
    "        channel_fig.update_layout(\n",
    "            title='Channel Distribution',\n",
    "            height=300\n",
    "        )\n",
    "        \n",
    "        # Hourly conversions\n",
    "        hourly_fig = go.Figure(data=[\n",
    "            go.Bar(\n",
    "                x=revenue_data['hour'],\n",
    "                y=np.random.poisson(50, 24),\n",
    "                name='Conversions'\n",
    "            )\n",
    "        ])\n",
    "        hourly_fig.update_layout(\n",
    "            title='Hourly Conversions',\n",
    "            xaxis_title='Hour',\n",
    "            yaxis_title='Conversions',\n",
    "            height=300\n",
    "        )\n",
    "        \n",
    "        return revenue, conversions, users, aov, revenue_fig, channel_fig, hourly_fig\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create and run dashboard\n",
    "# app = create_realtime_dashboard()\n",
    "# app.run_server(mode='inline', port=8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Redshift → Pandas → Visualization Pipeline <a name=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline from Redshift to interactive visualizations\n",
    "    Optimized for performance and scalability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loader):\n",
    "        self.loader = loader\n",
    "        self.cache = {}  # Simple in-memory cache\n",
    "        self.viz = MarketingVisualizations()\n",
    "    \n",
    "    def get_daily_metrics(self, table, date_from=None, date_to=None, cache_key='daily'):\n",
    "        \"\"\"\n",
    "        Get daily aggregated metrics from Redshift\n",
    "        \"\"\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        filters = []\n",
    "        if date_from:\n",
    "            filters.append(f\"timestamp >= '{date_from}'\")\n",
    "        if date_to:\n",
    "            filters.append(f\"timestamp <= '{date_to}'\")\n",
    "        \n",
    "        filter_str = ' AND '.join(filters) if filters else None\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(timestamp) as date,\n",
    "            COUNT(*) as events,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) as conversions,\n",
    "            SUM(revenue) as total_revenue,\n",
    "            AVG(revenue) as avg_revenue,\n",
    "            SUM(page_views) as total_pageviews,\n",
    "            AVG(time_on_site) as avg_time_on_site\n",
    "        FROM {table}\n",
    "        {f'WHERE {filter_str}' if filter_str else ''}\n",
    "        GROUP BY DATE(timestamp)\n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.loader.query(query)\n",
    "        self.cache[cache_key] = df\n",
    "        return df\n",
    "    \n",
    "    def get_channel_performance(self, table, date_from=None):\n",
    "        \"\"\"\n",
    "        Get channel performance metrics\n",
    "        \"\"\"\n",
    "        filter_str = f\"timestamp >= '{date_from}'\" if date_from else None\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            channel,\n",
    "            COUNT(*) as events,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            SUM(revenue) as total_revenue,\n",
    "            AVG(revenue) as avg_revenue,\n",
    "            SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) as conversions,\n",
    "            SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as conversion_rate,\n",
    "            SUM(revenue) / COUNT(DISTINCT user_id) as revenue_per_user\n",
    "        FROM {table}\n",
    "        {f'WHERE {filter_str}' if filter_str else ''}\n",
    "        GROUP BY channel\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.loader.query(query)\n",
    "    \n",
    "    def create_executive_dashboard(self, table, lookback_days=30):\n",
    "        \"\"\"\n",
    "        Create comprehensive executive dashboard\n",
    "        \"\"\"\n",
    "        date_from = (datetime.now() - timedelta(days=lookback_days)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Get data\n",
    "        print(\"Loading daily metrics...\")\n",
    "        daily_df = self.get_daily_metrics(table, date_from=date_from)\n",
    "        \n",
    "        print(\"Loading channel performance...\")\n",
    "        channel_df = self.get_channel_performance(table, date_from=date_from)\n",
    "        \n",
    "        # Create visualizations\n",
    "        print(\"Creating visualizations...\")\n",
    "        \n",
    "        # 1. Revenue trend\n",
    "        fig_revenue = self.viz.revenue_trend(daily_df, 'date', 'total_revenue')\n",
    "        \n",
    "        # 2. Channel comparison\n",
    "        fig_channels = self.viz.channel_performance_comparison(\n",
    "            channel_df, 'channel', ['total_revenue', 'conversions', 'conversion_rate']\n",
    "        )\n",
    "        \n",
    "        # 3. KPIs\n",
    "        total_revenue = daily_df['total_revenue'].sum()\n",
    "        total_conversions = daily_df['conversions'].sum()\n",
    "        total_users = daily_df['unique_users'].sum()\n",
    "        avg_aov = total_revenue / total_conversions if total_conversions > 0 else 0\n",
    "        \n",
    "        kpis = {\n",
    "            'Total Revenue': {\n",
    "                'value': total_revenue,\n",
    "                'delta': total_revenue * 0.9,  # Compare to previous period\n",
    "                'prefix': '$'\n",
    "            },\n",
    "            'Conversions': {\n",
    "                'value': total_conversions,\n",
    "                'delta': total_conversions * 0.95\n",
    "            },\n",
    "            'Unique Users': {\n",
    "                'value': total_users,\n",
    "                'delta': total_users * 0.88\n",
    "            },\n",
    "            'Avg Order Value': {\n",
    "                'value': avg_aov,\n",
    "                'delta': avg_aov * 1.05,\n",
    "                'prefix': '$'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        fig_kpis = self.viz.kpi_cards(kpis)\n",
    "        \n",
    "        print(\"Dashboard ready!\")\n",
    "        \n",
    "        return {\n",
    "            'kpis': fig_kpis,\n",
    "            'revenue_trend': fig_revenue,\n",
    "            'channel_performance': fig_channels,\n",
    "            'data': {\n",
    "                'daily': daily_df,\n",
    "                'channels': channel_df\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def export_dashboard_html(self, dashboard, filename='dashboard.html'):\n",
    "        \"\"\"\n",
    "        Export dashboard to standalone HTML file\n",
    "        \"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Marketing Performance Dashboard</title>\n",
    "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "                h1 {{ color: #333; }}\n",
    "                .chart {{ margin: 20px 0; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Marketing Performance Dashboard</h1>\n",
    "            <div class=\"chart\" id=\"kpis\"></div>\n",
    "            <div class=\"chart\" id=\"revenue\"></div>\n",
    "            <div class=\"chart\" id=\"channels\"></div>\n",
    "            <script>\n",
    "                var kpis = {dashboard['kpis'].to_json()};\n",
    "                var revenue = {dashboard['revenue_trend'].to_json()};\n",
    "                var channels = {dashboard['channel_performance'].to_json()};\n",
    "                \n",
    "                Plotly.newPlot('kpis', kpis.data, kpis.layout);\n",
    "                Plotly.newPlot('revenue', revenue.data, revenue.layout);\n",
    "                Plotly.newPlot('channels', channels.data, channels.layout);\n",
    "            </script>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"Dashboard exported to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# pipeline = VisualizationPipeline(loader)\n",
    "# dashboard = pipeline.create_executive_dashboard('marketing_events', lookback_days=30)\n",
    "# dashboard['revenue_trend'].show()\n",
    "# pipeline.export_dashboard_html(dashboard, 'my_dashboard.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization <a name=\"performance\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PERFORMANCE OPTIMIZATION TECHNIQUES\n",
    "\n",
    "1. Database-Side Optimizations:\n",
    "   - Always aggregate in Redshift, not in Pandas\n",
    "   - Use materialized views for common queries\n",
    "   - Add appropriate sort/dist keys\n",
    "   - Use UNLOAD for large data exports\n",
    "\n",
    "2. Data Transfer Optimizations:\n",
    "   - Minimize data transferred from database\n",
    "   - Use compression (Parquet)\n",
    "   - Batch queries when possible\n",
    "   - Implement connection pooling\n",
    "\n",
    "3. Visualization Optimizations:\n",
    "   - Limit data points (< 10k per chart)\n",
    "   - Use WebGL for scatter plots\n",
    "   - Implement progressive loading\n",
    "   - Cache aggregated results\n",
    "\n",
    "4. Dashboard Optimizations:\n",
    "   - Load data asynchronously\n",
    "   - Implement lazy loading\n",
    "   - Use callbacks efficiently\n",
    "   - Minimize re-renders\n",
    "\"\"\"\n",
    "\n",
    "class PerformanceOptimizer:\n",
    "    \"\"\"\n",
    "    Tools for optimizing visualization performance\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def downsample_timeseries(df, date_col, value_cols, target_points=500):\n",
    "        \"\"\"\n",
    "        Intelligently downsample time series data\n",
    "        Preserves trends while reducing data points\n",
    "        \"\"\"\n",
    "        if len(df) <= target_points:\n",
    "            return df\n",
    "        \n",
    "        # Calculate bin size\n",
    "        bin_size = len(df) // target_points\n",
    "        \n",
    "        # Create bins\n",
    "        df = df.copy()\n",
    "        df['bin'] = df.index // bin_size\n",
    "        \n",
    "        # Aggregate by bin\n",
    "        agg_dict = {date_col: 'first'}\n",
    "        for col in value_cols:\n",
    "            agg_dict[col] = 'mean'\n",
    "        \n",
    "        downsampled = df.groupby('bin').agg(agg_dict).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Downsampled from {len(df):,} to {len(downsampled):,} points\")\n",
    "        return downsampled\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_materialized_view(loader, view_name, query):\n",
    "        \"\"\"\n",
    "        Create materialized view in Redshift for faster access\n",
    "        \"\"\"\n",
    "        create_query = f\"\"\"\n",
    "        CREATE MATERIALIZED VIEW {view_name} AS\n",
    "        {query}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Drop if exists\n",
    "        loader.query(f\"DROP MATERIALIZED VIEW IF EXISTS {view_name}\")\n",
    "        \n",
    "        # Create\n",
    "        loader.query(create_query)\n",
    "        \n",
    "        print(f\"Materialized view {view_name} created\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_query(loader, query, iterations=3):\n",
    "        \"\"\"\n",
    "        Benchmark query performance\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        times = []\n",
    "        for i in range(iterations):\n",
    "            start = time.time()\n",
    "            result = loader.query(query)\n",
    "            end = time.time()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        print(f\"Query performance:\")\n",
    "        print(f\"  Avg time: {np.mean(times):.2f}s\")\n",
    "        print(f\"  Min time: {np.min(times):.2f}s\")\n",
    "        print(f\"  Max time: {np.max(times):.2f}s\")\n",
    "        print(f\"  Rows returned: {len(result):,}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "# optimizer = PerformanceOptimizer()\n",
    "# \n",
    "# # Create materialized view for daily metrics\n",
    "# daily_metrics_query = '''\n",
    "# SELECT \n",
    "#     DATE(timestamp) as date,\n",
    "#     channel,\n",
    "#     COUNT(*) as events,\n",
    "#     SUM(revenue) as revenue\n",
    "# FROM marketing_events\n",
    "# GROUP BY DATE(timestamp), channel\n",
    "# '''\n",
    "# optimizer.create_materialized_view(loader, 'daily_channel_metrics', daily_metrics_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Executive Dashboard Templates <a name=\"templates\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_executive_template():\n",
    "    \"\"\"\n",
    "    Production-ready executive dashboard template\n",
    "    Clean, professional design with key metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    app = JupyterDash(__name__, external_stylesheets=[dbc.themes.LUX])\n",
    "    \n",
    "    app.layout = dbc.Container([\n",
    "        # Header\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                html.H1(\"Executive Marketing Dashboard\", \n",
    "                       style={'color': '#2c3e50', 'fontWeight': 'bold'}),\n",
    "                html.P(f\"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
    "                      className=\"text-muted\")\n",
    "            ], width=8),\n",
    "            dbc.Col([\n",
    "                dbc.ButtonGroup([\n",
    "                    dbc.Button(\"Today\", id=\"btn-today\", size=\"sm\"),\n",
    "                    dbc.Button(\"Week\", id=\"btn-week\", size=\"sm\"),\n",
    "                    dbc.Button(\"Month\", id=\"btn-month\", size=\"sm\", active=True),\n",
    "                    dbc.Button(\"Quarter\", id=\"btn-quarter\", size=\"sm\"),\n",
    "                ])\n",
    "            ], width=4, className=\"text-end\")\n",
    "        ], className=\"mb-4\", style={'borderBottom': '2px solid #ecf0f1', 'paddingBottom': '20px'}),\n",
    "        \n",
    "        # KPI Row\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.P(\"Total Revenue\", className=\"text-muted mb-2\"),\n",
    "                        html.H2(\"$1.2M\", className=\"mb-0\", style={'color': '#27ae60'}),\n",
    "                        html.Small(\"↑ 12.5% vs last period\", style={'color': '#27ae60'})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=3),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.P(\"Conversions\", className=\"text-muted mb-2\"),\n",
    "                        html.H2(\"24,567\", className=\"mb-0\", style={'color': '#3498db'}),\n",
    "                        html.Small(\"↑ 8.3% vs last period\", style={'color': '#27ae60'})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=3),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.P(\"Conversion Rate\", className=\"text-muted mb-2\"),\n",
    "                        html.H2(\"3.2%\", className=\"mb-0\", style={'color': '#e74c3c'}),\n",
    "                        html.Small(\"↓ 0.2% vs last period\", style={'color': '#e74c3c'})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=3),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardBody([\n",
    "                        html.P(\"ROI\", className=\"text-muted mb-2\"),\n",
    "                        html.H2(\"245%\", className=\"mb-0\", style={'color': '#9b59b6'}),\n",
    "                        html.Small(\"↑ 15.7% vs last period\", style={'color': '#27ae60'})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=3),\n",
    "        ], className=\"mb-4\"),\n",
    "        \n",
    "        # Main Charts\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardHeader(\"Revenue Trend\"),\n",
    "                    dbc.CardBody([\n",
    "                        dcc.Graph(id=\"main-revenue-chart\", config={'displayModeBar': False})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=8),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardHeader(\"Channel Mix\"),\n",
    "                    dbc.CardBody([\n",
    "                        dcc.Graph(id=\"channel-pie-chart\", config={'displayModeBar': False})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=4),\n",
    "        ], className=\"mb-4\"),\n",
    "        \n",
    "        # Secondary Charts\n",
    "        dbc.Row([\n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardHeader(\"Channel Performance\"),\n",
    "                    dbc.CardBody([\n",
    "                        dcc.Graph(id=\"channel-bar-chart\", config={'displayModeBar': False})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=6),\n",
    "            \n",
    "            dbc.Col([\n",
    "                dbc.Card([\n",
    "                    dbc.CardHeader(\"Device Breakdown\"),\n",
    "                    dbc.CardBody([\n",
    "                        dcc.Graph(id=\"device-chart\", config={'displayModeBar': False})\n",
    "                    ])\n",
    "                ], className=\"shadow-sm\")\n",
    "            ], width=6),\n",
    "        ])\n",
    "    ], fluid=True, style={'backgroundColor': '#f8f9fa', 'padding': '20px'})\n",
    "    \n",
    "    # Add callbacks here for interactivity\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create template\n",
    "# app = create_executive_template()\n",
    "# app.run_server(mode='inline', port=8051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Project: Live Marketing Dashboard <a name=\"project\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROJECT: Build Production Marketing Performance Dashboard\n",
    "\n",
    "Requirements:\n",
    "1. Connect to Redshift database with 10M+ marketing events\n",
    "2. Create real-time dashboard updating every 60 seconds\n",
    "3. Display key metrics:\n",
    "   - Revenue (total, daily trend, YoY comparison)\n",
    "   - Conversions (total, rate, funnel)\n",
    "   - Channel performance (ROI, spend, revenue)\n",
    "   - User engagement (sessions, bounce rate, time on site)\n",
    "4. Enable filtering by:\n",
    "   - Date range\n",
    "   - Channel\n",
    "   - Device\n",
    "   - Country\n",
    "5. Export capabilities (PDF, CSV)\n",
    "6. Mobile-responsive design\n",
    "7. < 3 second load time\n",
    "\n",
    "Deliverables:\n",
    "- Functional Dash application\n",
    "- Deployment instructions\n",
    "- Performance benchmarks\n",
    "- User documentation\n",
    "\"\"\"\n",
    "\n",
    "# Your implementation here\n",
    "# This is a complete project - students should build this from scratch\n",
    "# using all the techniques learned in this notebook\n",
    "\n",
    "class ProductionMarketingDashboard:\n",
    "    \"\"\"\n",
    "    Production-ready marketing dashboard\n",
    "    Student project template\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, redshift_config):\n",
    "        self.loader = DataLoader(redshift_config)\n",
    "        self.app = None\n",
    "    \n",
    "    def build_app(self):\n",
    "        \"\"\"Build the Dash application\"\"\"\n",
    "        # TODO: Implement dashboard\n",
    "        pass\n",
    "    \n",
    "    def setup_callbacks(self):\n",
    "        \"\"\"Setup all dashboard callbacks\"\"\"\n",
    "        # TODO: Implement callbacks\n",
    "        pass\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=8050):\n",
    "        \"\"\"Run the dashboard\"\"\"\n",
    "        # TODO: Implement run logic\n",
    "        pass\n",
    "\n",
    "# Usage:\n",
    "# dashboard = ProductionMarketingDashboard(REDSHIFT_CONFIG)\n",
    "# dashboard.build_app()\n",
    "# dashboard.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises <a name=\"exercises\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Channel Performance Dashboard\n",
    "\n",
    "**Task:** Create an interactive dashboard that:\n",
    "1. Shows performance of all marketing channels\n",
    "2. Allows filtering by date range\n",
    "3. Displays metrics: Revenue, ROI, Conversions, CPA\n",
    "4. Updates visualizations based on filters\n",
    "5. Handles 1M+ events efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Real-Time Monitoring\n",
    "\n",
    "**Task:** Build a real-time monitoring dashboard that:\n",
    "1. Updates every 30 seconds\n",
    "2. Shows current hour's performance\n",
    "3. Alerts on anomalies (revenue drop > 20%)\n",
    "4. Displays rolling 24-hour metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Cohort Analysis Visualization\n",
    "\n",
    "**Task:** Create cohort retention visualization:\n",
    "1. Query cohort data from Redshift\n",
    "2. Calculate retention rates\n",
    "3. Create interactive heatmap\n",
    "4. Add drill-down capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Performance Optimization\n",
    "\n",
    "**Task:** Optimize a slow dashboard:\n",
    "1. Benchmark current performance\n",
    "2. Identify bottlenecks\n",
    "3. Implement optimizations (caching, materialized views, etc.)\n",
    "4. Measure improvements\n",
    "5. Document optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Visualization Best Practices**\n",
    "   - Always aggregate before visualizing\n",
    "   - Choose appropriate chart types\n",
    "   - Optimize for large datasets\n",
    "\n",
    "2. **Dashboard Development**\n",
    "   - Building with Plotly and Dash\n",
    "   - Real-time updates\n",
    "   - Interactive filtering\n",
    "   - Professional templates\n",
    "\n",
    "3. **Performance Optimization**\n",
    "   - Database-side aggregation\n",
    "   - Caching strategies\n",
    "   - Progressive loading\n",
    "   - Materialized views\n",
    "\n",
    "4. **Production Deployment**\n",
    "   - Complete pipelines\n",
    "   - Error handling\n",
    "   - Export capabilities\n",
    "   - Mobile responsiveness\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Week 7: Advanced Statistics for Big Data\n",
    "- Deploy your dashboard to production\n",
    "- Add more advanced features\n",
    "- Implement automated reporting\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Plotly Documentation](https://plotly.com/python/)\n",
    "- [Dash Documentation](https://dash.plotly.com/)\n",
    "- [Redshift Performance Tuning](https://docs.aws.amazon.com/redshift/latest/dg/c-optimizing-query-performance.html)\n",
    "- [Dashboard Design Best Practices](https://www.tableau.com/learn/articles/dashboard-design-principles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
