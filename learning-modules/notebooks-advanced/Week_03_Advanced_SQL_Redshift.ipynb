{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Advanced SQL for Redshift\n",
    "\n",
    "## Learning Objectives\n",
    "- Master Redshift-specific SQL features\n",
    "- Optimize queries using distribution and sort keys\n",
    "- Analyze and improve query performance\n",
    "- Use EXPLAIN to understand query plans\n",
    "- Implement advanced window functions at scale\n",
    "- Leverage columnar storage benefits\n",
    "- Design efficient joins for large tables\n",
    "- Create and use materialized views\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install pandas psycopg2-binary sqlalchemy ipython-sql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Redshift configuration\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.region.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', 5439)),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing'),\n",
    "    'user': os.getenv('REDSHIFT_USER', 'analyst'),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', 'your-password')\n",
    "}\n",
    "\n",
    "# Create connection string\n",
    "connection_string = (\n",
    "    f\"postgresql+psycopg2://{REDSHIFT_CONFIG['user']}:{REDSHIFT_CONFIG['password']}\"\n",
    "    f\"@{REDSHIFT_CONFIG['host']}:{REDSHIFT_CONFIG['port']}/{REDSHIFT_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(connection_string, pool_pre_ping=True)\n",
    "\n",
    "def execute_query(query: str, fetch: bool = True):\n",
    "    \"\"\"Execute query and return results.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        \n",
    "        if fetch:\n",
    "            df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "            elapsed = time.time() - start_time\n",
    "            logger.info(f\"Query returned {len(df):,} rows in {elapsed:.2f}s\")\n",
    "            return df\n",
    "        else:\n",
    "            conn.commit()\n",
    "            elapsed = time.time() - start_time\n",
    "            logger.info(f\"Query executed in {elapsed:.2f}s\")\n",
    "\n",
    "print(\"✓ Connected to Redshift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Table Design\n",
    "\n",
    "### 2.1 Distribution Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution style examples\n",
    "\n",
    "# KEY Distribution - Distribute based on a column (for joins)\n",
    "create_events_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS marketing_events (\n",
    "    event_id BIGINT,\n",
    "    user_id INTEGER,\n",
    "    campaign_id INTEGER,\n",
    "    channel VARCHAR(50),\n",
    "    event_type VARCHAR(50),\n",
    "    revenue DECIMAL(10,2),\n",
    "    timestamp TIMESTAMP,\n",
    "    date DATE\n",
    ")\n",
    "DISTKEY(user_id)  -- Distribute by user_id for user-based joins\n",
    "SORTKEY(timestamp, user_id);  -- Sort by timestamp first, then user_id\n",
    "\"\"\"\n",
    "\n",
    "# EVEN Distribution - Distribute evenly (for tables not joined often)\n",
    "create_campaigns_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS campaigns (\n",
    "    campaign_id INTEGER,\n",
    "    campaign_name VARCHAR(200),\n",
    "    channel VARCHAR(50),\n",
    "    budget DECIMAL(10,2),\n",
    "    start_date DATE,\n",
    "    end_date DATE\n",
    ")\n",
    "DISTSTYLE EVEN\n",
    "SORTKEY(campaign_id);\n",
    "\"\"\"\n",
    "\n",
    "# ALL Distribution - Copy table to all nodes (for small dimension tables)\n",
    "create_channels_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS channels (\n",
    "    channel_id INTEGER,\n",
    "    channel_name VARCHAR(50),\n",
    "    category VARCHAR(50)\n",
    ")\n",
    "DISTSTYLE ALL;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Distribution Style Guidelines:\n",
    "------------------------------\n",
    "KEY:  Use for large tables frequently joined on a specific column\n",
    "EVEN: Use for tables that don't join often or join on different columns\n",
    "ALL:  Use for small dimension tables (< 1-2 million rows) joined frequently\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sort Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound Sort Key - Multiple columns in order\n",
    "create_table_compound = \"\"\"\n",
    "CREATE TABLE user_sessions (\n",
    "    session_id BIGINT,\n",
    "    user_id INTEGER,\n",
    "    session_date DATE,\n",
    "    channel VARCHAR(50),\n",
    "    duration_seconds INTEGER\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "COMPOUND SORTKEY(session_date, user_id);  -- Date first for date range queries\n",
    "\"\"\"\n",
    "\n",
    "# Interleaved Sort Key - Equal weight to all columns\n",
    "create_table_interleaved = \"\"\"\n",
    "CREATE TABLE conversions (\n",
    "    conversion_id BIGINT,\n",
    "    user_id INTEGER,\n",
    "    campaign_id INTEGER,\n",
    "    conversion_date DATE,\n",
    "    revenue DECIMAL(10,2)\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "INTERLEAVED SORTKEY(user_id, campaign_id, conversion_date);  -- Query on any combination\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Sort Key Guidelines:\n",
    "--------------------\n",
    "COMPOUND:     Use when you have a clear query pattern (e.g., always filter by date)\n",
    "INTERLEAVED:  Use when queries filter on different column combinations\n",
    "              Note: Interleaved is more expensive to maintain\n",
    "\n",
    "Sort Key Column Order (Compound):\n",
    "1. Columns used in WHERE clauses (especially date ranges)\n",
    "2. Columns used in JOIN conditions\n",
    "3. Columns used in GROUP BY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Performance Analysis\n",
    "\n",
    "### 3.1 Using EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query_plan(query: str) -> pd.DataFrame:\n",
    "    \"\"\"Get and display EXPLAIN plan.\"\"\"\n",
    "    explain_query = f\"EXPLAIN {query}\"\n",
    "    return execute_query(explain_query)\n",
    "\n",
    "# Example query to analyze\n",
    "sample_query = \"\"\"\n",
    "SELECT \n",
    "    channel,\n",
    "    DATE_TRUNC('day', timestamp) as date,\n",
    "    COUNT(*) as events,\n",
    "    SUM(revenue) as total_revenue\n",
    "FROM marketing_events\n",
    "WHERE timestamp >= '2024-01-01'\n",
    "  AND timestamp < '2024-02-01'\n",
    "GROUP BY channel, DATE_TRUNC('day', timestamp)\n",
    "\"\"\"\n",
    "\n",
    "# Get query plan\n",
    "# plan = analyze_query_plan(sample_query)\n",
    "# print(plan)\n",
    "\n",
    "print(\"\"\"\n",
    "EXPLAIN Output - What to Look For:\n",
    "----------------------------------\n",
    "1. DS_DIST_ALL_NONE:    Good - No data redistribution needed\n",
    "2. DS_DIST_BOTH:        Bad - Data redistributed on both sides of join\n",
    "3. DS_BCAST_INNER:      OK - Small table broadcasted to all nodes\n",
    "4. Seq Scan:            Bad - Full table scan (missing where clause or index)\n",
    "5. Hash Join:           Good - Efficient join method\n",
    "6. Nested Loop:         Bad - Inefficient for large datasets\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Query Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to find slow queries\n",
    "slow_queries = \"\"\"\n",
    "SELECT \n",
    "    query,\n",
    "    TRIM(querytxt) as sql_text,\n",
    "    starttime,\n",
    "    endtime,\n",
    "    DATEDIFF(seconds, starttime, endtime) as duration_seconds,\n",
    "    aborted,\n",
    "    userid,\n",
    "    database\n",
    "FROM stl_query\n",
    "WHERE starttime >= DATEADD(hour, -24, GETDATE())\n",
    "  AND DATEDIFF(seconds, starttime, endtime) > 60  -- Queries longer than 60s\n",
    "  AND aborted = 0\n",
    "ORDER BY duration_seconds DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "# Query to check table statistics\n",
    "table_stats = \"\"\"\n",
    "SELECT \n",
    "    \"schema\",\n",
    "    \"table\",\n",
    "    size,\n",
    "    tbl_rows,\n",
    "    unsorted,\n",
    "    stats_off\n",
    "FROM svv_table_info\n",
    "WHERE \"schema\" NOT IN ('pg_catalog', 'information_schema')\n",
    "ORDER BY size DESC\n",
    "LIMIT 20;\n",
    "\"\"\"\n",
    "\n",
    "# Query to check distribution skew\n",
    "distribution_skew = \"\"\"\n",
    "SELECT \n",
    "    slice,\n",
    "    COUNT(*) as row_count\n",
    "FROM marketing_events\n",
    "GROUP BY slice\n",
    "ORDER BY row_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Performance Monitoring Queries:\n",
    "-------------------------------\n",
    "1. STL_QUERY:        Historical query performance\n",
    "2. SVV_TABLE_INFO:   Table size, rows, and statistics\n",
    "3. Distribution:     Check for data skew across slices\n",
    "4. STL_ALERT_EVENT_LOG: Alerts for performance issues\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Window Functions at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Running Totals\n",
    "running_total_query = \"\"\"\n",
    "SELECT \n",
    "    user_id,\n",
    "    timestamp,\n",
    "    revenue,\n",
    "    SUM(revenue) OVER (\n",
    "        PARTITION BY user_id \n",
    "        ORDER BY timestamp\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ) as cumulative_revenue\n",
    "FROM marketing_events\n",
    "WHERE event_type = 'conversion'\n",
    "ORDER BY user_id, timestamp;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Moving Averages\n",
    "moving_average_query = \"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('day', timestamp) as date,\n",
    "    channel,\n",
    "    SUM(revenue) as daily_revenue,\n",
    "    AVG(SUM(revenue)) OVER (\n",
    "        PARTITION BY channel \n",
    "        ORDER BY DATE_TRUNC('day', timestamp)\n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "    ) as revenue_7d_ma\n",
    "FROM marketing_events\n",
    "GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    "ORDER BY channel, date;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Ranking and Row Numbers\n",
    "ranking_query = \"\"\"\n",
    "SELECT \n",
    "    user_id,\n",
    "    campaign_id,\n",
    "    timestamp,\n",
    "    revenue,\n",
    "    ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as event_sequence,\n",
    "    RANK() OVER (PARTITION BY campaign_id ORDER BY revenue DESC) as revenue_rank,\n",
    "    PERCENT_RANK() OVER (PARTITION BY campaign_id ORDER BY revenue) as revenue_percentile\n",
    "FROM marketing_events\n",
    "WHERE event_type = 'conversion';\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Lead and Lag (Next/Previous Values)\n",
    "lead_lag_query = \"\"\"\n",
    "SELECT \n",
    "    user_id,\n",
    "    timestamp as current_event_time,\n",
    "    channel as current_channel,\n",
    "    LAG(channel) OVER (PARTITION BY user_id ORDER BY timestamp) as previous_channel,\n",
    "    LEAD(channel) OVER (PARTITION BY user_id ORDER BY timestamp) as next_channel,\n",
    "    DATEDIFF(\n",
    "        second, \n",
    "        LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp),\n",
    "        timestamp\n",
    "    ) as seconds_since_last_event\n",
    "FROM marketing_events\n",
    "ORDER BY user_id, timestamp;\n",
    "\"\"\"\n",
    "\n",
    "# Example 5: First/Last Value\n",
    "first_last_query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    user_id,\n",
    "    FIRST_VALUE(channel) OVER (\n",
    "        PARTITION BY user_id \n",
    "        ORDER BY timestamp\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) as first_touch_channel,\n",
    "    LAST_VALUE(channel) OVER (\n",
    "        PARTITION BY user_id \n",
    "        ORDER BY timestamp\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "    ) as last_touch_channel,\n",
    "    COUNT(*) OVER (PARTITION BY user_id) as total_touchpoints\n",
    "FROM marketing_events;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Window Function Best Practices:\n",
    "-------------------------------\n",
    "1. Use PARTITION BY to limit the window scope\n",
    "2. Specify explicit frame clauses for clarity\n",
    "3. Consider sort key alignment with ORDER BY\n",
    "4. Use ROWS vs RANGE appropriately:\n",
    "   - ROWS: Physical row positions\n",
    "   - RANGE: Logical value ranges\n",
    "5. Avoid multiple window functions with different partitions in one query\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimized Joins for Large Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Optimized JOIN with matching distribution keys\n",
    "optimized_join = \"\"\"\n",
    "-- Both tables distributed by user_id - No redistribution needed\n",
    "SELECT \n",
    "    e.user_id,\n",
    "    e.campaign_id,\n",
    "    e.revenue,\n",
    "    u.segment,\n",
    "    u.signup_date\n",
    "FROM marketing_events e\n",
    "INNER JOIN users u ON e.user_id = u.user_id  -- Same distkey\n",
    "WHERE e.date >= '2024-01-01';\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Join with small dimension table (DISTSTYLE ALL)\n",
    "dimension_join = \"\"\"\n",
    "-- Campaigns table is DISTSTYLE ALL, copied to all nodes\n",
    "SELECT \n",
    "    e.campaign_id,\n",
    "    c.campaign_name,\n",
    "    c.channel,\n",
    "    COUNT(*) as events,\n",
    "    SUM(e.revenue) as total_revenue\n",
    "FROM marketing_events e\n",
    "INNER JOIN campaigns c ON e.campaign_id = c.campaign_id\n",
    "GROUP BY e.campaign_id, c.campaign_name, c.channel;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Using subqueries to reduce join size\n",
    "filtered_join = \"\"\"\n",
    "-- Filter before joining to reduce data volume\n",
    "WITH recent_events AS (\n",
    "    SELECT user_id, campaign_id, SUM(revenue) as revenue\n",
    "    FROM marketing_events\n",
    "    WHERE date >= '2024-01-01'\n",
    "    GROUP BY user_id, campaign_id\n",
    "),\n",
    "high_value_users AS (\n",
    "    SELECT user_id, segment\n",
    "    FROM users\n",
    "    WHERE segment IN ('high', 'vip')\n",
    ")\n",
    "SELECT \n",
    "    e.user_id,\n",
    "    e.campaign_id,\n",
    "    e.revenue,\n",
    "    u.segment\n",
    "FROM recent_events e\n",
    "INNER JOIN high_value_users u ON e.user_id = u.user_id;\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Self-join for sequential analysis\n",
    "self_join_query = \"\"\"\n",
    "-- Find users who converted after seeing an ad\n",
    "SELECT \n",
    "    impressions.user_id,\n",
    "    impressions.timestamp as impression_time,\n",
    "    conversions.timestamp as conversion_time,\n",
    "    DATEDIFF(second, impressions.timestamp, conversions.timestamp) as time_to_conversion\n",
    "FROM marketing_events impressions\n",
    "INNER JOIN marketing_events conversions\n",
    "    ON impressions.user_id = conversions.user_id\n",
    "    AND conversions.timestamp > impressions.timestamp\n",
    "    AND conversions.timestamp <= DATEADD(day, 7, impressions.timestamp)\n",
    "WHERE impressions.event_type = 'impression'\n",
    "  AND conversions.event_type = 'conversion';\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Join Optimization Guidelines:\n",
    "----------------------------\n",
    "1. Match distribution keys for large table joins\n",
    "2. Use DISTSTYLE ALL for small dimension tables\n",
    "3. Filter data before joining (in CTEs or subqueries)\n",
    "4. Put largest table first in FROM clause\n",
    "5. Use INNER JOIN when possible (more efficient than OUTER)\n",
    "6. Avoid joining on expressions - join on columns directly\n",
    "7. Consider materializing complex join results\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Columnar Storage Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Selective column reading (fast with columnar storage)\n",
    "columnar_benefit_1 = \"\"\"\n",
    "-- Only reads 3 columns from disk (not full rows)\n",
    "SELECT \n",
    "    user_id,\n",
    "    campaign_id,\n",
    "    revenue\n",
    "FROM marketing_events\n",
    "WHERE date >= '2024-01-01';\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Compression benefits\n",
    "check_compression = \"\"\"\n",
    "-- Check column compression encodings\n",
    "SELECT \n",
    "    \"column\",\n",
    "    type,\n",
    "    encoding,\n",
    "    distkey,\n",
    "    sortkey\n",
    "FROM pg_table_def\n",
    "WHERE tablename = 'marketing_events';\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Optimize with column encodings\n",
    "create_with_encoding = \"\"\"\n",
    "CREATE TABLE marketing_events_optimized (\n",
    "    event_id BIGINT ENCODE DELTA,           -- Delta encoding for sequential IDs\n",
    "    user_id INTEGER ENCODE DELTA32K,        -- Delta for IDs\n",
    "    campaign_id INTEGER ENCODE RUNLENGTH,   -- Runlength if sorted by campaign\n",
    "    channel VARCHAR(50) ENCODE LZO,         -- LZO for text\n",
    "    event_type VARCHAR(50) ENCODE BYTEDICT, -- Dictionary for low cardinality\n",
    "    revenue DECIMAL(10,2) ENCODE DELTA32K,\n",
    "    timestamp TIMESTAMP ENCODE DELTA32K,\n",
    "    date DATE ENCODE DELTA32K\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(date, user_id);\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Analyze compression\n",
    "analyze_compression_query = \"\"\"\n",
    "ANALYZE COMPRESSION marketing_events;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Columnar Storage Benefits:\n",
    "-------------------------\n",
    "1. Only read columns needed (not full rows)\n",
    "2. Better compression (similar values together)\n",
    "3. Faster aggregations on specific columns\n",
    "4. Efficient column statistics (min/max zone maps)\n",
    "\n",
    "Compression Encoding Types:\n",
    "--------------------------\n",
    "RAW:       No compression\n",
    "DELTA:     Good for sorted numeric columns\n",
    "DELTA32K:  Delta with 32K block size\n",
    "LZO:       General purpose compression\n",
    "BYTEDICT:  Dictionary for low cardinality (<50K unique)\n",
    "RUNLENGTH: Good for sorted columns with runs of same value\n",
    "ZSTD:      High compression ratio (newer)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Materialized Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a materialized view for daily campaign metrics\n",
    "create_mv_daily_metrics = \"\"\"\n",
    "CREATE MATERIALIZED VIEW mv_daily_campaign_metrics\n",
    "DISTKEY(campaign_id)\n",
    "SORTKEY(date)\n",
    "AS\n",
    "SELECT \n",
    "    campaign_id,\n",
    "    DATE_TRUNC('day', timestamp) as date,\n",
    "    channel,\n",
    "    COUNT(*) as total_events,\n",
    "    COUNT(DISTINCT user_id) as unique_users,\n",
    "    SUM(CASE WHEN event_type = 'impression' THEN 1 ELSE 0 END) as impressions,\n",
    "    SUM(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) as clicks,\n",
    "    SUM(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) as conversions,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    AVG(revenue) as avg_revenue\n",
    "FROM marketing_events\n",
    "GROUP BY campaign_id, DATE_TRUNC('day', timestamp), channel;\n",
    "\"\"\"\n",
    "\n",
    "# Refresh materialized view\n",
    "refresh_mv = \"\"\"\n",
    "REFRESH MATERIALIZED VIEW mv_daily_campaign_metrics;\n",
    "\"\"\"\n",
    "\n",
    "# Query using materialized view (much faster)\n",
    "query_mv = \"\"\"\n",
    "SELECT \n",
    "    campaign_id,\n",
    "    channel,\n",
    "    SUM(impressions) as total_impressions,\n",
    "    SUM(clicks) as total_clicks,\n",
    "    SUM(conversions) as total_conversions,\n",
    "    SUM(total_revenue) as revenue,\n",
    "    SUM(clicks) * 1.0 / NULLIF(SUM(impressions), 0) as ctr,\n",
    "    SUM(conversions) * 1.0 / NULLIF(SUM(clicks), 0) as cvr\n",
    "FROM mv_daily_campaign_metrics\n",
    "WHERE date >= '2024-01-01'\n",
    "  AND date < '2024-02-01'\n",
    "GROUP BY campaign_id, channel;\n",
    "\"\"\"\n",
    "\n",
    "# Auto-refresh materialized view (if supported)\n",
    "create_auto_refresh_mv = \"\"\"\n",
    "CREATE MATERIALIZED VIEW mv_hourly_metrics\n",
    "AUTO REFRESH YES\n",
    "AS\n",
    "SELECT \n",
    "    DATE_TRUNC('hour', timestamp) as hour,\n",
    "    channel,\n",
    "    COUNT(*) as events,\n",
    "    SUM(revenue) as revenue\n",
    "FROM marketing_events\n",
    "WHERE timestamp >= DATEADD(day, -7, GETDATE())\n",
    "GROUP BY DATE_TRUNC('hour', timestamp), channel;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Materialized View Best Practices:\n",
    "---------------------------------\n",
    "1. Use for frequently-run expensive aggregations\n",
    "2. Add distribution and sort keys to MVs\n",
    "3. Refresh on a schedule or use auto-refresh\n",
    "4. Monitor MV staleness\n",
    "5. Consider incremental refresh if available\n",
    "6. Don't over-use (storage cost vs query benefit)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complex Analytics Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Cohort Analysis\n",
    "cohort_analysis = \"\"\"\n",
    "WITH user_cohorts AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        DATE_TRUNC('month', MIN(timestamp)) as cohort_month\n",
    "    FROM marketing_events\n",
    "    WHERE event_type = 'conversion'\n",
    "    GROUP BY user_id\n",
    "),\n",
    "user_activity AS (\n",
    "    SELECT \n",
    "        c.cohort_month,\n",
    "        DATE_TRUNC('month', e.timestamp) as activity_month,\n",
    "        COUNT(DISTINCT e.user_id) as active_users,\n",
    "        SUM(e.revenue) as revenue\n",
    "    FROM user_cohorts c\n",
    "    INNER JOIN marketing_events e ON c.user_id = e.user_id\n",
    "    WHERE e.event_type = 'conversion'\n",
    "    GROUP BY c.cohort_month, DATE_TRUNC('month', e.timestamp)\n",
    ")\n",
    "SELECT \n",
    "    cohort_month,\n",
    "    activity_month,\n",
    "    DATEDIFF(month, cohort_month, activity_month) as months_since_first_conversion,\n",
    "    active_users,\n",
    "    revenue,\n",
    "    revenue / active_users as revenue_per_user\n",
    "FROM user_activity\n",
    "ORDER BY cohort_month, activity_month;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Funnel Analysis\n",
    "funnel_analysis = \"\"\"\n",
    "WITH user_events AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        MAX(CASE WHEN event_type = 'impression' THEN 1 ELSE 0 END) as has_impression,\n",
    "        MAX(CASE WHEN event_type = 'click' THEN 1 ELSE 0 END) as has_click,\n",
    "        MAX(CASE WHEN event_type = 'cart_add' THEN 1 ELSE 0 END) as has_cart_add,\n",
    "        MAX(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) as has_conversion\n",
    "    FROM marketing_events\n",
    "    WHERE date >= '2024-01-01'\n",
    "      AND date < '2024-02-01'\n",
    "    GROUP BY user_id\n",
    ")\n",
    "SELECT \n",
    "    SUM(has_impression) as impressions,\n",
    "    SUM(has_click) as clicks,\n",
    "    SUM(has_cart_add) as cart_adds,\n",
    "    SUM(has_conversion) as conversions,\n",
    "    SUM(has_click) * 100.0 / NULLIF(SUM(has_impression), 0) as impression_to_click_rate,\n",
    "    SUM(has_cart_add) * 100.0 / NULLIF(SUM(has_click), 0) as click_to_cart_rate,\n",
    "    SUM(has_conversion) * 100.0 / NULLIF(SUM(has_cart_add), 0) as cart_to_conversion_rate\n",
    "FROM user_events;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: RFM Segmentation\n",
    "rfm_segmentation = \"\"\"\n",
    "WITH user_metrics AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        DATEDIFF(day, MAX(timestamp), GETDATE()) as recency_days,\n",
    "        COUNT(*) as frequency,\n",
    "        SUM(revenue) as monetary\n",
    "    FROM marketing_events\n",
    "    WHERE event_type = 'conversion'\n",
    "    GROUP BY user_id\n",
    "),\n",
    "rfm_scores AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        recency_days,\n",
    "        frequency,\n",
    "        monetary,\n",
    "        NTILE(5) OVER (ORDER BY recency_days) as r_score,\n",
    "        NTILE(5) OVER (ORDER BY frequency DESC) as f_score,\n",
    "        NTILE(5) OVER (ORDER BY monetary DESC) as m_score\n",
    "    FROM user_metrics\n",
    ")\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN r_score >= 4 AND f_score >= 4 AND m_score >= 4 THEN 'Champions'\n",
    "        WHEN r_score >= 3 AND f_score >= 3 AND m_score >= 3 THEN 'Loyal Customers'\n",
    "        WHEN r_score >= 4 AND f_score <= 2 THEN 'New Customers'\n",
    "        WHEN r_score <= 2 AND f_score >= 3 THEN 'At Risk'\n",
    "        WHEN r_score <= 2 AND f_score <= 2 THEN 'Lost'\n",
    "        ELSE 'Other'\n",
    "    END as segment,\n",
    "    COUNT(*) as user_count,\n",
    "    AVG(recency_days) as avg_recency,\n",
    "    AVG(frequency) as avg_frequency,\n",
    "    AVG(monetary) as avg_monetary\n",
    "FROM rfm_scores\n",
    "GROUP BY segment\n",
    "ORDER BY user_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Time-Series Anomaly Detection\n",
    "anomaly_detection = \"\"\"\n",
    "WITH daily_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', timestamp) as date,\n",
    "        channel,\n",
    "        SUM(revenue) as daily_revenue\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= DATEADD(day, -90, GETDATE())\n",
    "    GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    "),\n",
    "stats AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        channel,\n",
    "        daily_revenue,\n",
    "        AVG(daily_revenue) OVER (\n",
    "            PARTITION BY channel \n",
    "            ORDER BY date \n",
    "            ROWS BETWEEN 29 PRECEDING AND 1 PRECEDING\n",
    "        ) as avg_30d,\n",
    "        STDDEV(daily_revenue) OVER (\n",
    "            PARTITION BY channel \n",
    "            ORDER BY date \n",
    "            ROWS BETWEEN 29 PRECEDING AND 1 PRECEDING\n",
    "        ) as stddev_30d\n",
    "    FROM daily_metrics\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    channel,\n",
    "    daily_revenue,\n",
    "    avg_30d,\n",
    "    CASE \n",
    "        WHEN daily_revenue > avg_30d + 2 * stddev_30d THEN 'High Anomaly'\n",
    "        WHEN daily_revenue < avg_30d - 2 * stddev_30d THEN 'Low Anomaly'\n",
    "        ELSE 'Normal'\n",
    "    END as anomaly_flag,\n",
    "    (daily_revenue - avg_30d) / NULLIF(stddev_30d, 0) as z_score\n",
    "FROM stats\n",
    "WHERE date >= DATEADD(day, -30, GETDATE())\n",
    "ORDER BY date DESC, channel;\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Advanced analytics queries defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Project: Build Marketing Data Warehouse\n",
    "\n",
    "### Complete data warehouse schema design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create fact table (events)\n",
    "create_fact_events = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_marketing_events (\n",
    "    event_id BIGINT NOT NULL,\n",
    "    user_id INTEGER NOT NULL,\n",
    "    campaign_id INTEGER,\n",
    "    channel_id INTEGER,\n",
    "    event_type_id INTEGER,\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    date DATE NOT NULL,\n",
    "    revenue DECIMAL(10,2),\n",
    "    cost DECIMAL(10,2),\n",
    "    session_id BIGINT,\n",
    "    device_type VARCHAR(50),\n",
    "    location_id INTEGER\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(date, timestamp)\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Create dimension tables\n",
    "create_dim_users = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dim_users (\n",
    "    user_id INTEGER NOT NULL,\n",
    "    email_hash VARCHAR(64),\n",
    "    signup_date DATE,\n",
    "    segment VARCHAR(50),\n",
    "    country VARCHAR(2),\n",
    "    acquisition_channel VARCHAR(50),\n",
    "    lifetime_value DECIMAL(10,2),\n",
    "    is_active BOOLEAN\n",
    ")\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(user_id)\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "create_dim_campaigns = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dim_campaigns (\n",
    "    campaign_id INTEGER NOT NULL,\n",
    "    campaign_name VARCHAR(200),\n",
    "    campaign_type VARCHAR(50),\n",
    "    channel_id INTEGER,\n",
    "    start_date DATE,\n",
    "    end_date DATE,\n",
    "    budget DECIMAL(10,2),\n",
    "    objective VARCHAR(50)\n",
    ")\n",
    "DISTSTYLE ALL\n",
    "SORTKEY(campaign_id)\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "create_dim_channels = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS dim_channels (\n",
    "    channel_id INTEGER NOT NULL,\n",
    "    channel_name VARCHAR(50),\n",
    "    channel_category VARCHAR(50),\n",
    "    is_paid BOOLEAN\n",
    ")\n",
    "DISTSTYLE ALL\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Create aggregated fact table\n",
    "create_fact_daily_summary = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS fact_daily_campaign_summary (\n",
    "    date DATE NOT NULL,\n",
    "    campaign_id INTEGER NOT NULL,\n",
    "    channel_id INTEGER,\n",
    "    impressions INTEGER,\n",
    "    clicks INTEGER,\n",
    "    conversions INTEGER,\n",
    "    unique_users INTEGER,\n",
    "    revenue DECIMAL(12,2),\n",
    "    cost DECIMAL(12,2),\n",
    "    PRIMARY KEY (date, campaign_id)\n",
    ")\n",
    "DISTKEY(campaign_id)\n",
    "SORTKEY(date)\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: ETL query to populate daily summary\n",
    "populate_daily_summary = \"\"\"\n",
    "INSERT INTO fact_daily_campaign_summary\n",
    "SELECT \n",
    "    date,\n",
    "    campaign_id,\n",
    "    channel_id,\n",
    "    SUM(CASE WHEN event_type_id = 1 THEN 1 ELSE 0 END) as impressions,\n",
    "    SUM(CASE WHEN event_type_id = 2 THEN 1 ELSE 0 END) as clicks,\n",
    "    SUM(CASE WHEN event_type_id = 3 THEN 1 ELSE 0 END) as conversions,\n",
    "    COUNT(DISTINCT user_id) as unique_users,\n",
    "    SUM(revenue) as revenue,\n",
    "    SUM(cost) as cost\n",
    "FROM fact_marketing_events\n",
    "WHERE date = CURRENT_DATE - 1  -- Yesterday's data\n",
    "GROUP BY date, campaign_id, channel_id;\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Create reporting views\n",
    "create_campaign_performance_view = \"\"\"\n",
    "CREATE OR REPLACE VIEW vw_campaign_performance AS\n",
    "SELECT \n",
    "    f.date,\n",
    "    c.campaign_name,\n",
    "    ch.channel_name,\n",
    "    f.impressions,\n",
    "    f.clicks,\n",
    "    f.conversions,\n",
    "    f.unique_users,\n",
    "    f.revenue,\n",
    "    f.cost,\n",
    "    f.revenue - f.cost as profit,\n",
    "    f.clicks * 100.0 / NULLIF(f.impressions, 0) as ctr,\n",
    "    f.conversions * 100.0 / NULLIF(f.clicks, 0) as cvr,\n",
    "    f.cost / NULLIF(f.clicks, 0) as cpc,\n",
    "    f.cost / NULLIF(f.conversions, 0) as cpa,\n",
    "    f.revenue / NULLIF(f.cost, 0) as roas\n",
    "FROM fact_daily_campaign_summary f\n",
    "LEFT JOIN dim_campaigns c ON f.campaign_id = c.campaign_id\n",
    "LEFT JOIN dim_channels ch ON f.channel_id = ch.channel_id;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Data Warehouse Design Best Practices:\n",
    "------------------------------------\n",
    "1. Use star schema (fact + dimensions)\n",
    "2. Distribute large fact tables by commonly joined keys\n",
    "3. Use DISTSTYLE ALL for small dimension tables\n",
    "4. Create pre-aggregated fact tables for common queries\n",
    "5. Use appropriate data types to minimize storage\n",
    "6. Implement incremental loading strategies\n",
    "7. Create views for common business logic\n",
    "8. Document table structures and dependencies\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Maintenance and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vacuum to reclaim space and resort\n",
    "vacuum_table = \"\"\"\n",
    "VACUUM SORT ONLY marketing_events;  -- Resort rows\n",
    "VACUUM DELETE ONLY marketing_events;  -- Reclaim space\n",
    "VACUUM FULL marketing_events;  -- Both resort and reclaim\n",
    "\"\"\"\n",
    "\n",
    "# Analyze to update statistics\n",
    "analyze_table = \"\"\"\n",
    "ANALYZE marketing_events;\n",
    "\"\"\"\n",
    "\n",
    "# Check for tables needing vacuum\n",
    "check_vacuum_needed = \"\"\"\n",
    "SELECT \n",
    "    \"schema\",\n",
    "    \"table\",\n",
    "    unsorted,\n",
    "    stats_off,\n",
    "    tbl_rows\n",
    "FROM svv_table_info\n",
    "WHERE unsorted > 5  -- More than 5% unsorted\n",
    "   OR stats_off > 5  -- Statistics more than 5% off\n",
    "ORDER BY unsorted DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Monitor running queries\n",
    "running_queries = \"\"\"\n",
    "SELECT \n",
    "    pid,\n",
    "    user_name,\n",
    "    starttime,\n",
    "    DATEDIFF(seconds, starttime, GETDATE()) as runtime_seconds,\n",
    "    TRIM(query_text) as query\n",
    "FROM stv_recents\n",
    "WHERE status = 'Running'\n",
    "ORDER BY runtime_seconds DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Cancel a query\n",
    "cancel_query = \"\"\"\n",
    "CANCEL <pid>;\n",
    "\"\"\"\n",
    "\n",
    "# Check WLM queue wait times\n",
    "wlm_queue_state = \"\"\"\n",
    "SELECT \n",
    "    service_class,\n",
    "    num_queued_queries,\n",
    "    num_executing_queries,\n",
    "    num_executed_queries\n",
    "FROM stv_wlm_service_class_state\n",
    "WHERE service_class >= 6;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Maintenance Best Practices:\n",
    "---------------------------\n",
    "1. Run VACUUM regularly (weekly for active tables)\n",
    "2. Run ANALYZE after significant data changes\n",
    "3. Monitor table statistics (svv_table_info)\n",
    "4. Check for data skew across slices\n",
    "5. Monitor query performance (stl_query)\n",
    "6. Set up alerts for slow queries\n",
    "7. Review and optimize WLM configuration\n",
    "8. Archive old data regularly\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices Summary\n",
    "\n",
    "### Table Design\n",
    "1. Choose appropriate distribution style (KEY, EVEN, ALL)\n",
    "2. Select sort keys based on query patterns\n",
    "3. Use compound sort keys for ordered queries\n",
    "4. Use interleaved sort keys for varied queries\n",
    "5. Optimize column encodings for compression\n",
    "\n",
    "### Query Optimization\n",
    "1. Use EXPLAIN to analyze query plans\n",
    "2. Select only required columns\n",
    "3. Filter early and filter often\n",
    "4. Match distribution keys in joins\n",
    "5. Use materialized views for complex aggregations\n",
    "\n",
    "### Performance\n",
    "1. Monitor query performance regularly\n",
    "2. Check for data skew\n",
    "3. Vacuum and analyze tables\n",
    "4. Use appropriate WLM queues\n",
    "5. Archive old data\n",
    "\n",
    "### Cost Optimization\n",
    "1. Use appropriate node types\n",
    "2. Implement data lifecycle policies\n",
    "3. Use Redshift Spectrum for cold data\n",
    "4. Monitor and optimize storage\n",
    "5. Pause clusters when not in use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercises\n",
    "\n",
    "### Exercise 1: Table Design\n",
    "Design a Redshift table schema for:\n",
    "1. 100M row events table with optimal distribution and sort keys\n",
    "2. Associated dimension tables\n",
    "3. Appropriate column encodings\n",
    "4. Justify your design choices\n",
    "\n",
    "### Exercise 2: Query Optimization\n",
    "1. Write a complex multi-table join query\n",
    "2. Use EXPLAIN to analyze the query plan\n",
    "3. Identify and fix performance issues\n",
    "4. Benchmark before and after optimization\n",
    "\n",
    "### Exercise 3: Window Functions\n",
    "Implement the following using window functions:\n",
    "1. Running totals by user\n",
    "2. 7-day moving average by channel\n",
    "3. First and last touch attribution\n",
    "4. User journey sequence analysis\n",
    "\n",
    "### Exercise 4: Data Warehouse\n",
    "Build a complete data warehouse:\n",
    "1. Design star schema\n",
    "2. Create fact and dimension tables\n",
    "3. Implement ETL process\n",
    "4. Create reporting views\n",
    "5. Set up maintenance procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "### Documentation\n",
    "- [Redshift Best Practices](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)\n",
    "- [Redshift SQL Reference](https://docs.aws.amazon.com/redshift/latest/dg/c_SQL_reference.html)\n",
    "- [Distribution Styles](https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html)\n",
    "- [Window Functions](https://docs.aws.amazon.com/redshift/latest/dg/c_Window_functions.html)\n",
    "\n",
    "### Tools\n",
    "- AWS Redshift Console: Query editor and monitoring\n",
    "- DBeaver: SQL IDE with Redshift support\n",
    "- SQL Workbench/J: Redshift management tool\n",
    "- CloudWatch: Performance monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
