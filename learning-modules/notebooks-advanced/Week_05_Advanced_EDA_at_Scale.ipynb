{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Advanced EDA at Scale\n",
    "\n",
    "## Overview\n",
    "Master exploratory data analysis on massive datasets (10M+ rows) using Redshift, Dask, and advanced statistical techniques.\n",
    "\n",
    "## Learning Objectives\n",
    "- Perform EDA on 10M+ row datasets efficiently\n",
    "- Use Redshift SQL for statistical analysis at scale\n",
    "- Implement distributed computing with Dask\n",
    "- Apply advanced outlier detection techniques\n",
    "- Build automated data profiling pipelines\n",
    "- Design effective sampling strategies\n",
    "\n",
    "## Prerequisites\n",
    "- Redshift cluster access\n",
    "- 16GB+ RAM for local processing\n",
    "- Dask installed\n",
    "- Large marketing dataset (provided)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Environment](#setup)\n",
    "2. [EDA on Large Datasets](#eda-large)\n",
    "3. [Statistical Analysis with Redshift](#stats-redshift)\n",
    "4. [Distributed Computing with Dask](#dask)\n",
    "5. [Advanced Outlier Detection](#outliers)\n",
    "6. [Correlation Analysis at Scale](#correlation)\n",
    "7. [Automated Data Profiling](#profiling)\n",
    "8. [Sampling Strategies](#sampling)\n",
    "9. [Real-World Project: 100M Row Analysis](#project)\n",
    "10. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q psycopg2-binary redshift_connector dask[complete] pandas numpy scipy statsmodels\n",
    "!pip install -q seaborn matplotlib plotly great_expectations pandas-profiling\n",
    "!pip install -q sqlalchemy pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import redshift_connector\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy import stats\n",
    "from statsmodels import robust\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift connection configuration\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Get credentials (use environment variables or prompt)\n",
    "REDSHIFT_HOST = os.getenv('REDSHIFT_HOST', 'your-cluster.redshift.amazonaws.com')\n",
    "REDSHIFT_PORT = os.getenv('REDSHIFT_PORT', '5439')\n",
    "REDSHIFT_DB = os.getenv('REDSHIFT_DB', 'marketing_db')\n",
    "REDSHIFT_USER = os.getenv('REDSHIFT_USER', input('Redshift username: '))\n",
    "REDSHIFT_PASSWORD = os.getenv('REDSHIFT_PASSWORD', getpass('Redshift password: '))\n",
    "\n",
    "class RedshiftConnection:\n",
    "    \"\"\"Manages Redshift database connections with connection pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, host, port, database, user, password):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Redshift\"\"\"\n",
    "        try:\n",
    "            self.conn = redshift_connector.connect(\n",
    "                host=self.host,\n",
    "                port=int(self.port),\n",
    "                database=self.database,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"✓ Connected to Redshift: {self.database}\")\n",
    "            return self.conn\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Connection failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def execute_query(self, query, fetch=True):\n",
    "        \"\"\"Execute SQL query and return results\"\"\"\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        if fetch:\n",
    "            result = cursor.fetchall()\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            return pd.DataFrame(result, columns=columns)\n",
    "        else:\n",
    "            self.conn.commit()\n",
    "            return None\n",
    "    \n",
    "    def read_table(self, table_name, limit=None, where=None):\n",
    "        \"\"\"Read table into pandas DataFrame\"\"\"\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        if where:\n",
    "            query += f\" WHERE {where}\"\n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_table_stats(self, table_name):\n",
    "        \"\"\"Get basic statistics about a table\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as row_count,\n",
    "            COUNT(DISTINCT *) as distinct_rows,\n",
    "            pg_size_pretty(pg_total_relation_size('{table_name}')) as size\n",
    "        FROM {table_name}\n",
    "        \"\"\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close connection\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"✓ Connection closed\")\n",
    "\n",
    "# Initialize connection\n",
    "rs = RedshiftConnection(REDSHIFT_HOST, REDSHIFT_PORT, REDSHIFT_DB, \n",
    "                        REDSHIFT_USER, REDSHIFT_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Large Dataset (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_marketing_dataset(n_rows=10_000_000):\n",
    "    \"\"\"\n",
    "    Create a large synthetic marketing dataset for testing\n",
    "    \n",
    "    Dataset includes:\n",
    "    - User activity data\n",
    "    - Campaign performance\n",
    "    - Conversion events\n",
    "    - Revenue data\n",
    "    \"\"\"\n",
    "    print(f\"Generating {n_rows:,} rows...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate in chunks to avoid memory issues\n",
    "    chunk_size = 1_000_000\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, n_rows, chunk_size):\n",
    "        chunk_rows = min(chunk_size, n_rows - i)\n",
    "        \n",
    "        chunk_df = pd.DataFrame({\n",
    "            'user_id': np.random.randint(1, 1_000_000, chunk_rows),\n",
    "            'session_id': np.random.randint(1, 5_000_000, chunk_rows),\n",
    "            'timestamp': pd.date_range('2023-01-01', periods=chunk_rows, freq='30s'),\n",
    "            'campaign_id': np.random.choice(['campaign_a', 'campaign_b', 'campaign_c', \n",
    "                                            'campaign_d', 'campaign_e'], chunk_rows),\n",
    "            'channel': np.random.choice(['facebook', 'google', 'email', 'organic', 'direct'], \n",
    "                                       chunk_rows, p=[0.3, 0.25, 0.2, 0.15, 0.1]),\n",
    "            'device': np.random.choice(['mobile', 'desktop', 'tablet'], chunk_rows, \n",
    "                                      p=[0.6, 0.3, 0.1]),\n",
    "            'country': np.random.choice(['US', 'UK', 'CA', 'AU', 'DE', 'FR'], chunk_rows,\n",
    "                                       p=[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]),\n",
    "            'page_views': np.random.poisson(3, chunk_rows),\n",
    "            'time_on_site': np.random.exponential(120, chunk_rows),\n",
    "            'converted': np.random.choice([0, 1], chunk_rows, p=[0.97, 0.03]),\n",
    "            'revenue': 0.0\n",
    "        })\n",
    "        \n",
    "        # Add revenue for conversions\n",
    "        chunk_df.loc[chunk_df['converted'] == 1, 'revenue'] = \\\n",
    "            np.random.lognormal(3.5, 1.0, (chunk_df['converted'] == 1).sum())\n",
    "        \n",
    "        chunks.append(chunk_df)\n",
    "        print(f\"  Generated {i + chunk_rows:,} rows\")\n",
    "    \n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    print(f\"✓ Dataset created: {len(df):,} rows, {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    return df\n",
    "\n",
    "# Create sample dataset (comment out if using real data)\n",
    "# df_large = create_large_marketing_dataset(10_000_000)\n",
    "# df_large.to_parquet('marketing_data_10m.parquet', engine='pyarrow', compression='snappy')\n",
    "print(\"Sample dataset generation code ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA on Large Datasets <a name=\"eda-large\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: SQL-First Approach (Redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_table_redshift(rs_conn, table_name):\n",
    "    \"\"\"\n",
    "    Comprehensive profiling of large table using Redshift\n",
    "    Fast for large datasets - computation happens in database\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get column information\n",
    "    col_query = f\"\"\"\n",
    "    SELECT \n",
    "        column_name, \n",
    "        data_type,\n",
    "        is_nullable\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_name = '{table_name}'\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\"\n",
    "    columns_info = rs_conn.execute_query(col_query)\n",
    "    \n",
    "    # Get row count and basic stats\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT user_id) as unique_users,\n",
    "        MIN(timestamp) as min_date,\n",
    "        MAX(timestamp) as max_date,\n",
    "        SUM(CASE WHEN converted = 1 THEN 1 ELSE 0 END) as total_conversions,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        AVG(revenue) as avg_revenue,\n",
    "        AVG(page_views) as avg_page_views,\n",
    "        AVG(time_on_site) as avg_time_on_site\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    basic_stats = rs_conn.execute_query(stats_query)\n",
    "    \n",
    "    # Get null counts for each column\n",
    "    null_checks = []\n",
    "    for col in columns_info['column_name']:\n",
    "        null_checks.append(f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) as {col}_nulls\")\n",
    "    \n",
    "    null_query = f\"\"\"\n",
    "    SELECT \n",
    "        {', '.join(null_checks)}\n",
    "    FROM {table_name}\n",
    "    \"\"\"\n",
    "    null_stats = rs_conn.execute_query(null_query)\n",
    "    \n",
    "    return {\n",
    "        'columns': columns_info,\n",
    "        'basic_stats': basic_stats,\n",
    "        'null_stats': null_stats\n",
    "    }\n",
    "\n",
    "# Example usage (uncomment when connected to Redshift)\n",
    "# profile = profile_table_redshift(rs, 'marketing_events')\n",
    "# print(\"Table Profile:\")\n",
    "# print(profile['basic_stats'])\n",
    "# print(\"\\nNull Counts:\")\n",
    "# print(profile['null_stats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Smart Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_redshift(rs_conn, table_name, sample_size=100000, strata_col='channel'):\n",
    "    \"\"\"\n",
    "    Create stratified sample from large table\n",
    "    Ensures representative sample across important dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get strata proportions\n",
    "    prop_query = f\"\"\"\n",
    "    SELECT \n",
    "        {strata_col},\n",
    "        COUNT(*) as count,\n",
    "        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage\n",
    "    FROM {table_name}\n",
    "    GROUP BY {strata_col}\n",
    "    \"\"\"\n",
    "    proportions = rs_conn.execute_query(prop_query)\n",
    "    \n",
    "    # Calculate sample size per stratum\n",
    "    proportions['sample_size'] = (proportions['percentage'] / 100 * sample_size).astype(int)\n",
    "    \n",
    "    # Build sampling query\n",
    "    samples = []\n",
    "    for _, row in proportions.iterrows():\n",
    "        sample_query = f\"\"\"\n",
    "        SELECT * FROM {table_name}\n",
    "        WHERE {strata_col} = '{row[strata_col]}'\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT {row['sample_size']}\n",
    "        \"\"\"\n",
    "        samples.append(sample_query)\n",
    "    \n",
    "    # Combine samples\n",
    "    combined_query = ' UNION ALL '.join(samples)\n",
    "    sample_df = rs_conn.execute_query(combined_query)\n",
    "    \n",
    "    print(f\"✓ Stratified sample created: {len(sample_df):,} rows\")\n",
    "    print(\"\\nSample distribution:\")\n",
    "    print(sample_df[strata_col].value_counts())\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Example usage\n",
    "# sample_df = stratified_sample_redshift(rs, 'marketing_events', 100000, 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Dask for Local Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask client\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "print(f\"Dask dashboard available at: {client.dashboard_link}\")\n",
    "\n",
    "def eda_with_dask(file_path):\n",
    "    \"\"\"\n",
    "    Perform EDA on large parquet file using Dask\n",
    "    Handles datasets larger than RAM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read with Dask\n",
    "    ddf = dd.read_parquet(file_path)\n",
    "    \n",
    "    print(f\"Dataset info:\")\n",
    "    print(f\"  Partitions: {ddf.npartitions}\")\n",
    "    print(f\"  Columns: {len(ddf.columns)}\")\n",
    "    print(f\"  Dtypes:\\n{ddf.dtypes}\")\n",
    "    \n",
    "    # Basic statistics (computed lazily)\n",
    "    stats = {\n",
    "        'row_count': ddf.shape[0].compute(),\n",
    "        'memory_usage': ddf.memory_usage(deep=True).sum().compute() / 1024**2,\n",
    "        'numeric_summary': ddf.describe().compute(),\n",
    "        'null_counts': ddf.isnull().sum().compute(),\n",
    "        'value_counts': {}\n",
    "    }\n",
    "    \n",
    "    # Value counts for categorical columns\n",
    "    cat_cols = ddf.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        stats['value_counts'][col] = ddf[col].value_counts().compute()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Example usage\n",
    "# stats = eda_with_dask('marketing_data_10m.parquet')\n",
    "# print(stats['numeric_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis with Redshift <a name=\"stats-redshift\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_stats_redshift(rs_conn, table_name, numeric_col):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics using Redshift SQL\n",
    "    Much faster than pulling data to pandas for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH stats AS (\n",
    "        SELECT \n",
    "            COUNT(*) as n,\n",
    "            AVG({numeric_col}) as mean,\n",
    "            STDDEV({numeric_col}) as std,\n",
    "            MIN({numeric_col}) as min,\n",
    "            MAX({numeric_col}) as max,\n",
    "            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {numeric_col}) as q25,\n",
    "            PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY {numeric_col}) as median,\n",
    "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {numeric_col}) as q75,\n",
    "            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY {numeric_col}) as p95,\n",
    "            PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY {numeric_col}) as p99\n",
    "        FROM {table_name}\n",
    "        WHERE {numeric_col} IS NOT NULL\n",
    "    ),\n",
    "    moments AS (\n",
    "        SELECT\n",
    "            SUM(POWER({numeric_col} - (SELECT mean FROM stats), 3)) / \n",
    "                (COUNT(*) * POWER((SELECT std FROM stats), 3)) as skewness,\n",
    "            SUM(POWER({numeric_col} - (SELECT mean FROM stats), 4)) / \n",
    "                (COUNT(*) * POWER((SELECT std FROM stats), 4)) - 3 as kurtosis\n",
    "        FROM {table_name}\n",
    "        WHERE {numeric_col} IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        s.*,\n",
    "        s.q75 - s.q25 as iqr,\n",
    "        m.skewness,\n",
    "        m.kurtosis\n",
    "    FROM stats s, moments m\n",
    "    \"\"\"\n",
    "    \n",
    "    return rs_conn.execute_query(query)\n",
    "\n",
    "# Example usage\n",
    "# stats = comprehensive_stats_redshift(rs, 'marketing_events', 'revenue')\n",
    "# print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_analysis_redshift(rs_conn, table_name, numeric_col, bins=20):\n",
    "    \"\"\"\n",
    "    Analyze distribution of numeric column using binning\n",
    "    Creates histogram data efficiently in Redshift\n",
    "    \"\"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH bounds AS (\n",
    "        SELECT \n",
    "            MIN({numeric_col}) as min_val,\n",
    "            MAX({numeric_col}) as max_val,\n",
    "            (MAX({numeric_col}) - MIN({numeric_col})) / {bins} as bin_width\n",
    "        FROM {table_name}\n",
    "        WHERE {numeric_col} IS NOT NULL\n",
    "    ),\n",
    "    binned AS (\n",
    "        SELECT \n",
    "            FLOOR(({numeric_col} - b.min_val) / b.bin_width) as bin_num,\n",
    "            b.min_val + FLOOR(({numeric_col} - b.min_val) / b.bin_width) * b.bin_width as bin_start,\n",
    "            b.min_val + (FLOOR(({numeric_col} - b.min_val) / b.bin_width) + 1) * b.bin_width as bin_end\n",
    "        FROM {table_name}, bounds b\n",
    "        WHERE {numeric_col} IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        bin_start,\n",
    "        bin_end,\n",
    "        (bin_start + bin_end) / 2 as bin_center,\n",
    "        COUNT(*) as frequency,\n",
    "        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage\n",
    "    FROM binned\n",
    "    GROUP BY bin_start, bin_end\n",
    "    ORDER BY bin_start\n",
    "    \"\"\"\n",
    "    \n",
    "    return rs_conn.execute_query(query)\n",
    "\n",
    "# Example usage\n",
    "# dist = distribution_analysis_redshift(rs, 'marketing_events', 'revenue', bins=30)\n",
    "# plt.bar(dist['bin_center'], dist['frequency'])\n",
    "# plt.xlabel('Revenue')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Revenue Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distributed Computing with Dask <a name=\"dask\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_eda_dask(ddf):\n",
    "    \"\"\"\n",
    "    Comprehensive EDA using Dask's parallel computing\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic info\n",
    "    results['shape'] = (ddf.shape[0].compute(), len(ddf.columns))\n",
    "    results['dtypes'] = ddf.dtypes.to_dict()\n",
    "    results['memory_mb'] = ddf.memory_usage(deep=True).sum().compute() / 1024**2\n",
    "    \n",
    "    # 2. Numeric summaries (parallel computation)\n",
    "    numeric_cols = ddf.select_dtypes(include=[np.number]).columns\n",
    "    results['numeric_summary'] = ddf[numeric_cols].describe().compute()\n",
    "    \n",
    "    # 3. Missing values\n",
    "    results['missing'] = ddf.isnull().sum().compute()\n",
    "    results['missing_pct'] = (results['missing'] / results['shape'][0] * 100).round(2)\n",
    "    \n",
    "    # 4. Categorical summaries\n",
    "    cat_cols = ddf.select_dtypes(include=['object']).columns\n",
    "    results['categorical_stats'] = {}\n",
    "    for col in cat_cols:\n",
    "        results['categorical_stats'][col] = {\n",
    "            'unique_count': ddf[col].nunique().compute(),\n",
    "            'top_10': ddf[col].value_counts().nlargest(10).compute()\n",
    "        }\n",
    "    \n",
    "    # 5. Correlations (computed in parallel)\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Sample for correlation to avoid memory issues\n",
    "        sample = ddf[numeric_cols].sample(frac=0.1).compute()\n",
    "        results['correlation'] = sample.corr()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "# ddf = dd.read_parquet('marketing_data_10m.parquet')\n",
    "# eda_results = parallel_eda_dask(ddf)\n",
    "# print(f\"Dataset shape: {eda_results['shape']}\")\n",
    "# print(f\"Memory usage: {eda_results['memory_mb']:.2f} MB\")\n",
    "# print(\"\\nNumeric summary:\")\n",
    "# print(eda_results['numeric_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_analysis_dask(ddf, group_col, agg_col):\n",
    "    \"\"\"\n",
    "    Perform grouped analysis using Dask\n",
    "    Efficiently handles large groupby operations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parallel groupby\n",
    "    grouped = ddf.groupby(group_col)[agg_col].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]).compute()\n",
    "    \n",
    "    # Add percentiles (requires custom aggregation)\n",
    "    def percentile_25(x):\n",
    "        return x.quantile(0.25)\n",
    "    \n",
    "    def percentile_75(x):\n",
    "        return x.quantile(0.75)\n",
    "    \n",
    "    grouped['p25'] = ddf.groupby(group_col)[agg_col].apply(\n",
    "        percentile_25, meta=(agg_col, 'float64')\n",
    "    ).compute()\n",
    "    \n",
    "    grouped['p75'] = ddf.groupby(group_col)[agg_col].apply(\n",
    "        percentile_75, meta=(agg_col, 'float64')\n",
    "    ).compute()\n",
    "    \n",
    "    return grouped.sort_values('count', ascending=False)\n",
    "\n",
    "# Example usage\n",
    "# channel_analysis = group_analysis_dask(ddf, 'channel', 'revenue')\n",
    "# print(channel_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Outlier Detection <a name=\"outliers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_redshift(rs_conn, table_name, numeric_col, method='iqr'):\n",
    "    \"\"\"\n",
    "    Detect outliers using Redshift SQL\n",
    "    Methods: 'iqr', 'zscore', 'mad'\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        query = f\"\"\"\n",
    "        WITH quartiles AS (\n",
    "            SELECT \n",
    "                PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {numeric_col}) as q1,\n",
    "                PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {numeric_col}) as q3\n",
    "            FROM {table_name}\n",
    "            WHERE {numeric_col} IS NOT NULL\n",
    "        ),\n",
    "        bounds AS (\n",
    "            SELECT \n",
    "                q1,\n",
    "                q3,\n",
    "                q3 - q1 as iqr,\n",
    "                q1 - 1.5 * (q3 - q1) as lower_bound,\n",
    "                q3 + 1.5 * (q3 - q1) as upper_bound\n",
    "            FROM quartiles\n",
    "        )\n",
    "        SELECT \n",
    "            COUNT(*) as total_outliers,\n",
    "            SUM(CASE WHEN {numeric_col} < b.lower_bound THEN 1 ELSE 0 END) as lower_outliers,\n",
    "            SUM(CASE WHEN {numeric_col} > b.upper_bound THEN 1 ELSE 0 END) as upper_outliers,\n",
    "            b.lower_bound,\n",
    "            b.upper_bound,\n",
    "            b.iqr\n",
    "        FROM {table_name}, bounds b\n",
    "        WHERE {numeric_col} IS NOT NULL\n",
    "            AND ({numeric_col} < b.lower_bound OR {numeric_col} > b.upper_bound)\n",
    "        GROUP BY b.lower_bound, b.upper_bound, b.iqr\n",
    "        \"\"\"\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        query = f\"\"\"\n",
    "        WITH stats AS (\n",
    "            SELECT \n",
    "                AVG({numeric_col}) as mean,\n",
    "                STDDEV({numeric_col}) as std\n",
    "            FROM {table_name}\n",
    "            WHERE {numeric_col} IS NOT NULL\n",
    "        )\n",
    "        SELECT \n",
    "            COUNT(*) as total_outliers,\n",
    "            MIN(ABS(({numeric_col} - s.mean) / s.std)) as min_zscore,\n",
    "            MAX(ABS(({numeric_col} - s.mean) / s.std)) as max_zscore,\n",
    "            s.mean,\n",
    "            s.std\n",
    "        FROM {table_name}, stats s\n",
    "        WHERE {numeric_col} IS NOT NULL\n",
    "            AND ABS(({numeric_col} - s.mean) / s.std) > 3\n",
    "        GROUP BY s.mean, s.std\n",
    "        \"\"\"\n",
    "    \n",
    "    return rs_conn.execute_query(query)\n",
    "\n",
    "# Example usage\n",
    "# outliers = detect_outliers_redshift(rs, 'marketing_events', 'revenue', 'iqr')\n",
    "# print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_outliers_dask(ddf, numeric_cols):\n",
    "    \"\"\"\n",
    "    Detect multivariate outliers using Isolation Forest\n",
    "    Handles large datasets with Dask\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    \n",
    "    # Sample for training (IF doesn't need all data)\n",
    "    sample = ddf[numeric_cols].sample(frac=0.01).compute()\n",
    "    \n",
    "    # Train Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "    iso_forest.fit(sample)\n",
    "    \n",
    "    # Apply to full dataset in chunks\n",
    "    def predict_outliers(partition):\n",
    "        return pd.Series(\n",
    "            iso_forest.predict(partition[numeric_cols]),\n",
    "            index=partition.index\n",
    "        )\n",
    "    \n",
    "    ddf['outlier'] = ddf.map_partitions(\n",
    "        predict_outliers,\n",
    "        meta=('outlier', 'int64')\n",
    "    )\n",
    "    \n",
    "    outlier_count = (ddf['outlier'] == -1).sum().compute()\n",
    "    total_count = len(ddf)\n",
    "    \n",
    "    print(f\"Outliers detected: {outlier_count:,} ({outlier_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return ddf[ddf['outlier'] == -1].compute()\n",
    "\n",
    "# Example usage\n",
    "# outliers = multivariate_outliers_dask(ddf, ['revenue', 'page_views', 'time_on_site'])\n",
    "# print(outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis at Scale <a name=\"correlation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix_redshift(rs_conn, table_name, cols):\n",
    "    \"\"\"\n",
    "    Calculate correlation matrix using Redshift\n",
    "    Efficient for large datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    for i, col1 in enumerate(cols):\n",
    "        for col2 in cols[i:]:\n",
    "            query = f\"\"\"\n",
    "            WITH stats AS (\n",
    "                SELECT \n",
    "                    AVG({col1}) as mean1,\n",
    "                    AVG({col2}) as mean2,\n",
    "                    STDDEV({col1}) as std1,\n",
    "                    STDDEV({col2}) as std2\n",
    "                FROM {table_name}\n",
    "                WHERE {col1} IS NOT NULL AND {col2} IS NOT NULL\n",
    "            )\n",
    "            SELECT \n",
    "                SUM(({col1} - s.mean1) * ({col2} - s.mean2)) / \n",
    "                    (COUNT(*) * s.std1 * s.std2) as correlation\n",
    "            FROM {table_name}, stats s\n",
    "            WHERE {col1} IS NOT NULL AND {col2} IS NOT NULL\n",
    "            \"\"\"\n",
    "            \n",
    "            result = rs_conn.execute_query(query)\n",
    "            corr_value = result['correlation'].iloc[0]\n",
    "            correlations[(col1, col2)] = corr_value\n",
    "            if col1 != col2:\n",
    "                correlations[(col2, col1)] = corr_value\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    corr_df = pd.DataFrame(index=cols, columns=cols, dtype=float)\n",
    "    for (col1, col2), value in correlations.items():\n",
    "        corr_df.loc[col1, col2] = value\n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "# Example usage\n",
    "# numeric_cols = ['revenue', 'page_views', 'time_on_site']\n",
    "# corr_matrix = correlation_matrix_redshift(rs, 'marketing_events', numeric_cols)\n",
    "# \n",
    "# sns.heatmap(corr_matrix.astype(float), annot=True, cmap='coolwarm', center=0)\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Data Profiling <a name=\"profiling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeScaleProfiler:\n",
    "    \"\"\"\n",
    "    Automated profiling pipeline for large datasets\n",
    "    Combines Redshift SQL and Dask for comprehensive analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rs_conn, table_name):\n",
    "        self.rs_conn = rs_conn\n",
    "        self.table_name = table_name\n",
    "        self.profile = {}\n",
    "    \n",
    "    def run_full_profile(self):\n",
    "        \"\"\"Run complete profiling pipeline\"\"\"\n",
    "        print(\"Starting comprehensive profile...\")\n",
    "        \n",
    "        self.profile['metadata'] = self._get_metadata()\n",
    "        print(\"✓ Metadata collected\")\n",
    "        \n",
    "        self.profile['row_stats'] = self._get_row_stats()\n",
    "        print(\"✓ Row statistics calculated\")\n",
    "        \n",
    "        self.profile['column_profiles'] = self._profile_columns()\n",
    "        print(\"✓ Column profiling complete\")\n",
    "        \n",
    "        self.profile['data_quality'] = self._assess_quality()\n",
    "        print(\"✓ Data quality assessment complete\")\n",
    "        \n",
    "        self.profile['relationships'] = self._analyze_relationships()\n",
    "        print(\"✓ Relationship analysis complete\")\n",
    "        \n",
    "        return self.profile\n",
    "    \n",
    "    def _get_metadata(self):\n",
    "        \"\"\"Get table metadata\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type,\n",
    "            is_nullable,\n",
    "            character_maximum_length\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_name = '{self.table_name}'\n",
    "        ORDER BY ordinal_position\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def _get_row_stats(self):\n",
    "        \"\"\"Get row-level statistics\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(DISTINCT *) as unique_rows,\n",
    "            COUNT(*) - COUNT(DISTINCT *) as duplicate_rows\n",
    "        FROM {self.table_name}\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def _profile_columns(self):\n",
    "        \"\"\"Profile each column\"\"\"\n",
    "        metadata = self.profile.get('metadata', self._get_metadata())\n",
    "        profiles = {}\n",
    "        \n",
    "        for _, col_info in metadata.iterrows():\n",
    "            col_name = col_info['column_name']\n",
    "            data_type = col_info['data_type']\n",
    "            \n",
    "            if 'int' in data_type or 'numeric' in data_type or 'double' in data_type:\n",
    "                profiles[col_name] = self._profile_numeric_column(col_name)\n",
    "            else:\n",
    "                profiles[col_name] = self._profile_categorical_column(col_name)\n",
    "        \n",
    "        return profiles\n",
    "    \n",
    "    def _profile_numeric_column(self, col_name):\n",
    "        \"\"\"Profile numeric column\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as count,\n",
    "            COUNT({col_name}) as non_null,\n",
    "            COUNT(DISTINCT {col_name}) as unique,\n",
    "            AVG({col_name}) as mean,\n",
    "            STDDEV({col_name}) as std,\n",
    "            MIN({col_name}) as min,\n",
    "            MAX({col_name}) as max,\n",
    "            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {col_name}) as q25,\n",
    "            PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY {col_name}) as median,\n",
    "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {col_name}) as q75\n",
    "        FROM {self.table_name}\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query).iloc[0].to_dict()\n",
    "    \n",
    "    def _profile_categorical_column(self, col_name):\n",
    "        \"\"\"Profile categorical column\"\"\"\n",
    "        query = f\"\"\"\n",
    "        WITH counts AS (\n",
    "            SELECT \n",
    "                COUNT(*) as total_count,\n",
    "                COUNT({col_name}) as non_null,\n",
    "                COUNT(DISTINCT {col_name}) as unique_count\n",
    "            FROM {self.table_name}\n",
    "        ),\n",
    "        top_values AS (\n",
    "            SELECT \n",
    "                {col_name},\n",
    "                COUNT(*) as frequency\n",
    "            FROM {self.table_name}\n",
    "            WHERE {col_name} IS NOT NULL\n",
    "            GROUP BY {col_name}\n",
    "            ORDER BY frequency DESC\n",
    "            LIMIT 10\n",
    "        )\n",
    "        SELECT \n",
    "            c.total_count as count,\n",
    "            c.non_null,\n",
    "            c.unique_count as unique,\n",
    "            c.non_null * 1.0 / c.total_count as completeness\n",
    "        FROM counts c\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query).iloc[0].to_dict()\n",
    "    \n",
    "    def _assess_quality(self):\n",
    "        \"\"\"Assess overall data quality\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for high null rates\n",
    "        for col, profile in self.profile.get('column_profiles', {}).items():\n",
    "            null_rate = 1 - (profile.get('non_null', 0) / profile.get('count', 1))\n",
    "            if null_rate > 0.5:\n",
    "                issues.append(f\"{col}: High null rate ({null_rate*100:.1f}%)\")\n",
    "        \n",
    "        # Check for low cardinality in numeric columns\n",
    "        for col, profile in self.profile.get('column_profiles', {}).items():\n",
    "            if 'mean' in profile:  # Numeric column\n",
    "                cardinality = profile.get('unique', 0) / profile.get('count', 1)\n",
    "                if cardinality < 0.01:\n",
    "                    issues.append(f\"{col}: Low cardinality ({cardinality*100:.2f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'issues_found': len(issues),\n",
    "            'issues': issues,\n",
    "            'quality_score': max(0, 100 - len(issues) * 5)\n",
    "        }\n",
    "    \n",
    "    def _analyze_relationships(self):\n",
    "        \"\"\"Analyze relationships between columns\"\"\"\n",
    "        # This would include correlation analysis, dependency detection, etc.\n",
    "        return {\"status\": \"Analysis complete\"}\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate human-readable report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\"*60)\n",
    "        report.append(f\"DATA PROFILE: {self.table_name}\")\n",
    "        report.append(\"=\"*60)\n",
    "        \n",
    "        # Row stats\n",
    "        row_stats = self.profile.get('row_stats')\n",
    "        if row_stats is not None and not row_stats.empty:\n",
    "            report.append(\"\\nROW STATISTICS:\")\n",
    "            report.append(f\"  Total rows: {row_stats['total_rows'].iloc[0]:,}\")\n",
    "            report.append(f\"  Unique rows: {row_stats['unique_rows'].iloc[0]:,}\")\n",
    "        \n",
    "        # Data quality\n",
    "        quality = self.profile.get('data_quality', {})\n",
    "        report.append(f\"\\nDATA QUALITY SCORE: {quality.get('quality_score', 0)}/100\")\n",
    "        if quality.get('issues'):\n",
    "            report.append(\"\\nISSUES FOUND:\")\n",
    "            for issue in quality['issues'][:5]:  # Top 5 issues\n",
    "                report.append(f\"  - {issue}\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Example usage\n",
    "# profiler = LargeScaleProfiler(rs, 'marketing_events')\n",
    "# profile = profiler.run_full_profile()\n",
    "# print(profiler.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sampling Strategies <a name=\"sampling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(rs_conn, table_name, sample_size):\n",
    "    \"\"\"\n",
    "    Implement reservoir sampling for true random sampling\n",
    "    Works efficiently on large tables without knowing total size upfront\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {table_name}\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT {sample_size}\n",
    "    \"\"\"\n",
    "    return rs_conn.execute_query(query)\n",
    "\n",
    "def systematic_sampling(rs_conn, table_name, every_nth=100):\n",
    "    \"\"\"\n",
    "    Systematic sampling - select every nth row\n",
    "    Faster than random sampling, deterministic\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    WITH numbered AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            ROW_NUMBER() OVER (ORDER BY timestamp) as row_num\n",
    "        FROM {table_name}\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM numbered\n",
    "    WHERE row_num % {every_nth} = 0\n",
    "    \"\"\"\n",
    "    return rs_conn.execute_query(query)\n",
    "\n",
    "def time_stratified_sampling(rs_conn, table_name, time_col, sample_per_day=1000):\n",
    "    \"\"\"\n",
    "    Time-based stratified sampling\n",
    "    Ensures representation across time periods\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    WITH daily_data AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            DATE({time_col}) as date,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY DATE({time_col}) \n",
    "                ORDER BY RANDOM()\n",
    "            ) as rn\n",
    "        FROM {table_name}\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM daily_data\n",
    "    WHERE rn <= {sample_per_day}\n",
    "    \"\"\"\n",
    "    return rs_conn.execute_query(query)\n",
    "\n",
    "# Example usage\n",
    "# random_sample = reservoir_sampling(rs, 'marketing_events', 100000)\n",
    "# systematic_sample = systematic_sampling(rs, 'marketing_events', 100)\n",
    "# time_sample = time_stratified_sampling(rs, 'marketing_events', 'timestamp', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Project: Profile 100M Row Marketing Database <a name=\"project\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PROJECT: Comprehensive Analysis of Large-Scale Marketing Database\n",
    "\n",
    "Scenario:\n",
    "You have a Redshift table with 100M+ rows of marketing event data.\n",
    "You need to:\n",
    "1. Profile the entire dataset efficiently\n",
    "2. Identify data quality issues\n",
    "3. Analyze user behavior patterns\n",
    "4. Generate insights for stakeholders\n",
    "5. Create automated monitoring\n",
    "\n",
    "Dataset: marketing_events_100m\n",
    "Columns: user_id, session_id, timestamp, event_type, channel, device, \n",
    "         country, revenue, page_views, time_on_site\n",
    "\"\"\"\n",
    "\n",
    "class MarketingDatabaseAnalyzer:\n",
    "    def __init__(self, rs_conn, table_name):\n",
    "        self.rs_conn = rs_conn\n",
    "        self.table_name = table_name\n",
    "        self.results = {}\n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\"Execute full analysis pipeline\"\"\"\n",
    "        print(\"Starting 100M row analysis...\\n\")\n",
    "        \n",
    "        # Step 1: Get table overview\n",
    "        print(\"[1/6] Getting table overview...\")\n",
    "        self.results['overview'] = self._get_overview()\n",
    "        print(f\"     Total rows: {self.results['overview']['row_count'].iloc[0]:,}\\n\")\n",
    "        \n",
    "        # Step 2: Profile numeric columns\n",
    "        print(\"[2/6] Profiling numeric columns...\")\n",
    "        self.results['numeric_profiles'] = self._profile_numeric_columns()\n",
    "        print(\"     ✓ Complete\\n\")\n",
    "        \n",
    "        # Step 3: Profile categorical columns\n",
    "        print(\"[3/6] Profiling categorical columns...\")\n",
    "        self.results['categorical_profiles'] = self._profile_categorical_columns()\n",
    "        print(\"     ✓ Complete\\n\")\n",
    "        \n",
    "        # Step 4: Analyze user behavior\n",
    "        print(\"[4/6] Analyzing user behavior...\")\n",
    "        self.results['user_behavior'] = self._analyze_user_behavior()\n",
    "        print(\"     ✓ Complete\\n\")\n",
    "        \n",
    "        # Step 5: Channel performance\n",
    "        print(\"[5/6] Analyzing channel performance...\")\n",
    "        self.results['channel_performance'] = self._analyze_channels()\n",
    "        print(\"     ✓ Complete\\n\")\n",
    "        \n",
    "        # Step 6: Data quality assessment\n",
    "        print(\"[6/6] Assessing data quality...\")\n",
    "        self.results['data_quality'] = self._assess_data_quality()\n",
    "        print(\"     ✓ Complete\\n\")\n",
    "        \n",
    "        print(\"Analysis complete!\")\n",
    "        return self.results\n",
    "    \n",
    "    def _get_overview(self):\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as row_count,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            COUNT(DISTINCT session_id) as unique_sessions,\n",
    "            MIN(timestamp) as min_date,\n",
    "            MAX(timestamp) as max_date,\n",
    "            DATEDIFF(day, MIN(timestamp), MAX(timestamp)) as days_span\n",
    "        FROM {self.table_name}\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def _profile_numeric_columns(self):\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            'revenue' as metric,\n",
    "            COUNT(*) as count,\n",
    "            AVG(revenue) as mean,\n",
    "            STDDEV(revenue) as std,\n",
    "            MIN(revenue) as min,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY revenue) as median,\n",
    "            MAX(revenue) as max,\n",
    "            SUM(revenue) as total\n",
    "        FROM {self.table_name}\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'page_views' as metric,\n",
    "            COUNT(*) as count,\n",
    "            AVG(page_views) as mean,\n",
    "            STDDEV(page_views) as std,\n",
    "            MIN(page_views) as min,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY page_views) as median,\n",
    "            MAX(page_views) as max,\n",
    "            SUM(page_views) as total\n",
    "        FROM {self.table_name}\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            'time_on_site' as metric,\n",
    "            COUNT(*) as count,\n",
    "            AVG(time_on_site) as mean,\n",
    "            STDDEV(time_on_site) as std,\n",
    "            MIN(time_on_site) as min,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY time_on_site) as median,\n",
    "            MAX(time_on_site) as max,\n",
    "            SUM(time_on_site) as total\n",
    "        FROM {self.table_name}\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def _profile_categorical_columns(self):\n",
    "        results = {}\n",
    "        \n",
    "        for col in ['channel', 'device', 'country']:\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                {col},\n",
    "                COUNT(*) as count,\n",
    "                COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage,\n",
    "                AVG(revenue) as avg_revenue,\n",
    "                SUM(revenue) as total_revenue\n",
    "            FROM {self.table_name}\n",
    "            GROUP BY {col}\n",
    "            ORDER BY count DESC\n",
    "            \"\"\"\n",
    "            results[col] = self.rs_conn.execute_query(query)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _analyze_user_behavior(self):\n",
    "        query = f\"\"\"\n",
    "        WITH user_metrics AS (\n",
    "            SELECT \n",
    "                user_id,\n",
    "                COUNT(DISTINCT session_id) as session_count,\n",
    "                SUM(page_views) as total_page_views,\n",
    "                SUM(revenue) as total_revenue,\n",
    "                DATEDIFF(day, MIN(timestamp), MAX(timestamp)) as lifetime_days\n",
    "            FROM {self.table_name}\n",
    "            GROUP BY user_id\n",
    "        )\n",
    "        SELECT \n",
    "            COUNT(*) as total_users,\n",
    "            AVG(session_count) as avg_sessions_per_user,\n",
    "            AVG(total_page_views) as avg_page_views_per_user,\n",
    "            AVG(total_revenue) as avg_revenue_per_user,\n",
    "            AVG(lifetime_days) as avg_lifetime_days,\n",
    "            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_revenue) as median_revenue_per_user,\n",
    "            PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY total_revenue) as p90_revenue_per_user\n",
    "        FROM user_metrics\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def _analyze_channels(self):\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            channel,\n",
    "            COUNT(*) as events,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            SUM(revenue) as total_revenue,\n",
    "            AVG(revenue) as avg_revenue,\n",
    "            SUM(revenue) / COUNT(DISTINCT user_id) as revenue_per_user,\n",
    "            AVG(page_views) as avg_page_views,\n",
    "            AVG(time_on_site) as avg_time_on_site\n",
    "        FROM {self.table_name}\n",
    "        GROUP BY channel\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def _assess_data_quality(self):\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_user_ids,\n",
    "            SUM(CASE WHEN session_id IS NULL THEN 1 ELSE 0 END) as null_session_ids,\n",
    "            SUM(CASE WHEN timestamp IS NULL THEN 1 ELSE 0 END) as null_timestamps,\n",
    "            SUM(CASE WHEN revenue < 0 THEN 1 ELSE 0 END) as negative_revenue,\n",
    "            SUM(CASE WHEN page_views < 0 THEN 1 ELSE 0 END) as negative_page_views,\n",
    "            SUM(CASE WHEN time_on_site < 0 THEN 1 ELSE 0 END) as negative_time,\n",
    "            COUNT(*) as total_rows\n",
    "        FROM {self.table_name}\n",
    "        \"\"\"\n",
    "        return self.rs_conn.execute_query(query)\n",
    "    \n",
    "    def generate_executive_report(self):\n",
    "        \"\"\"Generate executive summary report\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\"*70)\n",
    "        report.append(\"MARKETING DATABASE ANALYSIS - EXECUTIVE SUMMARY\")\n",
    "        report.append(\"=\"*70)\n",
    "        \n",
    "        # Overview\n",
    "        overview = self.results['overview'].iloc[0]\n",
    "        report.append(\"\\n📊 DATASET OVERVIEW\")\n",
    "        report.append(f\"Total Events: {overview['row_count']:,}\")\n",
    "        report.append(f\"Unique Users: {overview['unique_users']:,}\")\n",
    "        report.append(f\"Date Range: {overview['min_date']} to {overview['max_date']}\")\n",
    "        report.append(f\"Time Span: {overview['days_span']} days\")\n",
    "        \n",
    "        # User Behavior\n",
    "        behavior = self.results['user_behavior'].iloc[0]\n",
    "        report.append(\"\\n👥 USER BEHAVIOR\")\n",
    "        report.append(f\"Avg Sessions per User: {behavior['avg_sessions_per_user']:.2f}\")\n",
    "        report.append(f\"Avg Revenue per User: ${behavior['avg_revenue_per_user']:.2f}\")\n",
    "        report.append(f\"Avg Lifetime: {behavior['avg_lifetime_days']:.0f} days\")\n",
    "        \n",
    "        # Channel Performance\n",
    "        channels = self.results['channel_performance']\n",
    "        report.append(\"\\n📱 TOP CHANNELS BY REVENUE\")\n",
    "        for i, row in channels.head(3).iterrows():\n",
    "            report.append(f\"{i+1}. {row['channel']}: ${row['total_revenue']:,.2f} \"\n",
    "                         f\"({row['unique_users']:,} users)\")\n",
    "        \n",
    "        # Data Quality\n",
    "        quality = self.results['data_quality'].iloc[0]\n",
    "        total_rows = quality['total_rows']\n",
    "        issues = sum([quality['null_user_ids'], quality['negative_revenue'], \n",
    "                     quality['negative_page_views']])\n",
    "        quality_score = (1 - issues / total_rows) * 100\n",
    "        report.append(\"\\n✅ DATA QUALITY\")\n",
    "        report.append(f\"Quality Score: {quality_score:.2f}%\")\n",
    "        report.append(f\"Issues Found: {issues:,} ({issues/total_rows*100:.3f}%)\")\n",
    "        \n",
    "        report.append(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Example usage\n",
    "# analyzer = MarketingDatabaseAnalyzer(rs, 'marketing_events_100m')\n",
    "# results = analyzer.run_analysis()\n",
    "# print(analyzer.generate_executive_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises <a name=\"exercises\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Large Dataset Profiling\n",
    "\n",
    "**Task:** Download a large Kaggle dataset (e.g., Criteo CTR data, 10M+ rows) and:\n",
    "1. Profile it using both Redshift SQL and Dask\n",
    "2. Compare performance of both approaches\n",
    "3. Identify data quality issues\n",
    "4. Create visualizations of key insights\n",
    "\n",
    "**Dataset:** https://www.kaggle.com/c/criteo-display-ad-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sampling Strategy Comparison\n",
    "\n",
    "**Task:** Using a large marketing dataset:\n",
    "1. Create samples using 3 different strategies (random, stratified, systematic)\n",
    "2. Compare how well each sample represents the full dataset\n",
    "3. Calculate sampling errors for key metrics\n",
    "4. Recommend best strategy for different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,\n",
    "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Outlier Detection at Scale\n",
    "\n",
    "**Task:** On a large revenue dataset:\n",
    "1. Implement multiple outlier detection methods\n",
    "2. Compare results across methods\n",
    "3. Investigate characteristics of detected outliers\n",
    "4. Recommend treatment strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Automated Monitoring Pipeline\n",
    "\n",
    "**Task:** Build an automated data quality monitoring system:\n",
    "1. Create a pipeline that runs daily on Redshift\n",
    "2. Track key metrics over time (null rates, outliers, distributions)\n",
    "3. Implement alerting for anomalies\n",
    "4. Generate automated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **EDA Strategies for Large Data**\n",
    "   - SQL-first approach with Redshift\n",
    "   - Smart sampling techniques\n",
    "   - Distributed computing with Dask\n",
    "\n",
    "2. **Statistical Analysis at Scale**\n",
    "   - Computing statistics in-database\n",
    "   - Distribution analysis with binning\n",
    "   - Efficient correlation analysis\n",
    "\n",
    "3. **Advanced Techniques**\n",
    "   - Multivariate outlier detection\n",
    "   - Automated profiling pipelines\n",
    "   - Data quality assessment\n",
    "\n",
    "4. **Production Patterns**\n",
    "   - Connection management\n",
    "   - Error handling\n",
    "   - Performance optimization\n",
    "   - Automated reporting\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Week 6: Advanced Visualization & Dashboards\n",
    "- Practice with real large-scale datasets\n",
    "- Build your own profiling tools\n",
    "- Optimize for your specific use cases\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Dask Documentation](https://docs.dask.org/)\n",
    "- [Redshift Best Practices](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)\n",
    "- [Great Expectations](https://greatexpectations.io/)\n",
    "- [Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
