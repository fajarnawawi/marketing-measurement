{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Advanced EDA at Scale\n",
    "\n",
    "## Learning Objectives\n",
    "- Perform EDA on 10M+ row datasets efficiently\n",
    "- Use Redshift SQL for statistical analysis at scale\n",
    "- Implement distributed computing with Dask for EDA\n",
    "- Apply advanced outlier detection techniques at scale\n",
    "- Execute correlation analysis on massive tables\n",
    "- Build automated data profiling pipelines\n",
    "- Design and implement effective sampling strategies\n",
    "- Profile 100M row marketing databases\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install pandas numpy scipy statsmodels dask[complete]\n",
    "pip install psycopg2-binary sqlalchemy redshift_connector\n",
    "pip install seaborn matplotlib plotly\n",
    "pip install pandas-profiling great-expectations\n",
    "pip install pyarrow fastparquet scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import gc\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, chi2, kstest, anderson\n",
    "from statsmodels import robust\n",
    "from statsmodels.stats import weightstats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Database\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Dask: {dask.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Connection for Large-Scale EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftEDA:\n",
    "    \"\"\"Redshift connection manager optimized for EDA queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, str]):\n",
    "        self.config = config\n",
    "        self.engine = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self._create_engine()\n",
    "    \n",
    "    def _create_engine(self):\n",
    "        \"\"\"Create SQLAlchemy engine with optimized settings.\"\"\"\n",
    "        conn_str = (\n",
    "            f\"postgresql+psycopg2://{self.config['user']}:{self.config['password']}\"\n",
    "            f\"@{self.config['host']}:{self.config.get('port', 5439)}/{self.config['database']}\"\n",
    "        )\n",
    "        \n",
    "        self.engine = create_engine(\n",
    "            conn_str,\n",
    "            pool_size=5,\n",
    "            max_overflow=10,\n",
    "            pool_pre_ping=True,\n",
    "            pool_recycle=3600\n",
    "        )\n",
    "        self.logger.info(\"Redshift engine created\")\n",
    "    \n",
    "    def get_table_profile(self, table: str, sample_size: int = 100000) -> Dict:\n",
    "        \"\"\"Get comprehensive profile of a table.\"\"\"\n",
    "        self.logger.info(f\"Profiling table: {table}\")\n",
    "        \n",
    "        profile = {}\n",
    "        \n",
    "        # Basic stats\n",
    "        basic_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as row_count,\n",
    "            COUNT(DISTINCT *) as unique_rows\n",
    "        FROM {table}\n",
    "        \"\"\"\n",
    "        profile['basic'] = pd.read_sql(basic_query, self.engine).to_dict('records')[0]\n",
    "        \n",
    "        # Column information\n",
    "        columns_query = f\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type,\n",
    "            is_nullable\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_name = '{table.split('.')[-1]}'\n",
    "        ORDER BY ordinal_position\n",
    "        \"\"\"\n",
    "        profile['columns'] = pd.read_sql(columns_query, self.engine)\n",
    "        \n",
    "        # Sample data\n",
    "        sample_query = f\"SELECT * FROM {table} LIMIT {sample_size}\"\n",
    "        profile['sample'] = pd.read_sql(sample_query, self.engine)\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    def compute_statistics(self, table: str, column: str) -> pd.DataFrame:\n",
    "        \"\"\"Compute comprehensive statistics for a column.\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as count,\n",
    "            COUNT(DISTINCT {column}) as unique_count,\n",
    "            COUNT(*) - COUNT({column}) as null_count,\n",
    "            MIN({column}) as min_value,\n",
    "            MAX({column}) as max_value,\n",
    "            AVG({column}::FLOAT) as mean,\n",
    "            STDDEV({column}::FLOAT) as stddev,\n",
    "            PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {column}) as q25,\n",
    "            PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY {column}) as median,\n",
    "            PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {column}) as q75,\n",
    "            PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY {column}) as p90,\n",
    "            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY {column}) as p95,\n",
    "            PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY {column}) as p99\n",
    "        FROM {table}\n",
    "        WHERE {column} IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def detect_outliers_sql(self, table: str, column: str, method: str = 'iqr') -> pd.DataFrame:\n",
    "        \"\"\"Detect outliers using SQL.\"\"\"\n",
    "        if method == 'iqr':\n",
    "            query = f\"\"\"\n",
    "            WITH stats AS (\n",
    "                SELECT \n",
    "                    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY {column}) as q1,\n",
    "                    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY {column}) as q3\n",
    "                FROM {table}\n",
    "            ),\n",
    "            bounds AS (\n",
    "                SELECT \n",
    "                    q1 - 1.5 * (q3 - q1) as lower_bound,\n",
    "                    q3 + 1.5 * (q3 - q1) as upper_bound\n",
    "                FROM stats\n",
    "            )\n",
    "            SELECT \n",
    "                *,\n",
    "                CASE \n",
    "                    WHEN {column} < (SELECT lower_bound FROM bounds) THEN 'low_outlier'\n",
    "                    WHEN {column} > (SELECT upper_bound FROM bounds) THEN 'high_outlier'\n",
    "                    ELSE 'normal'\n",
    "                END as outlier_flag\n",
    "            FROM {table}\n",
    "            WHERE {column} IS NOT NULL\n",
    "            \"\"\"\n",
    "        elif method == 'zscore':\n",
    "            query = f\"\"\"\n",
    "            WITH stats AS (\n",
    "                SELECT \n",
    "                    AVG({column}::FLOAT) as mean,\n",
    "                    STDDEV({column}::FLOAT) as stddev\n",
    "                FROM {table}\n",
    "            )\n",
    "            SELECT \n",
    "                *,\n",
    "                ABS(({column} - (SELECT mean FROM stats)) / (SELECT stddev FROM stats)) as z_score,\n",
    "                CASE \n",
    "                    WHEN ABS(({column} - (SELECT mean FROM stats)) / (SELECT stddev FROM stats)) > 3 THEN 'outlier'\n",
    "                    ELSE 'normal'\n",
    "                END as outlier_flag\n",
    "            FROM {table}\n",
    "            WHERE {column} IS NOT NULL\n",
    "            \"\"\"\n",
    "        \n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def correlation_matrix(self, table: str, columns: List[str], sample_size: int = 1000000) -> pd.DataFrame:\n",
    "        \"\"\"Compute correlation matrix for large table.\"\"\"\n",
    "        cols_str = ', '.join(columns)\n",
    "        query = f\"SELECT {cols_str} FROM {table} WHERE RANDOM() < {sample_size} / (SELECT COUNT(*) FROM {table})\"\n",
    "        \n",
    "        df = pd.read_sql(query, self.engine)\n",
    "        return df.corr()\n",
    "\n",
    "\n",
    "# Configuration\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.region.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', 5439)),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing'),\n",
    "    'user': os.getenv('REDSHIFT_USER', 'analyst'),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', 'password')\n",
    "}\n",
    "\n",
    "# rs_eda = RedshiftEDA(REDSHIFT_CONFIG)\n",
    "print(\"Redshift EDA manager configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA on 10M+ Row Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeScaleEDA:\n",
    "    \"\"\"Perform EDA on massive datasets efficiently.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 100000):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def chunked_statistics(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Calculate statistics using chunked processing.\"\"\"\n",
    "        self.logger.info(f\"Computing statistics for {file_path}\")\n",
    "        \n",
    "        stats_list = []\n",
    "        total_rows = 0\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(pd.read_csv(file_path, chunksize=self.chunk_size), 1):\n",
    "            chunk_stats = {\n",
    "                'chunk': chunk_num,\n",
    "                'rows': len(chunk),\n",
    "                'numeric_cols': chunk.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "                'mean': chunk.select_dtypes(include=[np.number]).mean().to_dict(),\n",
    "                'std': chunk.select_dtypes(include=[np.number]).std().to_dict(),\n",
    "                'min': chunk.select_dtypes(include=[np.number]).min().to_dict(),\n",
    "                'max': chunk.select_dtypes(include=[np.number]).max().to_dict()\n",
    "            }\n",
    "            stats_list.append(chunk_stats)\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            if chunk_num % 10 == 0:\n",
    "                self.logger.info(f\"Processed {total_rows:,} rows\")\n",
    "                gc.collect()\n",
    "        \n",
    "        return pd.DataFrame(stats_list)\n",
    "    \n",
    "    def incremental_histogram(self, file_path: str, column: str, bins: int = 50) -> Tuple:\n",
    "        \"\"\"Build histogram incrementally from large file.\"\"\"\n",
    "        self.logger.info(f\"Building histogram for {column}\")\n",
    "        \n",
    "        # First pass: find min/max\n",
    "        min_val = float('inf')\n",
    "        max_val = float('-inf')\n",
    "        \n",
    "        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):\n",
    "            if column in chunk.columns:\n",
    "                min_val = min(min_val, chunk[column].min())\n",
    "                max_val = max(max_val, chunk[column].max())\n",
    "        \n",
    "        # Create bins\n",
    "        bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        hist = np.zeros(bins)\n",
    "        \n",
    "        # Second pass: build histogram\n",
    "        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):\n",
    "            if column in chunk.columns:\n",
    "                chunk_hist, _ = np.histogram(chunk[column].dropna(), bins=bin_edges)\n",
    "                hist += chunk_hist\n",
    "        \n",
    "        return hist, bin_edges\n",
    "    \n",
    "    def memory_efficient_describe(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Generate describe() output for large files.\"\"\"\n",
    "        self.logger.info(\"Generating descriptive statistics\")\n",
    "        \n",
    "        # Online algorithm for statistics\n",
    "        n = 0\n",
    "        mean = None\n",
    "        m2 = None\n",
    "        min_vals = None\n",
    "        max_vals = None\n",
    "        \n",
    "        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):\n",
    "            numeric_chunk = chunk.select_dtypes(include=[np.number])\n",
    "            \n",
    "            if mean is None:\n",
    "                mean = numeric_chunk.mean()\n",
    "                m2 = ((numeric_chunk - mean) ** 2).sum()\n",
    "                min_vals = numeric_chunk.min()\n",
    "                max_vals = numeric_chunk.max()\n",
    "            else:\n",
    "                # Welford online algorithm\n",
    "                delta = numeric_chunk - mean\n",
    "                mean += delta.sum() / (n + len(chunk))\n",
    "                m2 += ((numeric_chunk - mean) ** 2).sum()\n",
    "                min_vals = pd.concat([min_vals, numeric_chunk.min()], axis=1).min(axis=1)\n",
    "                max_vals = pd.concat([max_vals, numeric_chunk.max()], axis=1).max(axis=1)\n",
    "            \n",
    "            n += len(chunk)\n",
    "        \n",
    "        variance = m2 / n\n",
    "        std = np.sqrt(variance)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'count': n,\n",
    "            'mean': mean,\n",
    "            'std': std,\n",
    "            'min': min_vals,\n",
    "            'max': max_vals\n",
    "        })\n",
    "\n",
    "\n",
    "# Example usage\n",
    "eda = LargeScaleEDA(chunk_size=100000)\n",
    "print(\"Large-scale EDA tools ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distributed Computing with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask client\n",
    "# client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "# print(client)\n",
    "\n",
    "def dask_eda_example(file_pattern: str) -> Dict:\n",
    "    \"\"\"Perform EDA using Dask for distributed computing.\"\"\"\n",
    "    \n",
    "    # Read large CSV with Dask\n",
    "    ddf = dd.read_csv(\n",
    "        file_pattern,\n",
    "        blocksize='64MB',\n",
    "        dtype={'user_id': 'int32', 'campaign_id': 'int16'},\n",
    "        parse_dates=['timestamp']\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Loaded {ddf.npartitions} partitions\")\n",
    "    \n",
    "    # Describe (lazy computation)\n",
    "    description = ddf.describe()\n",
    "    \n",
    "    # Value counts\n",
    "    value_counts = ddf['campaign_id'].value_counts()\n",
    "    \n",
    "    # Groupby aggregation\n",
    "    grouped = ddf.groupby('campaign_id').agg({\n",
    "        'revenue': ['sum', 'mean', 'std', 'count'],\n",
    "        'user_id': 'nunique'\n",
    "    })\n",
    "    \n",
    "    # Compute all (triggers execution)\n",
    "    results = {\n",
    "        'description': description.compute(),\n",
    "        'value_counts': value_counts.compute(),\n",
    "        'grouped': grouped.compute()\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def dask_correlation_analysis(ddf: dd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Compute correlation matrix using Dask.\"\"\"\n",
    "    \n",
    "    logger.info(\"Computing correlation matrix with Dask\")\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_ddf = ddf[columns]\n",
    "    \n",
    "    # Compute correlation (Dask will parallelize this)\n",
    "    corr_matrix = numeric_ddf.corr().compute()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "def dask_time_series_analysis(ddf: dd.DataFrame, date_col: str, value_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Perform time series aggregation with Dask.\"\"\"\n",
    "    \n",
    "    # Set index\n",
    "    ddf = ddf.set_index(date_col)\n",
    "    \n",
    "    # Resample by day\n",
    "    daily = ddf[value_col].resample('D').agg(['sum', 'mean', 'count'])\n",
    "    \n",
    "    # Compute\n",
    "    return daily.compute()\n",
    "\n",
    "\n",
    "print(\"Dask EDA functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Outlier Detection at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetector:\n",
    "    \"\"\"Advanced outlier detection for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, method: str = 'iqr'):\n",
    "        self.method = method\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def detect_univariate(self, df: pd.DataFrame, column: str) -> pd.Series:\n",
    "        \"\"\"Detect univariate outliers.\"\"\"\n",
    "        \n",
    "        if self.method == 'iqr':\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            return (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "        \n",
    "        elif self.method == 'zscore':\n",
    "            z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "            return z_scores > 3\n",
    "        \n",
    "        elif self.method == 'modified_zscore':\n",
    "            median = df[column].median()\n",
    "            mad = robust.mad(df[column].dropna())\n",
    "            modified_z = 0.6745 * (df[column] - median) / mad\n",
    "            return np.abs(modified_z) > 3.5\n",
    "    \n",
    "    def detect_multivariate(self, df: pd.DataFrame, columns: List[str], \n",
    "                          contamination: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"Detect multivariate outliers using Isolation Forest.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Detecting multivariate outliers with Isolation Forest\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X = df[columns].dropna()\n",
    "        \n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        outliers = iso_forest.fit_predict(X)\n",
    "        \n",
    "        # -1 for outliers, 1 for inliers\n",
    "        return outliers == -1\n",
    "    \n",
    "    def detect_mahalanobis(self, df: pd.DataFrame, columns: List[str]) -> np.ndarray:\n",
    "        \"\"\"Detect outliers using Mahalanobis distance.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Detecting outliers with Mahalanobis distance\")\n",
    "        \n",
    "        X = df[columns].dropna()\n",
    "        \n",
    "        # Robust covariance estimation\n",
    "        robust_cov = EllipticEnvelope(contamination=0.1, random_state=42)\n",
    "        outliers = robust_cov.fit_predict(X)\n",
    "        \n",
    "        return outliers == -1\n",
    "    \n",
    "    def detect_chunked(self, file_path: str, column: str, chunk_size: int = 100000) -> List:\n",
    "        \"\"\"Detect outliers in chunks for very large files.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Detecting outliers in {column} using chunked processing\")\n",
    "        \n",
    "        # First pass: calculate statistics\n",
    "        values = []\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            values.extend(chunk[column].dropna().tolist())\n",
    "        \n",
    "        # Calculate bounds\n",
    "        Q1 = np.percentile(values, 25)\n",
    "        Q3 = np.percentile(values, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Second pass: flag outliers\n",
    "        outlier_indices = []\n",
    "        offset = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            chunk_outliers = (chunk[column] < lower_bound) | (chunk[column] > upper_bound)\n",
    "            outlier_indices.extend((chunk_outliers[chunk_outliers].index + offset).tolist())\n",
    "            offset += len(chunk)\n",
    "        \n",
    "        return outlier_indices\n",
    "\n",
    "\n",
    "# Example: Detect outliers\n",
    "detector = OutlierDetector(method='iqr')\n",
    "print(\"Outlier detector initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalableCorrelation:\n",
    "    \"\"\"Compute correlations for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def incremental_correlation(self, file_path: str, columns: List[str], \n",
    "                               chunk_size: int = 100000) -> pd.DataFrame:\n",
    "        \"\"\"Calculate correlation incrementally.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Computing incremental correlation\")\n",
    "        \n",
    "        n = 0\n",
    "        means = None\n",
    "        cov_matrix = None\n",
    "        \n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            chunk_data = chunk[columns].dropna()\n",
    "            \n",
    "            if means is None:\n",
    "                means = chunk_data.mean()\n",
    "                cov_matrix = np.zeros((len(columns), len(columns)))\n",
    "            \n",
    "            # Update means and covariance\n",
    "            for i, col1 in enumerate(columns):\n",
    "                for j, col2 in enumerate(columns):\n",
    "                    if i <= j:\n",
    "                        cov = ((chunk_data[col1] - means[col1]) * \n",
    "                               (chunk_data[col2] - means[col2])).sum()\n",
    "                        cov_matrix[i, j] += cov\n",
    "                        cov_matrix[j, i] = cov_matrix[i, j]\n",
    "            \n",
    "            n += len(chunk_data)\n",
    "        \n",
    "        # Convert covariance to correlation\n",
    "        cov_matrix /= n\n",
    "        std = np.sqrt(np.diag(cov_matrix))\n",
    "        corr_matrix = cov_matrix / np.outer(std, std)\n",
    "        \n",
    "        return pd.DataFrame(corr_matrix, index=columns, columns=columns)\n",
    "    \n",
    "    def sparse_correlation(self, df: pd.DataFrame, threshold: float = 0.3) -> pd.DataFrame:\n",
    "        \"\"\"Find only significant correlations above threshold.\"\"\"\n",
    "        \n",
    "        corr = df.corr()\n",
    "        \n",
    "        # Create mask for significant correlations\n",
    "        mask = (np.abs(corr) >= threshold) & (corr != 1.0)\n",
    "        \n",
    "        # Extract significant pairs\n",
    "        significant_pairs = []\n",
    "        for i in range(len(corr.columns)):\n",
    "            for j in range(i+1, len(corr.columns)):\n",
    "                if mask.iloc[i, j]:\n",
    "                    significant_pairs.append({\n",
    "                        'var1': corr.columns[i],\n",
    "                        'var2': corr.columns[j],\n",
    "                        'correlation': corr.iloc[i, j]\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(significant_pairs).sort_values('correlation', \n",
    "                                                           key=abs, \n",
    "                                                           ascending=False)\n",
    "    \n",
    "    def visualize_correlation_heatmap(self, corr_matrix: pd.DataFrame, \n",
    "                                     title: str = 'Correlation Matrix'):\n",
    "        \"\"\"Create interactive correlation heatmap.\"\"\"\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.columns,\n",
    "            colorscale='RdBu',\n",
    "            zmid=0,\n",
    "            text=corr_matrix.values,\n",
    "            texttemplate='%{text:.2f}',\n",
    "            textfont={\"size\": 10}\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            width=800,\n",
    "            height=800\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "corr_analyzer = ScalableCorrelation()\n",
    "print(\"Correlation analyzer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Data Profiling Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatedProfiler:\n",
    "    \"\"\"Automated data profiling for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.profile_results = {}\n",
    "    \n",
    "    def profile_table(self, df: pd.DataFrame, sample_size: int = 100000) -> Dict:\n",
    "        \"\"\"Generate comprehensive profile of a table.\"\"\"\n",
    "        \n",
    "        self.logger.info(\"Profiling dataset\")\n",
    "        \n",
    "        # Sample if too large\n",
    "        if len(df) > sample_size:\n",
    "            df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            df_sample = df\n",
    "        \n",
    "        profile = {\n",
    "            'shape': df.shape,\n",
    "            'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            'dtypes': df.dtypes.value_counts().to_dict(),\n",
    "            'missing': df.isnull().sum().to_dict(),\n",
    "            'missing_pct': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "            'duplicates': df.duplicated().sum(),\n",
    "            'columns': {}\n",
    "        }\n",
    "        \n",
    "        # Profile each column\n",
    "        for col in df.columns:\n",
    "            profile['columns'][col] = self._profile_column(df_sample, col)\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    def _profile_column(self, df: pd.DataFrame, column: str) -> Dict:\n",
    "        \"\"\"Profile individual column.\"\"\"\n",
    "        \n",
    "        col_profile = {\n",
    "            'dtype': str(df[column].dtype),\n",
    "            'missing': df[column].isnull().sum(),\n",
    "            'missing_pct': df[column].isnull().sum() / len(df) * 100,\n",
    "            'unique': df[column].nunique(),\n",
    "            'unique_pct': df[column].nunique() / len(df) * 100\n",
    "        }\n",
    "        \n",
    "        # Numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            col_profile.update({\n",
    "                'mean': df[column].mean(),\n",
    "                'std': df[column].std(),\n",
    "                'min': df[column].min(),\n",
    "                'q25': df[column].quantile(0.25),\n",
    "                'median': df[column].median(),\n",
    "                'q75': df[column].quantile(0.75),\n",
    "                'max': df[column].max(),\n",
    "                'skew': df[column].skew(),\n",
    "                'kurtosis': df[column].kurtosis(),\n",
    "                'zeros': (df[column] == 0).sum(),\n",
    "                'negative': (df[column] < 0).sum()\n",
    "            })\n",
    "        \n",
    "        # Categorical columns\n",
    "        elif pd.api.types.is_object_dtype(df[column]):\n",
    "            value_counts = df[column].value_counts()\n",
    "            col_profile.update({\n",
    "                'top_values': value_counts.head(10).to_dict(),\n",
    "                'cardinality': len(value_counts),\n",
    "                'mode': df[column].mode()[0] if len(df[column].mode()) > 0 else None\n",
    "            })\n",
    "        \n",
    "        return col_profile\n",
    "    \n",
    "    def data_quality_report(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate data quality report.\"\"\"\n",
    "        \n",
    "        quality_metrics = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            metrics = {\n",
    "                'column': col,\n",
    "                'dtype': str(df[col].dtype),\n",
    "                'completeness': (1 - df[col].isnull().sum() / len(df)) * 100,\n",
    "                'uniqueness': df[col].nunique() / len(df) * 100,\n",
    "                'validity': self._check_validity(df, col)\n",
    "            }\n",
    "            quality_metrics.append(metrics)\n",
    "        \n",
    "        return pd.DataFrame(quality_metrics)\n",
    "    \n",
    "    def _check_validity(self, df: pd.DataFrame, column: str) -> float:\n",
    "        \"\"\"Check data validity.\"\"\"\n",
    "        \n",
    "        total = len(df)\n",
    "        valid = total - df[column].isnull().sum()\n",
    "        \n",
    "        # Additional checks for numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Check for inf values\n",
    "            valid -= np.isinf(df[column]).sum()\n",
    "        \n",
    "        return (valid / total) * 100\n",
    "    \n",
    "    def generate_html_report(self, profile: Dict, output_file: str):\n",
    "        \"\"\"Generate HTML report from profile.\"\"\"\n",
    "        \n",
    "        html = f\"\"\"\n",
    "        <html>\n",
    "        <head><title>Data Profile Report</title></head>\n",
    "        <body>\n",
    "        <h1>Data Profile Report</h1>\n",
    "        <h2>Overview</h2>\n",
    "        <p>Shape: {profile['shape']}</p>\n",
    "        <p>Memory: {profile['memory_mb']:.2f} MB</p>\n",
    "        <p>Duplicates: {profile['duplicates']}</p>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(html)\n",
    "        \n",
    "        self.logger.info(f\"Report saved to {output_file}\")\n",
    "\n",
    "\n",
    "profiler = AutomatedProfiler()\n",
    "print(\"Automated profiler ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sampling Strategies for Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingStrategies:\n",
    "    \"\"\"Advanced sampling techniques for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.random_state = random_state\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "    \n",
    "    def simple_random_sample(self, df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "        \"\"\"Simple random sampling.\"\"\"\n",
    "        return df.sample(n=n, random_state=self.random_state)\n",
    "    \n",
    "    def stratified_sample(self, df: pd.DataFrame, strata_col: str, \n",
    "                         n_per_stratum: int) -> pd.DataFrame:\n",
    "        \"\"\"Stratified random sampling.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Stratified sampling on {strata_col}\")\n",
    "        \n",
    "        samples = []\n",
    "        for stratum in df[strata_col].unique():\n",
    "            stratum_df = df[df[strata_col] == stratum]\n",
    "            sample_size = min(n_per_stratum, len(stratum_df))\n",
    "            samples.append(stratum_df.sample(n=sample_size, random_state=self.random_state))\n",
    "        \n",
    "        return pd.concat(samples, ignore_index=True)\n",
    "    \n",
    "    def systematic_sample(self, df: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "        \"\"\"Systematic sampling (every kth row).\"\"\"\n",
    "        return df.iloc[::k]\n",
    "    \n",
    "    def reservoir_sample(self, file_path: str, n: int, \n",
    "                        chunk_size: int = 100000) -> pd.DataFrame:\n",
    "        \"\"\"Reservoir sampling for streaming data.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Reservoir sampling {n} rows\")\n",
    "        \n",
    "        reservoir = []\n",
    "        count = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            for _, row in chunk.iterrows():\n",
    "                count += 1\n",
    "                \n",
    "                if len(reservoir) < n:\n",
    "                    reservoir.append(row)\n",
    "                else:\n",
    "                    # Random replacement\n",
    "                    j = np.random.randint(0, count)\n",
    "                    if j < n:\n",
    "                        reservoir[j] = row\n",
    "        \n",
    "        return pd.DataFrame(reservoir)\n",
    "    \n",
    "    def time_based_sample(self, df: pd.DataFrame, date_col: str, \n",
    "                         freq: str = 'W') -> pd.DataFrame:\n",
    "        \"\"\"Time-based sampling.\"\"\"\n",
    "        \n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        return df.set_index(date_col).resample(freq).first().reset_index()\n",
    "    \n",
    "    def evaluate_sample_quality(self, original: pd.DataFrame, \n",
    "                               sample: pd.DataFrame,\n",
    "                               numeric_cols: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate how well sample represents population.\"\"\"\n",
    "        \n",
    "        comparison = []\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            comparison.append({\n",
    "                'column': col,\n",
    "                'pop_mean': original[col].mean(),\n",
    "                'sample_mean': sample[col].mean(),\n",
    "                'mean_diff': abs(original[col].mean() - sample[col].mean()),\n",
    "                'pop_std': original[col].std(),\n",
    "                'sample_std': sample[col].std(),\n",
    "                'std_diff': abs(original[col].std() - sample[col].std())\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "sampler = SamplingStrategies(random_state=42)\n",
    "print(\"Sampling strategies ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Real-World Project: Profile 100M Row Marketing Database\n",
    "\n",
    "### Comprehensive Marketing Database Profiling System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingDatabaseProfiler:\n",
    "    \"\"\"Profile massive marketing database with 100M+ rows.\"\"\"\n",
    "    \n",
    "    def __init__(self, rs_config: Dict):\n",
    "        self.rs = RedshiftEDA(rs_config)\n",
    "        self.profiler = AutomatedProfiler()\n",
    "        self.sampler = SamplingStrategies()\n",
    "        self.outlier_detector = OutlierDetector()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.results = {}\n",
    "    \n",
    "    def execute_full_profile(self, table: str, date_col: str = 'event_date',\n",
    "                           start_date: str = '2024-01-01',\n",
    "                           end_date: str = '2024-12-31') -> Dict:\n",
    "        \"\"\"Execute comprehensive profiling pipeline.\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Starting comprehensive profile of {table}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Get table overview\n",
    "        self.logger.info(\"Step 1: Table overview\")\n",
    "        overview_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            COUNT(DISTINCT campaign_id) as unique_campaigns,\n",
    "            MIN({date_col}) as min_date,\n",
    "            MAX({date_col}) as max_date,\n",
    "            SUM(revenue) as total_revenue\n",
    "        FROM {table}\n",
    "        WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'\n",
    "        \"\"\"\n",
    "        self.results['overview'] = pd.read_sql(overview_query, self.rs.engine).to_dict('records')[0]\n",
    "        \n",
    "        # 2. Sample data for detailed analysis\n",
    "        self.logger.info(\"Step 2: Sampling data\")\n",
    "        sample_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table}\n",
    "        WHERE {date_col} BETWEEN '{start_date}' AND '{end_date}'\n",
    "          AND RANDOM() < 1000000.0 / (SELECT COUNT(*) FROM {table})\n",
    "        LIMIT 1000000\n",
    "        \"\"\"\n",
    "        sample_df = pd.read_sql(sample_query, self.rs.engine)\n",
    "        self.results['sample_size'] = len(sample_df)\n",
    "        \n",
    "        # 3. Profile sample\n",
    "        self.logger.info(\"Step 3: Profiling sample\")\n",
    "        self.results['profile'] = self.profiler.profile_table(sample_df)\n",
    "        \n",
    "        # 4. Statistical analysis\n",
    "        self.logger.info(\"Step 4: Statistical analysis\")\n",
    "        numeric_cols = sample_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.results['statistics'] = sample_df[numeric_cols].describe().to_dict()\n",
    "        \n",
    "        # 5. Outlier detection\n",
    "        self.logger.info(\"Step 5: Outlier detection\")\n",
    "        self.results['outliers'] = {}\n",
    "        for col in ['revenue', 'clicks', 'impressions']:\n",
    "            if col in sample_df.columns:\n",
    "                outliers = self.outlier_detector.detect_univariate(sample_df, col)\n",
    "                self.results['outliers'][col] = outliers.sum()\n",
    "        \n",
    "        # 6. Correlation analysis\n",
    "        self.logger.info(\"Step 6: Correlation analysis\")\n",
    "        if len(numeric_cols) > 1:\n",
    "            self.results['correlations'] = sample_df[numeric_cols].corr().to_dict()\n",
    "        \n",
    "        # 7. Data quality metrics\n",
    "        self.logger.info(\"Step 7: Data quality assessment\")\n",
    "        self.results['quality'] = self.profiler.data_quality_report(sample_df).to_dict('records')\n",
    "        \n",
    "        # 8. Time series analysis\n",
    "        self.logger.info(\"Step 8: Time series trends\")\n",
    "        if date_col in sample_df.columns:\n",
    "            sample_df[date_col] = pd.to_datetime(sample_df[date_col])\n",
    "            daily_stats = sample_df.groupby(sample_df[date_col].dt.date).agg({\n",
    "                'revenue': ['sum', 'mean', 'count']\n",
    "            })\n",
    "            self.results['daily_trends'] = daily_stats.to_dict()\n",
    "        \n",
    "        # 9. Channel analysis\n",
    "        self.logger.info(\"Step 9: Channel performance\")\n",
    "        if 'channel' in sample_df.columns:\n",
    "            channel_stats = sample_df.groupby('channel').agg({\n",
    "                'revenue': ['sum', 'mean', 'count'],\n",
    "                'user_id': 'nunique'\n",
    "            })\n",
    "            self.results['channel_performance'] = channel_stats.to_dict()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        self.results['execution_time'] = elapsed\n",
    "        \n",
    "        self.logger.info(f\"Profiling complete in {elapsed:.2f}s\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_executive_summary(self) -> str:\n",
    "        \"\"\"Generate executive summary of findings.\"\"\"\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "        MARKETING DATABASE PROFILE SUMMARY\n",
    "        ==================================\n",
    "        \n",
    "        Total Rows: {self.results['overview']['total_rows']:,}\n",
    "        Unique Users: {self.results['overview']['unique_users']:,}\n",
    "        Unique Campaigns: {self.results['overview']['unique_campaigns']:,}\n",
    "        Date Range: {self.results['overview']['min_date']} to {self.results['overview']['max_date']}\n",
    "        Total Revenue: ${self.results['overview']['total_revenue']:,.2f}\n",
    "        \n",
    "        Sample Size: {self.results['sample_size']:,} rows\n",
    "        \n",
    "        Data Quality:\n",
    "        - Completeness: {np.mean([q['completeness'] for q in self.results['quality']]):.1f}%\n",
    "        - Outliers Detected: {sum(self.results['outliers'].values()):,}\n",
    "        \n",
    "        Execution Time: {self.results['execution_time']:.2f}s\n",
    "        \"\"\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# profiler = MarketingDatabaseProfiler(REDSHIFT_CONFIG)\n",
    "# results = profiler.execute_full_profile('marketing_events')\n",
    "# print(profiler.generate_executive_summary())\n",
    "\n",
    "print(\"Marketing database profiler ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Large-Scale Statistical Analysis\n",
    "Using a 10M+ row dataset:\n",
    "1. Calculate comprehensive statistics without loading entire dataset into memory\n",
    "2. Identify distributions of key metrics\n",
    "3. Detect outliers using multiple methods\n",
    "4. Generate statistical report\n",
    "\n",
    "### Exercise 2: Distributed EDA with Dask\n",
    "1. Load a large dataset using Dask\n",
    "2. Perform groupby aggregations\n",
    "3. Compute correlation matrix\n",
    "4. Compare performance with pandas\n",
    "\n",
    "### Exercise 3: Automated Profiling Pipeline\n",
    "Build an automated profiling system that:\n",
    "1. Profiles all tables in a Redshift schema\n",
    "2. Generates quality metrics\n",
    "3. Identifies data issues\n",
    "4. Creates HTML reports\n",
    "5. Sends alerts for quality issues\n",
    "\n",
    "### Exercise 4: Sampling Strategy Comparison\n",
    "1. Implement 3+ sampling strategies\n",
    "2. Compare sample quality metrics\n",
    "3. Measure bias in each method\n",
    "4. Recommend best strategy for different scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "### Documentation\n",
    "- [Dask Documentation](https://docs.dask.org/)\n",
    "- [Pandas Profiling](https://pandas-profiling.github.io/)\n",
    "- [SciPy Statistics](https://docs.scipy.org/doc/scipy/reference/stats.html)\n",
    "- [Scikit-learn Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "\n",
    "### Papers\n",
    "- Scalable Data Profiling - IEEE\n",
    "- Outlier Detection at Scale - KDD\n",
    "- Sampling Techniques for Big Data - ACM\n",
    "\n",
    "### Tools\n",
    "- Great Expectations: Data quality framework\n",
    "- Pandas Profiling: Automated EDA\n",
    "- Dask: Distributed computing\n",
    "- PyOD: Outlier detection library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
