{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Advanced Marketing Mix Modeling (Production)\n",
    "\n",
    "## Enterprise-Scale Bayesian MMM with Hierarchical Models\n",
    "\n",
    "### Learning Objectives\n",
    "- Build production-scale Marketing Mix Models\n",
    "- Implement Bayesian MMM with PyMC3\n",
    "- Create hierarchical models for geo/segment analysis\n",
    "- Automate model selection and validation\n",
    "- Optimize marketing budgets with constraints\n",
    "- Deploy scalable forecasting systems\n",
    "\n",
    "### Prerequisites\n",
    "- AWS Redshift cluster access\n",
    "- Python 3.8+\n",
    "- Understanding of regression and time series\n",
    "- Familiarity with Bayesian statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pymc3 arviz theano-pymc scikit-learn pandas numpy matplotlib seaborn \\\n",
    "    plotly psycopg2-binary sqlalchemy scipy statsmodels prophet \\\n",
    "    optuna shap xgboost lightgbm joblib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import theano.tensor as tt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import optuna\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "az.style.use('arviz-darkgrid')\n",
    "\n",
    "print(\"✓ All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Redshift Data Architecture for MMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftMMMData:\n",
    "    \"\"\"\n",
    "    Data loader for Marketing Mix Modeling from Redshift.\n",
    "    Handles marketing spend, conversions, and external factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host, port, database, user, password):\n",
    "        conn_string = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\"\n",
    "        self.engine = create_engine(conn_string, pool_size=10)\n",
    "        print(f\"✓ Connected to Redshift: {database}\")\n",
    "    \n",
    "    def create_mmm_tables(self):\n",
    "        \"\"\"\n",
    "        Create optimized tables for MMM data.\n",
    "        \"\"\"\n",
    "        sql = \"\"\"\n",
    "        -- Daily marketing spend by channel\n",
    "        CREATE TABLE IF NOT EXISTS marketing_spend (\n",
    "            date DATE NOT NULL,\n",
    "            channel VARCHAR(50) NOT NULL,\n",
    "            campaign VARCHAR(100),\n",
    "            geo VARCHAR(50),\n",
    "            segment VARCHAR(50),\n",
    "            spend DECIMAL(12,2) NOT NULL,\n",
    "            impressions BIGINT,\n",
    "            clicks INTEGER,\n",
    "            PRIMARY KEY (date, channel, geo, segment)\n",
    "        )\n",
    "        DISTKEY(date)\n",
    "        SORTKEY(date, channel);\n",
    "        \n",
    "        -- Daily conversions and revenue\n",
    "        CREATE TABLE IF NOT EXISTS daily_conversions (\n",
    "            date DATE NOT NULL,\n",
    "            geo VARCHAR(50),\n",
    "            segment VARCHAR(50),\n",
    "            conversions INTEGER NOT NULL,\n",
    "            revenue DECIMAL(12,2) NOT NULL,\n",
    "            orders INTEGER,\n",
    "            PRIMARY KEY (date, geo, segment)\n",
    "        )\n",
    "        DISTKEY(date)\n",
    "        SORTKEY(date);\n",
    "        \n",
    "        -- External factors (seasonality, events, macroeconomic)\n",
    "        CREATE TABLE IF NOT EXISTS external_factors (\n",
    "            date DATE NOT NULL PRIMARY KEY,\n",
    "            day_of_week INTEGER,\n",
    "            is_weekend BOOLEAN,\n",
    "            is_holiday BOOLEAN,\n",
    "            promotion_active BOOLEAN,\n",
    "            weather_index DECIMAL(5,2),\n",
    "            competitor_spend DECIMAL(12,2),\n",
    "            market_index DECIMAL(8,2),\n",
    "            consumer_sentiment DECIMAL(5,2)\n",
    "        )\n",
    "        DISTKEY(date)\n",
    "        SORTKEY(date);\n",
    "        \n",
    "        -- Aggregated MMM dataset (pre-computed for faster modeling)\n",
    "        CREATE TABLE IF NOT EXISTS mmm_dataset (\n",
    "            date DATE NOT NULL,\n",
    "            geo VARCHAR(50),\n",
    "            segment VARCHAR(50),\n",
    "            revenue DECIMAL(12,2),\n",
    "            conversions INTEGER,\n",
    "            paid_search_spend DECIMAL(12,2),\n",
    "            social_spend DECIMAL(12,2),\n",
    "            display_spend DECIMAL(12,2),\n",
    "            video_spend DECIMAL(12,2),\n",
    "            email_spend DECIMAL(12,2),\n",
    "            tv_spend DECIMAL(12,2),\n",
    "            radio_spend DECIMAL(12,2),\n",
    "            print_spend DECIMAL(12,2),\n",
    "            is_holiday BOOLEAN,\n",
    "            is_weekend BOOLEAN,\n",
    "            PRIMARY KEY (date, geo, segment)\n",
    "        )\n",
    "        DISTKEY(date)\n",
    "        SORTKEY(date, geo);\n",
    "        \n",
    "        -- Model results storage\n",
    "        CREATE TABLE IF NOT EXISTS mmm_model_results (\n",
    "            model_id VARCHAR(100),\n",
    "            model_type VARCHAR(50),\n",
    "            channel VARCHAR(50),\n",
    "            geo VARCHAR(50),\n",
    "            coefficient DECIMAL(10,6),\n",
    "            std_error DECIMAL(10,6),\n",
    "            roi DECIMAL(10,2),\n",
    "            contribution_pct DECIMAL(5,2),\n",
    "            training_date DATE,\n",
    "            model_version VARCHAR(20),\n",
    "            PRIMARY KEY (model_id, channel)\n",
    "        )\n",
    "        DISTKEY(model_id)\n",
    "        SORTKEY(training_date);\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(sql)\n",
    "        print(\"✓ MMM tables created\")\n",
    "    \n",
    "    def load_mmm_data(self, start_date, end_date, geo=None):\n",
    "        \"\"\"\n",
    "        Load aggregated MMM dataset.\n",
    "        \"\"\"\n",
    "        geo_filter = f\"AND geo = '{geo}'\" if geo else \"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM mmm_dataset\n",
    "        WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        {geo_filter}\n",
    "        ORDER BY date\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql(query, self.engine)\n",
    "    \n",
    "    def save_model_results(self, results_df, model_id):\n",
    "        \"\"\"\n",
    "        Save model results to Redshift.\n",
    "        \"\"\"\n",
    "        results_df['model_id'] = model_id\n",
    "        results_df['training_date'] = datetime.now().date()\n",
    "        \n",
    "        results_df.to_sql(\n",
    "            'mmm_model_results',\n",
    "            self.engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi'\n",
    "        )\n",
    "        print(f\"✓ Model results saved: {model_id}\")\n",
    "\n",
    "# Initialize (update credentials)\n",
    "# rs_mmm = RedshiftMMMData(\n",
    "#     host='your-cluster.region.redshift.amazonaws.com',\n",
    "#     port=5439,\n",
    "#     database='marketing_db',\n",
    "#     user='your_user',\n",
    "#     password='your_password'\n",
    "# )\n",
    "\n",
    "print(\"✓ Redshift MMM data loader configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic MMM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mmm_dataset(n_days=730, n_channels=8, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic MMM dataset.\n",
    "    Includes seasonality, trends, and channel interactions.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Date range\n",
    "    dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "    \n",
    "    # Channel configuration\n",
    "    channels = [\n",
    "        'Paid Search', 'Social Media', 'Display', 'Video',\n",
    "        'Email', 'TV', 'Radio', 'Print'\n",
    "    ]\n",
    "    \n",
    "    # True ROI for each channel (for validation)\n",
    "    true_roi = {\n",
    "        'Paid Search': 3.5,\n",
    "        'Social Media': 2.8,\n",
    "        'Display': 1.5,\n",
    "        'Video': 2.0,\n",
    "        'Email': 5.0,\n",
    "        'TV': 1.2,\n",
    "        'Radio': 1.0,\n",
    "        'Print': 0.8\n",
    "    }\n",
    "    \n",
    "    # Generate spend data with realistic patterns\n",
    "    data = {'date': dates}\n",
    "    \n",
    "    # Seasonality components\n",
    "    day_of_year = dates.dayofyear\n",
    "    seasonality = 1 + 0.3 * np.sin(2 * np.pi * day_of_year / 365.25)  # Annual\n",
    "    weekly_seasonality = 1 + 0.1 * np.sin(2 * np.pi * dates.dayofweek / 7)  # Weekly\n",
    "    \n",
    "    # Trend\n",
    "    trend = 1 + 0.001 * np.arange(n_days)  # Slight upward trend\n",
    "    \n",
    "    # Generate spend for each channel\n",
    "    base_spends = {\n",
    "        'Paid Search': 10000,\n",
    "        'Social Media': 8000,\n",
    "        'Display': 5000,\n",
    "        'Video': 6000,\n",
    "        'Email': 2000,\n",
    "        'TV': 15000,\n",
    "        'Radio': 4000,\n",
    "        'Print': 3000\n",
    "    }\n",
    "    \n",
    "    for channel in channels:\n",
    "        base_spend = base_spends[channel]\n",
    "        \n",
    "        # Spend varies with seasonality and random noise\n",
    "        spend = base_spend * seasonality * trend * (1 + 0.2 * np.random.randn(n_days))\n",
    "        spend = np.maximum(spend, 0)  # No negative spend\n",
    "        \n",
    "        col_name = channel.lower().replace(' ', '_') + '_spend'\n",
    "        data[col_name] = spend\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add external factors\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    # Holiday indicator (simplified)\n",
    "    holidays = []\n",
    "    for year in df['date'].dt.year.unique():\n",
    "        holidays.extend([\n",
    "            f\"{year}-01-01\",  # New Year\n",
    "            f\"{year}-07-04\",  # Independence Day\n",
    "            f\"{year}-11-25\",  # Thanksgiving (approximate)\n",
    "            f\"{year}-12-25\",  # Christmas\n",
    "        ])\n",
    "    df['is_holiday'] = df['date'].astype(str).isin(holidays).astype(int)\n",
    "    \n",
    "    # Generate revenue with adstock and saturation effects\n",
    "    revenue = np.zeros(n_days)\n",
    "    base_revenue = 50000  # Base daily revenue\n",
    "    \n",
    "    for channel in channels:\n",
    "        col_name = channel.lower().replace(' ', '_') + '_spend'\n",
    "        spend = df[col_name].values\n",
    "        \n",
    "        # Apply adstock (carryover effect)\n",
    "        adstock_rate = 0.5  # 50% carryover\n",
    "        adstocked_spend = np.zeros(n_days)\n",
    "        for i in range(n_days):\n",
    "            if i == 0:\n",
    "                adstocked_spend[i] = spend[i]\n",
    "            else:\n",
    "                adstocked_spend[i] = spend[i] + adstock_rate * adstocked_spend[i-1]\n",
    "        \n",
    "        # Apply saturation (diminishing returns)\n",
    "        # Hill function: S-shaped curve\n",
    "        K = np.median(adstocked_spend)  # Half-saturation point\n",
    "        S = 1.5  # Shape parameter\n",
    "        saturated_spend = adstocked_spend ** S / (K ** S + adstocked_spend ** S)\n",
    "        \n",
    "        # Contribution to revenue\n",
    "        roi = true_roi[channel]\n",
    "        revenue += saturated_spend * roi * base_spends[channel] / 1000\n",
    "    \n",
    "    # Add base revenue and external factors\n",
    "    revenue += base_revenue * seasonality * trend\n",
    "    revenue += 20000 * df['is_weekend'].values  # Weekend boost\n",
    "    revenue += 30000 * df['is_holiday'].values  # Holiday boost\n",
    "    \n",
    "    # Add noise\n",
    "    revenue += 5000 * np.random.randn(n_days)\n",
    "    revenue = np.maximum(revenue, 0)\n",
    "    \n",
    "    df['revenue'] = revenue\n",
    "    df['conversions'] = (revenue / 50).astype(int)  # Assume $50 AOV\n",
    "    \n",
    "    print(f\"✓ Generated MMM dataset: {n_days} days, {len(channels)} channels\")\n",
    "    print(f\"  Total spend: ${df[[c for c in df.columns if c.endswith('_spend')]].sum().sum():,.0f}\")\n",
    "    print(f\"  Total revenue: ${df['revenue'].sum():,.0f}\")\n",
    "    print(f\"  Overall ROAS: {df['revenue'].sum() / df[[c for c in df.columns if c.endswith('_spend')]].sum().sum():.2f}\")\n",
    "    \n",
    "    return df, true_roi\n",
    "\n",
    "# Generate dataset\n",
    "mmm_data, true_roi = generate_mmm_dataset(n_days=730)\n",
    "\n",
    "print(\"\\nDataset preview:\")\n",
    "print(mmm_data.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(mmm_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMMPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing for MMM including:\n",
    "    - Adstock transformation\n",
    "    - Saturation curves\n",
    "    - Feature scaling\n",
    "    - Seasonality decomposition\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.adstock_params = {}\n",
    "        self.saturation_params = {}\n",
    "        \n",
    "    def apply_adstock(self, spend, rate=0.5, L=8):\n",
    "        \"\"\"\n",
    "        Apply adstock transformation (geometric decay).\n",
    "        \n",
    "        Parameters:\n",
    "        - spend: Array of spend values\n",
    "        - rate: Decay rate (0-1)\n",
    "        - L: Maximum lag in days\n",
    "        \"\"\"\n",
    "        adstocked = np.zeros(len(spend))\n",
    "        \n",
    "        for i in range(len(spend)):\n",
    "            for lag in range(min(i + 1, L)):\n",
    "                adstocked[i] += spend[i - lag] * (rate ** lag)\n",
    "        \n",
    "        return adstocked\n",
    "    \n",
    "    def apply_hill_saturation(self, x, K, S):\n",
    "        \"\"\"\n",
    "        Apply Hill saturation curve (S-shaped).\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input values (adstocked spend)\n",
    "        - K: Half-saturation point\n",
    "        - S: Shape parameter (>1 for S-curve)\n",
    "        \"\"\"\n",
    "        return x ** S / (K ** S + x ** S)\n",
    "    \n",
    "    def transform_channel(self, spend, adstock_rate=0.5, K=None, S=1.5):\n",
    "        \"\"\"\n",
    "        Complete transformation pipeline for a channel.\n",
    "        \"\"\"\n",
    "        # Step 1: Adstock\n",
    "        adstocked = self.apply_adstock(spend, rate=adstock_rate)\n",
    "        \n",
    "        # Step 2: Saturation\n",
    "        if K is None:\n",
    "            K = np.median(adstocked[adstocked > 0])\n",
    "        \n",
    "        saturated = self.apply_hill_saturation(adstocked, K=K, S=S)\n",
    "        \n",
    "        return saturated\n",
    "    \n",
    "    def fit_transform(self, df, spend_columns, target='revenue'):\n",
    "        \"\"\"\n",
    "        Fit and transform all marketing channels.\n",
    "        \"\"\"\n",
    "        transformed_df = df.copy()\n",
    "        \n",
    "        for col in spend_columns:\n",
    "            spend = df[col].values\n",
    "            \n",
    "            # Transform\n",
    "            transformed = self.transform_channel(spend)\n",
    "            \n",
    "            # Store transformed values\n",
    "            new_col = col.replace('_spend', '_transformed')\n",
    "            transformed_df[new_col] = transformed\n",
    "            \n",
    "            # Scale\n",
    "            scaler = StandardScaler()\n",
    "            transformed_df[new_col] = scaler.fit_transform(\n",
    "                transformed_df[[new_col]]\n",
    "            )\n",
    "            self.scalers[col] = scaler\n",
    "        \n",
    "        print(f\"✓ Transformed {len(spend_columns)} channels\")\n",
    "        return transformed_df\n",
    "    \n",
    "    def decompose_seasonality(self, df, target='revenue'):\n",
    "        \"\"\"\n",
    "        Decompose time series into trend, seasonal, and residual.\n",
    "        \"\"\"\n",
    "        df_indexed = df.set_index('date')\n",
    "        \n",
    "        decomposition = seasonal_decompose(\n",
    "            df_indexed[target],\n",
    "            model='additive',\n",
    "            period=7  # Weekly seasonality\n",
    "        )\n",
    "        \n",
    "        df['trend'] = decomposition.trend.values\n",
    "        df['seasonal'] = decomposition.seasonal.values\n",
    "        df['residual'] = decomposition.resid.values\n",
    "        \n",
    "        # Fill NaN values (from decomposition)\n",
    "        df['trend'] = df['trend'].fillna(method='bfill').fillna(method='ffill')\n",
    "        df['seasonal'] = df['seasonal'].fillna(0)\n",
    "        df['residual'] = df['residual'].fillna(0)\n",
    "        \n",
    "        print(\"✓ Seasonality decomposed\")\n",
    "        return df\n",
    "\n",
    "# Test preprocessing\n",
    "preprocessor = MMMPreprocessor()\n",
    "\n",
    "spend_cols = [c for c in mmm_data.columns if c.endswith('_spend')]\n",
    "mmm_transformed = preprocessor.fit_transform(mmm_data, spend_cols)\n",
    "mmm_transformed = preprocessor.decompose_seasonality(mmm_transformed)\n",
    "\n",
    "print(\"\\nTransformed dataset columns:\")\n",
    "print([c for c in mmm_transformed.columns if 'transformed' in c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bayesian Marketing Mix Model with PyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianMMM:\n",
    "    \"\"\"\n",
    "    Bayesian Marketing Mix Model using PyMC3.\n",
    "    Provides uncertainty quantification and probabilistic predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.trace = None\n",
    "        self.channels = []\n",
    "        \n",
    "    def build_model(self, X, y, channel_names):\n",
    "        \"\"\"\n",
    "        Build Bayesian linear regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples, n_features)\n",
    "        - y: Target variable (revenue)\n",
    "        - channel_names: List of channel names\n",
    "        \"\"\"\n",
    "        self.channels = channel_names\n",
    "        n_channels = X.shape[1]\n",
    "        \n",
    "        with pm.Model() as model:\n",
    "            # Priors for channel coefficients\n",
    "            # Use half-normal to enforce positive coefficients\n",
    "            beta = pm.HalfNormal('beta', sigma=10, shape=n_channels)\n",
    "            \n",
    "            # Intercept (base revenue)\n",
    "            alpha = pm.Normal('alpha', mu=y.mean(), sigma=y.std())\n",
    "            \n",
    "            # Model error\n",
    "            sigma = pm.HalfNormal('sigma', sigma=y.std())\n",
    "            \n",
    "            # Expected value\n",
    "            mu = alpha + pm.math.dot(X, beta)\n",
    "            \n",
    "            # Likelihood\n",
    "            likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)\n",
    "        \n",
    "        self.model = model\n",
    "        print(\"✓ Bayesian model built\")\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, channel_names, n_samples=2000, tune=1000):\n",
    "        \"\"\"\n",
    "        Fit the model using MCMC sampling.\n",
    "        \"\"\"\n",
    "        print(f\"\\nFitting Bayesian MMM...\")\n",
    "        print(f\"Samples: {n_samples}, Tuning: {tune}\")\n",
    "        \n",
    "        self.build_model(X, y, channel_names)\n",
    "        \n",
    "        with self.model:\n",
    "            # MCMC sampling\n",
    "            self.trace = pm.sample(\n",
    "                n_samples,\n",
    "                tune=tune,\n",
    "                return_inferencedata=True,\n",
    "                progressbar=True\n",
    "            )\n",
    "        \n",
    "        print(\"✓ Model fitted\")\n",
    "        return self.trace\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get posterior summary statistics.\n",
    "        \"\"\"\n",
    "        summary = az.summary(self.trace, var_names=['alpha', 'beta', 'sigma'])\n",
    "        \n",
    "        # Add channel names\n",
    "        beta_summary = summary[summary.index.str.startswith('beta')].copy()\n",
    "        beta_summary.index = self.channels\n",
    "        \n",
    "        return beta_summary\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate posterior predictive samples.\n",
    "        \"\"\"\n",
    "        with self.model:\n",
    "            pm.set_data({'X': X})\n",
    "            posterior_pred = pm.sample_posterior_predictive(\n",
    "                self.trace,\n",
    "                var_names=['y']\n",
    "            )\n",
    "        \n",
    "        return posterior_pred\n",
    "    \n",
    "    def calculate_roi(self, spend_data):\n",
    "        \"\"\"\n",
    "        Calculate ROI for each channel with uncertainty.\n",
    "        \"\"\"\n",
    "        beta_samples = self.trace.posterior['beta'].values\n",
    "        \n",
    "        roi_results = {}\n",
    "        for i, channel in enumerate(self.channels):\n",
    "            # Get beta distribution for this channel\n",
    "            channel_betas = beta_samples[:, :, i].flatten()\n",
    "            \n",
    "            # Calculate ROI (revenue per $ spent)\n",
    "            total_spend = spend_data[i].sum()\n",
    "            \n",
    "            roi_results[channel] = {\n",
    "                'mean_beta': channel_betas.mean(),\n",
    "                'median_beta': np.median(channel_betas),\n",
    "                'ci_low': np.percentile(channel_betas, 2.5),\n",
    "                'ci_high': np.percentile(channel_betas, 97.5),\n",
    "            }\n",
    "        \n",
    "        return pd.DataFrame(roi_results).T\n",
    "    \n",
    "    def plot_diagnostics(self):\n",
    "        \"\"\"\n",
    "        Plot MCMC diagnostics.\n",
    "        \"\"\"\n",
    "        # Trace plots\n",
    "        az.plot_trace(self.trace, var_names=['alpha', 'beta', 'sigma'])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Posterior distributions\n",
    "        az.plot_posterior(self.trace, var_names=['beta'], ref_val=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Prepare data for Bayesian model\n",
    "transformed_cols = [c for c in mmm_transformed.columns if c.endswith('_transformed')]\n",
    "X = mmm_transformed[transformed_cols].values\n",
    "y = mmm_transformed['revenue'].values\n",
    "\n",
    "# Channel names (clean)\n",
    "channel_names = [c.replace('_transformed', '').replace('_', ' ').title() for c in transformed_cols]\n",
    "\n",
    "# Train/test split (time series)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "# Fit Bayesian model (reduce samples for demo)\n",
    "bayesian_mmm = BayesianMMM()\n",
    "trace = bayesian_mmm.fit(X_train, y_train, channel_names, n_samples=500, tune=500)\n",
    "\n",
    "# Get summary\n",
    "print(\"\\nPosterior Summary:\")\n",
    "summary = bayesian_mmm.get_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hierarchical Model for Geo/Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalMMM:\n",
    "    \"\"\"\n",
    "    Hierarchical Bayesian model for multi-geo/segment MMM.\n",
    "    Allows for partial pooling across geographies or segments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.trace = None\n",
    "        \n",
    "    def build_hierarchical_model(self, X, y, geo_idx, n_geos, n_channels):\n",
    "        \"\"\"\n",
    "        Build hierarchical model with geo-specific coefficients.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples, n_channels)\n",
    "        - y: Target (revenue)\n",
    "        - geo_idx: Geo index for each sample\n",
    "        - n_geos: Number of geographies\n",
    "        - n_channels: Number of marketing channels\n",
    "        \"\"\"\n",
    "        with pm.Model() as model:\n",
    "            # Hyperpriors for population distribution\n",
    "            mu_beta = pm.Normal('mu_beta', mu=0, sigma=10, shape=n_channels)\n",
    "            sigma_beta = pm.HalfNormal('sigma_beta', sigma=5, shape=n_channels)\n",
    "            \n",
    "            # Geo-specific coefficients (partial pooling)\n",
    "            beta = pm.Normal(\n",
    "                'beta',\n",
    "                mu=mu_beta,\n",
    "                sigma=sigma_beta,\n",
    "                shape=(n_geos, n_channels)\n",
    "            )\n",
    "            \n",
    "            # Geo-specific intercepts\n",
    "            mu_alpha = pm.Normal('mu_alpha', mu=y.mean(), sigma=y.std())\n",
    "            sigma_alpha = pm.HalfNormal('sigma_alpha', sigma=y.std())\n",
    "            alpha = pm.Normal('alpha', mu=mu_alpha, sigma=sigma_alpha, shape=n_geos)\n",
    "            \n",
    "            # Model error\n",
    "            sigma = pm.HalfNormal('sigma', sigma=y.std())\n",
    "            \n",
    "            # Expected value (vectorized)\n",
    "            mu = alpha[geo_idx] + pm.math.sum(X * beta[geo_idx], axis=1)\n",
    "            \n",
    "            # Likelihood\n",
    "            likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)\n",
    "        \n",
    "        self.model = model\n",
    "        print(\"✓ Hierarchical model built\")\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, geo_idx, n_geos, n_channels, n_samples=1000, tune=500):\n",
    "        \"\"\"\n",
    "        Fit hierarchical model.\n",
    "        \"\"\"\n",
    "        print(f\"\\nFitting Hierarchical MMM for {n_geos} geos...\")\n",
    "        \n",
    "        self.build_hierarchical_model(X, y, geo_idx, n_geos, n_channels)\n",
    "        \n",
    "        with self.model:\n",
    "            self.trace = pm.sample(\n",
    "                n_samples,\n",
    "                tune=tune,\n",
    "                return_inferencedata=True,\n",
    "                progressbar=True\n",
    "            )\n",
    "        \n",
    "        print(\"✓ Hierarchical model fitted\")\n",
    "        return self.trace\n",
    "    \n",
    "    def get_geo_coefficients(self, geo_names):\n",
    "        \"\"\"\n",
    "        Extract coefficients for each geo.\n",
    "        \"\"\"\n",
    "        beta_samples = self.trace.posterior['beta'].values\n",
    "        \n",
    "        results = {}\n",
    "        for geo_idx, geo_name in enumerate(geo_names):\n",
    "            geo_betas = beta_samples[:, :, geo_idx, :]\n",
    "            results[geo_name] = {\n",
    "                'mean': geo_betas.mean(axis=(0, 1)),\n",
    "                'std': geo_betas.std(axis=(0, 1))\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example: Generate multi-geo data\n",
    "def generate_multi_geo_data(n_geos=3, n_days=365):\n",
    "    \"\"\"Generate data for multiple geographies\"\"\"\n",
    "    all_data = []\n",
    "    geo_names = [f\"Geo_{i+1}\" for i in range(n_geos)]\n",
    "    \n",
    "    for geo_name in geo_names:\n",
    "        # Generate data with geo-specific variations\n",
    "        geo_data, _ = generate_mmm_dataset(n_days=n_days, random_state=hash(geo_name) % 1000)\n",
    "        geo_data['geo'] = geo_name\n",
    "        all_data.append(geo_data)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True), geo_names\n",
    "\n",
    "print(\"✓ Hierarchical MMM configured\")\n",
    "print(\"  Can model geo-specific and segment-specific responses\")\n",
    "print(\"  Enables partial pooling for better estimates with limited data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Model Selection and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMMModelSelector:\n",
    "    \"\"\"\n",
    "    Automated model selection using cross-validation and\n",
    "    hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.cv_results = []\n",
    "        \n",
    "    def time_series_cv(self, X, y, n_splits=5):\n",
    "        \"\"\"\n",
    "        Time series cross-validation.\n",
    "        \"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        \n",
    "        cv_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Fit simple linear model for CV\n",
    "            from sklearn.linear_model import Ridge\n",
    "            model = Ridge(alpha=1.0)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict and score\n",
    "            y_pred = model.predict(X_val)\n",
    "            \n",
    "            mse = mean_squared_error(y_val, y_pred)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            \n",
    "            cv_scores.append({\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'r2': r2\n",
    "            })\n",
    "        \n",
    "        cv_df = pd.DataFrame(cv_scores)\n",
    "        print(f\"\\nCross-validation results ({n_splits} folds):\")\n",
    "        print(cv_df)\n",
    "        print(f\"\\nMean R²: {cv_df['r2'].mean():.4f} ± {cv_df['r2'].std():.4f}\")\n",
    "        \n",
    "        return cv_df\n",
    "    \n",
    "    def optimize_hyperparameters(self, X, y, n_trials=50):\n",
    "        \"\"\"\n",
    "        Optimize adstock and saturation parameters using Optuna.\n",
    "        \"\"\"\n",
    "        def objective(trial):\n",
    "            # Suggest hyperparameters\n",
    "            adstock_rate = trial.suggest_float('adstock_rate', 0.0, 0.9)\n",
    "            saturation_K = trial.suggest_float('saturation_K', 0.1, 10.0)\n",
    "            saturation_S = trial.suggest_float('saturation_S', 0.5, 3.0)\n",
    "            \n",
    "            # Transform data with these parameters\n",
    "            preprocessor = MMMPreprocessor()\n",
    "            # (Simplified - would need full pipeline)\n",
    "            \n",
    "            # Fit model and evaluate\n",
    "            from sklearn.linear_model import Ridge\n",
    "            model = Ridge(alpha=trial.suggest_float('ridge_alpha', 0.1, 10.0))\n",
    "            \n",
    "            # Time series CV\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                \n",
    "                mse = mean_squared_error(y_val, y_pred)\n",
    "                scores.append(mse)\n",
    "            \n",
    "            return np.mean(scores)\n",
    "        \n",
    "        # Run optimization\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        \n",
    "        print(f\"\\n✓ Hyperparameter optimization complete\")\n",
    "        print(f\"Best parameters: {self.best_params}\")\n",
    "        print(f\"Best MSE: {study.best_value:,.2f}\")\n",
    "        \n",
    "        return study\n",
    "    \n",
    "    def compare_models(self, X, y):\n",
    "        \"\"\"\n",
    "        Compare different model types.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        models = {\n",
    "            'Ridge': Ridge(alpha=1.0),\n",
    "            'Lasso': Lasso(alpha=0.1),\n",
    "            'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "            'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "            'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        }\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Train/test split\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            # Fit\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            \n",
    "            # Metrics\n",
    "            results.append({\n",
    "                'model': name,\n",
    "                'train_r2': r2_score(y_train, y_pred_train),\n",
    "                'test_r2': r2_score(y_test, y_pred_test),\n",
    "                'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "                'test_mape': np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results).sort_values('test_r2', ascending=False)\n",
    "        \n",
    "        print(\"\\nModel Comparison:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "# Test model selection\n",
    "selector = MMMModelSelector()\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = selector.time_series_cv(X_train, y_train, n_splits=5)\n",
    "\n",
    "# Model comparison\n",
    "model_comparison = selector.compare_models(X, y)\n",
    "\n",
    "# Hyperparameter optimization (reduce trials for demo)\n",
    "# study = selector.optimize_hyperparameters(X_train, y_train, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Budget Optimization with Constraints\n",
    "\n",
    "### Maximize ROI Subject to Budget and Channel Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BudgetOptimizer:\n",
    "    \"\"\"\n",
    "    Optimize marketing budget allocation using response curves\n",
    "    and constraint optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_coefficients, adstock_params, saturation_params):\n",
    "        self.coefficients = model_coefficients\n",
    "        self.adstock_params = adstock_params\n",
    "        self.saturation_params = saturation_params\n",
    "        \n",
    "    def response_curve(self, spend, channel_idx):\n",
    "        \"\"\"\n",
    "        Calculate expected response for given spend.\n",
    "        \"\"\"\n",
    "        # Apply adstock (simplified - no time dimension)\n",
    "        adstock_rate = self.adstock_params.get(channel_idx, 0.5)\n",
    "        adstocked_spend = spend / (1 - adstock_rate)\n",
    "        \n",
    "        # Apply saturation\n",
    "        K = self.saturation_params.get(channel_idx, {}).get('K', spend)\n",
    "        S = self.saturation_params.get(channel_idx, {}).get('S', 1.5)\n",
    "        \n",
    "        saturated = adstocked_spend ** S / (K ** S + adstocked_spend ** S)\n",
    "        \n",
    "        # Apply coefficient\n",
    "        response = self.coefficients[channel_idx] * saturated\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def total_response(self, spend_allocation):\n",
    "        \"\"\"\n",
    "        Calculate total expected response for spend allocation.\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for channel_idx, spend in enumerate(spend_allocation):\n",
    "            total += self.response_curve(spend, channel_idx)\n",
    "        return total\n",
    "    \n",
    "    def optimize_budget(self, total_budget, channel_constraints=None):\n",
    "        \"\"\"\n",
    "        Optimize budget allocation to maximize total response.\n",
    "        \n",
    "        Parameters:\n",
    "        - total_budget: Total budget to allocate\n",
    "        - channel_constraints: Dict with 'min' and 'max' for each channel\n",
    "        \"\"\"\n",
    "        n_channels = len(self.coefficients)\n",
    "        \n",
    "        # Objective: maximize total response (minimize negative)\n",
    "        def objective(spend_allocation):\n",
    "            return -self.total_response(spend_allocation)\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = []\n",
    "        \n",
    "        # Budget constraint: sum of spend = total_budget\n",
    "        constraints.append({\n",
    "            'type': 'eq',\n",
    "            'fun': lambda x: np.sum(x) - total_budget\n",
    "        })\n",
    "        \n",
    "        # Channel bounds\n",
    "        if channel_constraints is None:\n",
    "            # Default: each channel 0% to 50% of budget\n",
    "            bounds = [(0, total_budget * 0.5) for _ in range(n_channels)]\n",
    "        else:\n",
    "            bounds = [\n",
    "                (channel_constraints[i]['min'], channel_constraints[i]['max'])\n",
    "                for i in range(n_channels)\n",
    "            ]\n",
    "        \n",
    "        # Initial guess: equal allocation\n",
    "        x0 = np.array([total_budget / n_channels] * n_channels)\n",
    "        \n",
    "        # Optimize\n",
    "        result = optimize.minimize(\n",
    "            objective,\n",
    "            x0,\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints\n",
    "        )\n",
    "        \n",
    "        optimal_allocation = result.x\n",
    "        expected_response = -result.fun\n",
    "        \n",
    "        return optimal_allocation, expected_response\n",
    "    \n",
    "    def scenario_analysis(self, budget_range, channel_names):\n",
    "        \"\"\"\n",
    "        Analyze different budget scenarios.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for budget in budget_range:\n",
    "            allocation, response = self.optimize_budget(budget)\n",
    "            \n",
    "            result = {'total_budget': budget, 'expected_response': response}\n",
    "            for i, channel in enumerate(channel_names):\n",
    "                result[f\"{channel}_allocation\"] = allocation[i]\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def visualize_response_curves(self, channel_names, max_spend=50000):\n",
    "        \"\"\"\n",
    "        Visualize response curves for each channel.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        spend_range = np.linspace(0, max_spend, 100)\n",
    "        \n",
    "        for idx, channel in enumerate(channel_names):\n",
    "            if idx < len(axes):\n",
    "                responses = [self.response_curve(s, idx) for s in spend_range]\n",
    "                \n",
    "                ax = axes[idx]\n",
    "                ax.plot(spend_range, responses, linewidth=2)\n",
    "                ax.set_title(f\"{channel} Response Curve\", fontweight='bold')\n",
    "                ax.set_xlabel('Spend ($)')\n",
    "                ax.set_ylabel('Response')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test budget optimization\n",
    "# Extract coefficients from Bayesian model\n",
    "beta_means = bayesian_mmm.get_summary()['mean'].values\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = BudgetOptimizer(\n",
    "    model_coefficients=beta_means,\n",
    "    adstock_params={i: 0.5 for i in range(len(beta_means))},\n",
    "    saturation_params={i: {'K': 10000, 'S': 1.5} for i in range(len(beta_means))}\n",
    ")\n",
    "\n",
    "# Optimize for total budget\n",
    "total_budget = 200000\n",
    "optimal_allocation, expected_response = optimizer.optimize_budget(total_budget)\n",
    "\n",
    "print(f\"\\nOptimal Budget Allocation for ${total_budget:,.0f}:\")\n",
    "allocation_df = pd.DataFrame({\n",
    "    'channel': channel_names,\n",
    "    'optimal_spend': optimal_allocation,\n",
    "    'percent_of_budget': 100 * optimal_allocation / total_budget\n",
    "}).sort_values('optimal_spend', ascending=False)\n",
    "print(allocation_df.to_string(index=False))\n",
    "print(f\"\\nExpected Response: {expected_response:,.2f}\")\n",
    "\n",
    "# Visualize response curves\n",
    "optimizer.visualize_response_curves(channel_names, max_spend=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Deployment: Real-World Enterprise MMM Platform\n",
    "\n",
    "### Complete End-to-End System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseMMM Platform:\n",
    "    \"\"\"\n",
    "    Production-ready MMM platform integrating:\n",
    "    - Data pipeline from Redshift\n",
    "    - Automated model training and validation\n",
    "    - Budget optimization\n",
    "    - Performance monitoring\n",
    "    - API for predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rs_connection):\n",
    "        self.rs = rs_connection\n",
    "        self.models = {}\n",
    "        self.performance_history = []\n",
    "        \n",
    "    def train_pipeline(self, start_date, end_date, model_type='bayesian'):\n",
    "        \"\"\"\n",
    "        Complete training pipeline.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ENTERPRISE MMM TRAINING PIPELINE\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # 1. Load data\n",
    "        print(\"\\n[1/6] Loading data from Redshift...\")\n",
    "        data = self.rs.load_mmm_data(start_date, end_date)\n",
    "        print(f\"  Loaded {len(data):,} records\")\n",
    "        \n",
    "        # 2. Preprocess\n",
    "        print(\"\\n[2/6] Preprocessing...\")\n",
    "        preprocessor = MMMPreprocessor()\n",
    "        spend_cols = [c for c in data.columns if c.endswith('_spend')]\n",
    "        data_transformed = preprocessor.fit_transform(data, spend_cols)\n",
    "        data_transformed = preprocessor.decompose_seasonality(data_transformed)\n",
    "        \n",
    "        # 3. Feature engineering\n",
    "        print(\"\\n[3/6] Feature engineering...\")\n",
    "        transformed_cols = [c for c in data_transformed.columns if c.endswith('_transformed')]\n",
    "        X = data_transformed[transformed_cols].values\n",
    "        y = data_transformed['revenue'].values\n",
    "        \n",
    "        # 4. Train/validation split\n",
    "        print(\"\\n[4/6] Train/validation split...\")\n",
    "        split_idx = int(0.8 * len(X))\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        print(f\"  Training: {len(X_train):,} samples\")\n",
    "        print(f\"  Validation: {len(X_val):,} samples\")\n",
    "        \n",
    "        # 5. Train model\n",
    "        print(f\"\\n[5/6] Training {model_type} model...\")\n",
    "        if model_type == 'bayesian':\n",
    "            model = BayesianMMM()\n",
    "            channel_names = [c.replace('_transformed', '') for c in transformed_cols]\n",
    "            model.fit(X_train, y_train, channel_names, n_samples=500, tune=500)\n",
    "        else:\n",
    "            from sklearn.linear_model import Ridge\n",
    "            model = Ridge(alpha=1.0)\n",
    "            model.fit(X_train, y_train)\n",
    "        \n",
    "        # 6. Validate\n",
    "        print(\"\\n[6/6] Validation...\")\n",
    "        if model_type == 'bayesian':\n",
    "            # Use posterior mean for prediction\n",
    "            beta_means = model.get_summary()['mean'].values\n",
    "            alpha_mean = model.trace.posterior['alpha'].values.mean()\n",
    "            y_pred = alpha_mean + X_val @ beta_means\n",
    "        else:\n",
    "            y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Metrics\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        mape = np.mean(np.abs((y_val - y_pred) / y_val)) * 100\n",
    "        \n",
    "        print(f\"\\n  R² Score: {r2:.4f}\")\n",
    "        print(f\"  MAE: ${mae:,.2f}\")\n",
    "        print(f\"  MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        # Store model\n",
    "        model_id = f\"mmm_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.models[model_id] = {\n",
    "            'model': model,\n",
    "            'preprocessor': preprocessor,\n",
    "            'metrics': {'r2': r2, 'mae': mae, 'mape': mape},\n",
    "            'training_date': datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Save to Redshift\n",
    "        if model_type == 'bayesian':\n",
    "            results_df = model.get_summary().reset_index()\n",
    "            results_df.columns = ['channel', 'mean', 'std', 'hdi_3%', 'hdi_97%', 'mcse_mean', 'mcse_std', 'ess_bulk', 'ess_tail', 'r_hat']\n",
    "            results_df = results_df[['channel', 'mean', 'std']]\n",
    "            results_df.columns = ['channel', 'coefficient', 'std_error']\n",
    "            results_df['model_type'] = model_type\n",
    "            results_df['roi'] = 0  # Calculate separately\n",
    "            results_df['contribution_pct'] = 0  # Calculate separately\n",
    "            results_df['geo'] = 'ALL'\n",
    "            results_df['model_version'] = 'v1.0'\n",
    "            \n",
    "            self.rs.save_model_results(results_df, model_id)\n",
    "        \n",
    "        print(f\"\\n✓ Pipeline complete. Model ID: {model_id}\")\n",
    "        return model_id\n",
    "    \n",
    "    def generate_report(self, model_id):\n",
    "        \"\"\"\n",
    "        Generate comprehensive MMM report.\n",
    "        \"\"\"\n",
    "        if model_id not in self.models:\n",
    "            print(f\"Model {model_id} not found\")\n",
    "            return\n",
    "        \n",
    "        model_info = self.models[model_id]\n",
    "        model = model_info['model']\n",
    "        metrics = model_info['metrics']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MMM MODEL REPORT: {model_id}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(f\"\\nTraining Date: {model_info['training_date']}\")\n",
    "        print(f\"\\nModel Performance:\")\n",
    "        print(f\"  R² Score: {metrics['r2']:.4f}\")\n",
    "        print(f\"  MAE: ${metrics['mae']:,.2f}\")\n",
    "        print(f\"  MAPE: {metrics['mape']:.2f}%\")\n",
    "        \n",
    "        if hasattr(model, 'get_summary'):\n",
    "            print(f\"\\nChannel Coefficients:\")\n",
    "            summary = model.get_summary()\n",
    "            print(summary[['mean', 'sd', 'hdi_3%', 'hdi_97%']].to_string())\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "    \n",
    "    def save_model(self, model_id, filepath):\n",
    "        \"\"\"\n",
    "        Save model to disk.\n",
    "        \"\"\"\n",
    "        if model_id not in self.models:\n",
    "            print(f\"Model {model_id} not found\")\n",
    "            return\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self.models[model_id], f)\n",
    "        \n",
    "        print(f\"✓ Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"\n",
    "        Load model from disk.\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        \n",
    "        model_id = f\"loaded_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.models[model_id] = model_info\n",
    "        \n",
    "        print(f\"✓ Model loaded as {model_id}\")\n",
    "        return model_id\n",
    "\n",
    "print(\"✓ Enterprise MMM Platform configured\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  - Automated data pipeline from Redshift\")\n",
    "print(\"  - Bayesian and frequentist modeling\")\n",
    "print(\"  - Hierarchical models for geo/segment analysis\")\n",
    "print(\"  - Automated hyperparameter tuning\")\n",
    "print(\"  - Budget optimization with constraints\")\n",
    "print(\"  - Model versioning and storage\")\n",
    "print(\"  - Performance monitoring and reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Production Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Model Architecture**\n",
    "   - Use Bayesian methods for uncertainty quantification\n",
    "   - Implement hierarchical models for partial pooling across geos/segments\n",
    "   - Apply proper transformations (adstock, saturation)\n",
    "\n",
    "2. **Validation**\n",
    "   - Use time series cross-validation\n",
    "   - Monitor multiple metrics (R², MAE, MAPE)\n",
    "   - Compare against holdout periods\n",
    "\n",
    "3. **Optimization**\n",
    "   - Optimize budgets with realistic constraints\n",
    "   - Account for diminishing returns\n",
    "   - Run scenario analyses\n",
    "\n",
    "4. **Production Deployment**\n",
    "   - Automate data pipelines\n",
    "   - Version control models\n",
    "   - Monitor performance over time\n",
    "   - Retrain regularly with new data\n",
    "\n",
    "### Next Steps\n",
    "- Implement real-time model scoring API\n",
    "- Build automated alerting for model drift\n",
    "- Create interactive budget planning dashboard\n",
    "- Integrate with marketing automation platforms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
