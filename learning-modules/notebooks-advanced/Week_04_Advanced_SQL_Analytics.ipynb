{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Advanced SQL Analytics for Redshift\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement advanced analytics with Redshift SQL\n",
    "- Create and use Python UDFs in Redshift\n",
    "- Master recursive CTEs for complex hierarchies\n",
    "- Perform graph analytics for attribution paths\n",
    "- Analyze time-series data at scale\n",
    "- Build advanced aggregations and pivots\n",
    "- Implement data quality checks at scale\n",
    "- Optimize multi-touch attribution queries\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install pandas psycopg2-binary sqlalchemy numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Redshift configuration\n",
    "REDSHIFT_CONFIG = {\n",
    "    'host': os.getenv('REDSHIFT_HOST', 'your-cluster.region.redshift.amazonaws.com'),\n",
    "    'port': int(os.getenv('REDSHIFT_PORT', 5439)),\n",
    "    'database': os.getenv('REDSHIFT_DB', 'marketing'),\n",
    "    'user': os.getenv('REDSHIFT_USER', 'analyst'),\n",
    "    'password': os.getenv('REDSHIFT_PASSWORD', 'your-password')\n",
    "}\n",
    "\n",
    "# Create connection\n",
    "connection_string = (\n",
    "    f\"postgresql+psycopg2://{REDSHIFT_CONFIG['user']}:{REDSHIFT_CONFIG['password']}\"\n",
    "    f\"@{REDSHIFT_CONFIG['host']}:{REDSHIFT_CONFIG['port']}/{REDSHIFT_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(connection_string, pool_pre_ping=True)\n",
    "\n",
    "def execute_query(query: str, params: Dict = None) -> pd.DataFrame:\n",
    "    \"\"\"Execute query and return results as DataFrame.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), params or {})\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "        \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Query returned {len(df):,} rows in {elapsed:.2f}s\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✓ Connected to Redshift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. User-Defined Functions (UDFs)\n",
    "\n",
    "### 2.1 Python UDFs in Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Scalar Python UDF for advanced string manipulation\n",
    "create_udf_extract_domain = \"\"\"\n",
    "CREATE OR REPLACE FUNCTION f_extract_domain(email VARCHAR)\n",
    "RETURNS VARCHAR\n",
    "IMMUTABLE\n",
    "AS $$\n",
    "    if email and '@' in email:\n",
    "        return email.split('@')[1].lower()\n",
    "    return None\n",
    "$$ LANGUAGE plpythonu;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: UDF for JSON parsing\n",
    "create_udf_parse_json = \"\"\"\n",
    "CREATE OR REPLACE FUNCTION f_parse_json_field(json_str VARCHAR, field_name VARCHAR)\n",
    "RETURNS VARCHAR\n",
    "STABLE\n",
    "AS $$\n",
    "    import json\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        return str(data.get(field_name, ''))\n",
    "    except:\n",
    "        return None\n",
    "$$ LANGUAGE plpythonu;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: UDF for custom scoring logic\n",
    "create_udf_lead_score = \"\"\"\n",
    "CREATE OR REPLACE FUNCTION f_calculate_lead_score(\n",
    "    page_views INTEGER,\n",
    "    email_opens INTEGER,\n",
    "    downloads INTEGER,\n",
    "    recency_days INTEGER\n",
    ")\n",
    "RETURNS INTEGER\n",
    "IMMUTABLE\n",
    "AS $$\n",
    "    # Scoring logic\n",
    "    score = 0\n",
    "    \n",
    "    # Page views (max 30 points)\n",
    "    score += min(page_views * 2, 30)\n",
    "    \n",
    "    # Email engagement (max 25 points)\n",
    "    score += min(email_opens * 5, 25)\n",
    "    \n",
    "    # Downloads (max 30 points)\n",
    "    score += min(downloads * 10, 30)\n",
    "    \n",
    "    # Recency bonus (max 15 points)\n",
    "    if recency_days <= 7:\n",
    "        score += 15\n",
    "    elif recency_days <= 30:\n",
    "        score += 10\n",
    "    elif recency_days <= 90:\n",
    "        score += 5\n",
    "    \n",
    "    return min(score, 100)\n",
    "$$ LANGUAGE plpythonu;\n",
    "\"\"\"\n",
    "\n",
    "# Usage example\n",
    "use_udf_example = \"\"\"\n",
    "SELECT \n",
    "    user_id,\n",
    "    email,\n",
    "    f_extract_domain(email) as email_domain,\n",
    "    page_views,\n",
    "    email_opens,\n",
    "    downloads,\n",
    "    recency_days,\n",
    "    f_calculate_lead_score(page_views, email_opens, downloads, recency_days) as lead_score\n",
    "FROM user_engagement\n",
    "WHERE f_calculate_lead_score(page_views, email_opens, downloads, recency_days) >= 70;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Python UDF Best Practices:\n",
    "-------------------------\n",
    "1. Use IMMUTABLE when function always returns same result for same inputs\n",
    "2. Use STABLE when result depends on database state but not time\n",
    "3. Use VOLATILE when result can change (least optimal)\n",
    "4. Keep UDFs simple - complex logic should be in application layer\n",
    "5. Test UDFs thoroughly with edge cases\n",
    "6. Document UDF behavior and expected inputs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recursive CTEs for Hierarchical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: User referral hierarchy\n",
    "recursive_referrals = \"\"\"\n",
    "WITH RECURSIVE referral_chain AS (\n",
    "    -- Base case: users with no referrer (level 0)\n",
    "    SELECT \n",
    "        user_id,\n",
    "        referred_by_user_id,\n",
    "        user_id as root_user_id,\n",
    "        0 as level,\n",
    "        CAST(user_id AS VARCHAR) as path\n",
    "    FROM users\n",
    "    WHERE referred_by_user_id IS NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Recursive case: users referred by previous level\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.referred_by_user_id,\n",
    "        rc.root_user_id,\n",
    "        rc.level + 1,\n",
    "        rc.path || ' -> ' || CAST(u.user_id AS VARCHAR)\n",
    "    FROM users u\n",
    "    INNER JOIN referral_chain rc ON u.referred_by_user_id = rc.user_id\n",
    "    WHERE rc.level < 10  -- Prevent infinite loops\n",
    ")\n",
    "SELECT \n",
    "    root_user_id,\n",
    "    COUNT(*) as total_referrals,\n",
    "    MAX(level) as max_depth,\n",
    "    AVG(level) as avg_depth\n",
    "FROM referral_chain\n",
    "GROUP BY root_user_id\n",
    "HAVING COUNT(*) > 10\n",
    "ORDER BY total_referrals DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Campaign hierarchy (parent-child campaigns)\n",
    "recursive_campaign_hierarchy = \"\"\"\n",
    "WITH RECURSIVE campaign_tree AS (\n",
    "    -- Root campaigns\n",
    "    SELECT \n",
    "        campaign_id,\n",
    "        parent_campaign_id,\n",
    "        campaign_name,\n",
    "        budget,\n",
    "        0 as level,\n",
    "        CAST(campaign_name AS VARCHAR(1000)) as hierarchy_path\n",
    "    FROM campaigns\n",
    "    WHERE parent_campaign_id IS NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    -- Child campaigns\n",
    "    SELECT \n",
    "        c.campaign_id,\n",
    "        c.parent_campaign_id,\n",
    "        c.campaign_name,\n",
    "        c.budget,\n",
    "        ct.level + 1,\n",
    "        ct.hierarchy_path || ' > ' || c.campaign_name\n",
    "    FROM campaigns c\n",
    "    INNER JOIN campaign_tree ct ON c.parent_campaign_id = ct.campaign_id\n",
    "    WHERE ct.level < 5\n",
    ")\n",
    "SELECT \n",
    "    campaign_id,\n",
    "    campaign_name,\n",
    "    level,\n",
    "    hierarchy_path,\n",
    "    budget,\n",
    "    SUM(budget) OVER (PARTITION BY SPLIT_PART(hierarchy_path, ' > ', 1)) as root_total_budget\n",
    "FROM campaign_tree\n",
    "ORDER BY hierarchy_path;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Event sequence chains\n",
    "recursive_event_sequences = \"\"\"\n",
    "WITH RECURSIVE event_chain AS (\n",
    "    -- First event for each user\n",
    "    SELECT \n",
    "        user_id,\n",
    "        event_id,\n",
    "        event_type,\n",
    "        channel,\n",
    "        timestamp,\n",
    "        1 as step_number,\n",
    "        event_type as event_path,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as rn\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= '2024-01-01'\n",
    "),\n",
    "first_events AS (\n",
    "    SELECT *\n",
    "    FROM event_chain\n",
    "    WHERE rn = 1\n",
    "),\n",
    "chain AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        event_id,\n",
    "        event_type,\n",
    "        channel,\n",
    "        timestamp,\n",
    "        step_number,\n",
    "        event_path\n",
    "    FROM first_events\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        e.user_id,\n",
    "        e.event_id,\n",
    "        e.event_type,\n",
    "        e.channel,\n",
    "        e.timestamp,\n",
    "        c.step_number + 1,\n",
    "        c.event_path || ' -> ' || e.event_type\n",
    "    FROM marketing_events e\n",
    "    INNER JOIN chain c \n",
    "        ON e.user_id = c.user_id \n",
    "        AND e.timestamp > c.timestamp\n",
    "    WHERE c.step_number < 20\n",
    "      AND e.timestamp <= c.timestamp + INTERVAL '7 days'\n",
    ")\n",
    "SELECT \n",
    "    event_path,\n",
    "    COUNT(DISTINCT user_id) as user_count,\n",
    "    AVG(step_number) as avg_steps\n",
    "FROM chain\n",
    "WHERE event_path LIKE '%conversion'\n",
    "GROUP BY event_path\n",
    "ORDER BY user_count DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "Recursive CTE Best Practices:\n",
    "----------------------------\n",
    "1. Always include termination condition (level < N)\n",
    "2. Monitor query performance - recursion can be expensive\n",
    "3. Use appropriate indexes on join columns\n",
    "4. Consider materialized views for frequently-used hierarchies\n",
    "5. Test with realistic data volumes\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Analytics for Attribution Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Build user journey graph\n",
    "user_journey_graph = \"\"\"\n",
    "WITH user_events AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        channel,\n",
    "        event_type,\n",
    "        timestamp,\n",
    "        revenue,\n",
    "        LAG(channel) OVER (PARTITION BY user_id ORDER BY timestamp) as prev_channel,\n",
    "        LEAD(channel) OVER (PARTITION BY user_id ORDER BY timestamp) as next_channel,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as position\n",
    "    FROM marketing_events\n",
    "    WHERE event_type IN ('impression', 'click', 'conversion')\n",
    "),\n",
    "channel_transitions AS (\n",
    "    SELECT \n",
    "        prev_channel as from_channel,\n",
    "        channel as to_channel,\n",
    "        COUNT(*) as transition_count,\n",
    "        COUNT(DISTINCT user_id) as user_count,\n",
    "        SUM(CASE WHEN event_type = 'conversion' THEN revenue ELSE 0 END) as attributed_revenue\n",
    "    FROM user_events\n",
    "    WHERE prev_channel IS NOT NULL\n",
    "    GROUP BY prev_channel, channel\n",
    ")\n",
    "SELECT \n",
    "    from_channel,\n",
    "    to_channel,\n",
    "    transition_count,\n",
    "    user_count,\n",
    "    attributed_revenue,\n",
    "    transition_count * 1.0 / SUM(transition_count) OVER (PARTITION BY from_channel) as transition_probability\n",
    "FROM channel_transitions\n",
    "ORDER BY transition_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Calculate path values (Shapley value approximation)\n",
    "path_value_analysis = \"\"\"\n",
    "WITH conversion_paths AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        LISTAGG(channel, ' > ') WITHIN GROUP (ORDER BY timestamp) as full_path,\n",
    "        COUNT(*) as touchpoints,\n",
    "        MAX(CASE WHEN event_type = 'conversion' THEN revenue ELSE 0 END) as conversion_value\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= '2024-01-01'\n",
    "    GROUP BY user_id\n",
    "    HAVING MAX(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) = 1\n",
    "),\n",
    "path_stats AS (\n",
    "    SELECT \n",
    "        full_path,\n",
    "        COUNT(*) as path_frequency,\n",
    "        SUM(conversion_value) as total_value,\n",
    "        AVG(conversion_value) as avg_value,\n",
    "        AVG(touchpoints) as avg_touchpoints\n",
    "    FROM conversion_paths\n",
    "    GROUP BY full_path\n",
    ")\n",
    "SELECT \n",
    "    full_path,\n",
    "    path_frequency,\n",
    "    total_value,\n",
    "    avg_value,\n",
    "    avg_touchpoints,\n",
    "    path_frequency * 1.0 / SUM(path_frequency) OVER () as path_percentage\n",
    "FROM path_stats\n",
    "WHERE path_frequency >= 10\n",
    "ORDER BY total_value DESC\n",
    "LIMIT 50;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Channel network analysis\n",
    "channel_network_metrics = \"\"\"\n",
    "WITH channel_pairs AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        channel as current_channel,\n",
    "        LAG(channel, 1) OVER (PARTITION BY user_id ORDER BY timestamp) as prev_1,\n",
    "        LAG(channel, 2) OVER (PARTITION BY user_id ORDER BY timestamp) as prev_2,\n",
    "        event_type,\n",
    "        revenue\n",
    "    FROM marketing_events\n",
    "),\n",
    "channel_influence AS (\n",
    "    SELECT \n",
    "        current_channel,\n",
    "        prev_1,\n",
    "        -- Direct influence (immediate predecessor)\n",
    "        COUNT(*) as direct_count,\n",
    "        SUM(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) as direct_conversions,\n",
    "        SUM(CASE WHEN event_type = 'conversion' THEN revenue ELSE 0 END) as direct_revenue\n",
    "    FROM channel_pairs\n",
    "    WHERE prev_1 IS NOT NULL\n",
    "    GROUP BY current_channel, prev_1\n",
    ")\n",
    "SELECT \n",
    "    prev_1 as influencing_channel,\n",
    "    current_channel as influenced_channel,\n",
    "    direct_count,\n",
    "    direct_conversions,\n",
    "    direct_revenue,\n",
    "    direct_conversions * 1.0 / NULLIF(direct_count, 0) as conversion_rate,\n",
    "    direct_revenue / NULLIF(direct_conversions, 0) as avg_conversion_value\n",
    "FROM channel_influence\n",
    "WHERE direct_count >= 100\n",
    "ORDER BY direct_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Graph analytics queries defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time-Series Analysis at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Time-series decomposition (trend, seasonality)\n",
    "time_series_decomposition = \"\"\"\n",
    "WITH daily_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', timestamp) as date,\n",
    "        channel,\n",
    "        SUM(revenue) as daily_revenue\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= DATEADD(year, -2, GETDATE())\n",
    "    GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    "),\n",
    "metrics_with_trend AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        channel,\n",
    "        daily_revenue,\n",
    "        -- 30-day moving average (trend)\n",
    "        AVG(daily_revenue) OVER (\n",
    "            PARTITION BY channel \n",
    "            ORDER BY date \n",
    "            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n",
    "        ) as trend_30d,\n",
    "        -- Day of week pattern\n",
    "        EXTRACT(DOW FROM date) as day_of_week,\n",
    "        -- Week of year pattern\n",
    "        EXTRACT(WEEK FROM date) as week_of_year\n",
    "    FROM daily_metrics\n",
    "),\n",
    "seasonality AS (\n",
    "    SELECT \n",
    "        channel,\n",
    "        day_of_week,\n",
    "        AVG(daily_revenue / NULLIF(trend_30d, 0)) as dow_seasonality_factor\n",
    "    FROM metrics_with_trend\n",
    "    WHERE trend_30d > 0\n",
    "    GROUP BY channel, day_of_week\n",
    ")\n",
    "SELECT \n",
    "    m.date,\n",
    "    m.channel,\n",
    "    m.daily_revenue as actual,\n",
    "    m.trend_30d as trend,\n",
    "    s.dow_seasonality_factor as seasonality,\n",
    "    m.daily_revenue - m.trend_30d as detrended,\n",
    "    m.daily_revenue - (m.trend_30d * s.dow_seasonality_factor) as residual\n",
    "FROM metrics_with_trend m\n",
    "LEFT JOIN seasonality s ON m.channel = s.channel AND m.day_of_week = s.day_of_week\n",
    "WHERE m.date >= DATEADD(day, -90, GETDATE())\n",
    "ORDER BY m.channel, m.date;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Forecasting with linear regression\n",
    "revenue_forecast = \"\"\"\n",
    "WITH daily_revenue AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', timestamp) as date,\n",
    "        channel,\n",
    "        SUM(revenue) as revenue,\n",
    "        ROW_NUMBER() OVER (PARTITION BY channel ORDER BY DATE_TRUNC('day', timestamp)) as day_number\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= DATEADD(day, -90, GETDATE())\n",
    "    GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    "),\n",
    "regression_stats AS (\n",
    "    SELECT \n",
    "        channel,\n",
    "        COUNT(*) as n,\n",
    "        AVG(day_number) as avg_x,\n",
    "        AVG(revenue) as avg_y,\n",
    "        SUM((day_number - AVG(day_number) OVER (PARTITION BY channel)) * \n",
    "            (revenue - AVG(revenue) OVER (PARTITION BY channel))) as sum_xy,\n",
    "        SUM(POWER(day_number - AVG(day_number) OVER (PARTITION BY channel), 2)) as sum_xx\n",
    "    FROM daily_revenue\n",
    "    GROUP BY channel, day_number, revenue\n",
    "),\n",
    "coefficients AS (\n",
    "    SELECT \n",
    "        channel,\n",
    "        SUM(sum_xy) / NULLIF(SUM(sum_xx), 0) as slope,\n",
    "        AVG(avg_y) - (SUM(sum_xy) / NULLIF(SUM(sum_xx), 0)) * AVG(avg_x) as intercept\n",
    "    FROM regression_stats\n",
    "    GROUP BY channel\n",
    ")\n",
    "SELECT \n",
    "    c.channel,\n",
    "    c.slope,\n",
    "    c.intercept,\n",
    "    -- Forecast next 7 days\n",
    "    c.intercept + c.slope * 91 as forecast_day_91,\n",
    "    c.intercept + c.slope * 97 as forecast_day_97,\n",
    "    CASE \n",
    "        WHEN c.slope > 0 THEN 'Growing'\n",
    "        WHEN c.slope < 0 THEN 'Declining'\n",
    "        ELSE 'Stable'\n",
    "    END as trend_direction\n",
    "FROM coefficients c;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Change point detection\n",
    "change_point_detection = \"\"\"\n",
    "WITH daily_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', timestamp) as date,\n",
    "        channel,\n",
    "        SUM(revenue) as revenue,\n",
    "        COUNT(*) as event_count\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= DATEADD(day, -60, GETDATE())\n",
    "    GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    "),\n",
    "rolling_stats AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        channel,\n",
    "        revenue,\n",
    "        AVG(revenue) OVER (\n",
    "            PARTITION BY channel \n",
    "            ORDER BY date \n",
    "            ROWS BETWEEN 13 PRECEDING AND CURRENT ROW\n",
    "        ) as ma_14d,\n",
    "        STDDEV(revenue) OVER (\n",
    "            PARTITION BY channel \n",
    "            ORDER BY date \n",
    "            ROWS BETWEEN 13 PRECEDING AND CURRENT ROW\n",
    "        ) as stddev_14d\n",
    "    FROM daily_metrics\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    channel,\n",
    "    revenue,\n",
    "    ma_14d,\n",
    "    stddev_14d,\n",
    "    (revenue - ma_14d) / NULLIF(stddev_14d, 0) as z_score,\n",
    "    CASE \n",
    "        WHEN ABS((revenue - ma_14d) / NULLIF(stddev_14d, 0)) > 3 THEN 'Significant Change'\n",
    "        WHEN ABS((revenue - ma_14d) / NULLIF(stddev_14d, 0)) > 2 THEN 'Notable Change'\n",
    "        ELSE 'Normal'\n",
    "    END as change_flag\n",
    "FROM rolling_stats\n",
    "WHERE date >= DATEADD(day, -30, GETDATE())\n",
    "ORDER BY channel, date;\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Exponential smoothing\n",
    "exponential_smoothing = \"\"\"\n",
    "WITH daily_data AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', timestamp) as date,\n",
    "        channel,\n",
    "        SUM(revenue) as revenue,\n",
    "        ROW_NUMBER() OVER (PARTITION BY channel ORDER BY DATE_TRUNC('day', timestamp)) as rn\n",
    "    FROM marketing_events\n",
    "    WHERE timestamp >= DATEADD(day, -90, GETDATE())\n",
    "    GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    "),\n",
    "smoothed AS (\n",
    "    SELECT \n",
    "        date,\n",
    "        channel,\n",
    "        revenue,\n",
    "        -- Simple exponential smoothing with alpha=0.3\n",
    "        revenue as current_value,\n",
    "        LAG(revenue) OVER (PARTITION BY channel ORDER BY date) as prev_value,\n",
    "        0.3 * revenue + 0.7 * LAG(revenue) OVER (PARTITION BY channel ORDER BY date) as smoothed_value\n",
    "    FROM daily_data\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    channel,\n",
    "    revenue as actual,\n",
    "    smoothed_value as smoothed,\n",
    "    revenue - smoothed_value as residual,\n",
    "    POWER(revenue - smoothed_value, 2) as squared_error\n",
    "FROM smoothed\n",
    "WHERE smoothed_value IS NOT NULL\n",
    "ORDER BY channel, date;\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Time-series analysis queries defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Aggregations and Pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Dynamic pivot using CASE statements\n",
    "pivot_revenue_by_channel = \"\"\"\n",
    "SELECT \n",
    "    DATE_TRUNC('month', timestamp) as month,\n",
    "    SUM(CASE WHEN channel = 'email' THEN revenue ELSE 0 END) as email_revenue,\n",
    "    SUM(CASE WHEN channel = 'social' THEN revenue ELSE 0 END) as social_revenue,\n",
    "    SUM(CASE WHEN channel = 'search' THEN revenue ELSE 0 END) as search_revenue,\n",
    "    SUM(CASE WHEN channel = 'display' THEN revenue ELSE 0 END) as display_revenue,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    -- Calculate percentages\n",
    "    SUM(CASE WHEN channel = 'email' THEN revenue ELSE 0 END) * 100.0 / NULLIF(SUM(revenue), 0) as email_pct,\n",
    "    SUM(CASE WHEN channel = 'social' THEN revenue ELSE 0 END) * 100.0 / NULLIF(SUM(revenue), 0) as social_pct,\n",
    "    SUM(CASE WHEN channel = 'search' THEN revenue ELSE 0 END) * 100.0 / NULLIF(SUM(revenue), 0) as search_pct,\n",
    "    SUM(CASE WHEN channel = 'display' THEN revenue ELSE 0 END) * 100.0 / NULLIF(SUM(revenue), 0) as display_pct\n",
    "FROM marketing_events\n",
    "WHERE timestamp >= '2024-01-01'\n",
    "GROUP BY DATE_TRUNC('month', timestamp)\n",
    "ORDER BY month;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: ROLLUP for hierarchical aggregations\n",
    "hierarchical_aggregation = \"\"\"\n",
    "SELECT \n",
    "    channel,\n",
    "    campaign_id,\n",
    "    event_type,\n",
    "    COUNT(*) as event_count,\n",
    "    SUM(revenue) as total_revenue,\n",
    "    GROUPING(channel) as is_channel_total,\n",
    "    GROUPING(campaign_id) as is_campaign_total,\n",
    "    GROUPING(event_type) as is_event_type_total\n",
    "FROM marketing_events\n",
    "WHERE date >= '2024-01-01'\n",
    "GROUP BY ROLLUP(channel, campaign_id, event_type)\n",
    "ORDER BY channel, campaign_id, event_type;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: CUBE for multi-dimensional analysis\n",
    "cube_analysis = \"\"\"\n",
    "SELECT \n",
    "    channel,\n",
    "    DATE_TRUNC('month', timestamp) as month,\n",
    "    event_type,\n",
    "    COUNT(*) as events,\n",
    "    SUM(revenue) as revenue,\n",
    "    COUNT(DISTINCT user_id) as unique_users\n",
    "FROM marketing_events\n",
    "WHERE timestamp >= '2024-01-01'\n",
    "GROUP BY CUBE(channel, DATE_TRUNC('month', timestamp), event_type)\n",
    "ORDER BY channel, month, event_type;\n",
    "\"\"\"\n",
    "\n",
    "# Example 4: Percentile aggregations\n",
    "percentile_analysis = \"\"\"\n",
    "SELECT \n",
    "    channel,\n",
    "    COUNT(*) as total_events,\n",
    "    AVG(revenue) as avg_revenue,\n",
    "    MEDIAN(revenue) as median_revenue,\n",
    "    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY revenue) as p25_revenue,\n",
    "    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY revenue) as p75_revenue,\n",
    "    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY revenue) as p90_revenue,\n",
    "    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY revenue) as p95_revenue,\n",
    "    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY revenue) as p99_revenue\n",
    "FROM marketing_events\n",
    "WHERE event_type = 'conversion'\n",
    "  AND revenue > 0\n",
    "GROUP BY channel;\n",
    "\"\"\"\n",
    "\n",
    "# Example 5: Moving aggregations\n",
    "moving_aggregations = \"\"\"\n",
    "WITH daily_metrics AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('day', timestamp) as date,\n",
    "        channel,\n",
    "        SUM(revenue) as daily_revenue,\n",
    "        COUNT(*) as daily_events\n",
    "    FROM marketing_events\n",
    "    GROUP BY DATE_TRUNC('day', timestamp), channel\n",
    ")\n",
    "SELECT \n",
    "    date,\n",
    "    channel,\n",
    "    daily_revenue,\n",
    "    -- Moving aggregations\n",
    "    SUM(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as revenue_7d,\n",
    "    AVG(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as avg_revenue_7d,\n",
    "    MIN(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as min_revenue_7d,\n",
    "    MAX(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as max_revenue_7d,\n",
    "    STDDEV(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as stddev_revenue_7d,\n",
    "    -- Cumulative aggregations\n",
    "    SUM(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS UNBOUNDED PRECEDING) as cumulative_revenue,\n",
    "    AVG(daily_revenue) OVER (PARTITION BY channel ORDER BY date ROWS UNBOUNDED PRECEDING) as cumulative_avg_revenue\n",
    "FROM daily_metrics\n",
    "ORDER BY channel, date;\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Advanced aggregation queries defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality and Validation at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Comprehensive data quality checks\n",
    "data_quality_checks = \"\"\"\n",
    "WITH quality_checks AS (\n",
    "    SELECT \n",
    "        'Row Count' as check_type,\n",
    "        COUNT(*) as metric_value,\n",
    "        CAST(NULL AS DECIMAL(10,2)) as threshold,\n",
    "        'INFO' as severity\n",
    "    FROM marketing_events\n",
    "    WHERE date = CURRENT_DATE - 1\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Null user_id' as check_type,\n",
    "        COUNT(*) as metric_value,\n",
    "        0 as threshold,\n",
    "        'ERROR' as severity\n",
    "    FROM marketing_events\n",
    "    WHERE date = CURRENT_DATE - 1\n",
    "      AND user_id IS NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Negative Revenue' as check_type,\n",
    "        COUNT(*) as metric_value,\n",
    "        0 as threshold,\n",
    "        'ERROR' as severity\n",
    "    FROM marketing_events\n",
    "    WHERE date = CURRENT_DATE - 1\n",
    "      AND revenue < 0\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Future Timestamps' as check_type,\n",
    "        COUNT(*) as metric_value,\n",
    "        0 as threshold,\n",
    "        'ERROR' as severity\n",
    "    FROM marketing_events\n",
    "    WHERE date = CURRENT_DATE - 1\n",
    "      AND timestamp > GETDATE()\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Duplicate event_id' as check_type,\n",
    "        COUNT(*) as metric_value,\n",
    "        0 as threshold,\n",
    "        'ERROR' as severity\n",
    "    FROM (\n",
    "        SELECT event_id, COUNT(*) as cnt\n",
    "        FROM marketing_events\n",
    "        WHERE date = CURRENT_DATE - 1\n",
    "        GROUP BY event_id\n",
    "        HAVING COUNT(*) > 1\n",
    "    )\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Conversion without click' as check_type,\n",
    "        COUNT(DISTINCT c.user_id) as metric_value,\n",
    "        CAST(NULL AS DECIMAL) as threshold,\n",
    "        'WARNING' as severity\n",
    "    FROM marketing_events c\n",
    "    WHERE c.date = CURRENT_DATE - 1\n",
    "      AND c.event_type = 'conversion'\n",
    "      AND NOT EXISTS (\n",
    "          SELECT 1 FROM marketing_events e\n",
    "          WHERE e.user_id = c.user_id\n",
    "            AND e.event_type = 'click'\n",
    "            AND e.timestamp < c.timestamp\n",
    "            AND e.timestamp >= c.timestamp - INTERVAL '30 days'\n",
    "      )\n",
    ")\n",
    "SELECT \n",
    "    check_type,\n",
    "    metric_value,\n",
    "    threshold,\n",
    "    severity,\n",
    "    CASE \n",
    "        WHEN threshold IS NULL THEN 'N/A'\n",
    "        WHEN metric_value <= threshold THEN 'PASS'\n",
    "        ELSE 'FAIL'\n",
    "    END as status\n",
    "FROM quality_checks\n",
    "ORDER BY \n",
    "    CASE severity \n",
    "        WHEN 'ERROR' THEN 1 \n",
    "        WHEN 'WARNING' THEN 2 \n",
    "        ELSE 3 \n",
    "    END,\n",
    "    check_type;\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Statistical outlier detection\n",
    "outlier_detection = \"\"\"\n",
    "WITH revenue_stats AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        SUM(revenue) as total_revenue,\n",
    "        COUNT(*) as transaction_count,\n",
    "        AVG(revenue) as avg_transaction_value\n",
    "    FROM marketing_events\n",
    "    WHERE event_type = 'conversion'\n",
    "      AND date >= CURRENT_DATE - 30\n",
    "    GROUP BY user_id\n",
    "),\n",
    "distribution_stats AS (\n",
    "    SELECT \n",
    "        AVG(total_revenue) as mean_revenue,\n",
    "        STDDEV(total_revenue) as stddev_revenue,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_revenue) as q1,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_revenue) as q3\n",
    "    FROM revenue_stats\n",
    ")\n",
    "SELECT \n",
    "    r.user_id,\n",
    "    r.total_revenue,\n",
    "    r.transaction_count,\n",
    "    r.avg_transaction_value,\n",
    "    d.mean_revenue,\n",
    "    d.stddev_revenue,\n",
    "    -- Z-score method\n",
    "    (r.total_revenue - d.mean_revenue) / NULLIF(d.stddev_revenue, 0) as z_score,\n",
    "    -- IQR method\n",
    "    d.q3 - d.q1 as iqr,\n",
    "    d.q1 - 1.5 * (d.q3 - d.q1) as lower_bound,\n",
    "    d.q3 + 1.5 * (d.q3 - d.q1) as upper_bound,\n",
    "    CASE \n",
    "        WHEN ABS((r.total_revenue - d.mean_revenue) / NULLIF(d.stddev_revenue, 0)) > 3 THEN 'Z-Score Outlier'\n",
    "        WHEN r.total_revenue < d.q1 - 1.5 * (d.q3 - d.q1) THEN 'IQR Low Outlier'\n",
    "        WHEN r.total_revenue > d.q3 + 1.5 * (d.q3 - d.q1) THEN 'IQR High Outlier'\n",
    "        ELSE 'Normal'\n",
    "    END as outlier_type\n",
    "FROM revenue_stats r\n",
    "CROSS JOIN distribution_stats d\n",
    "WHERE ABS((r.total_revenue - d.mean_revenue) / NULLIF(d.stddev_revenue, 0)) > 3\n",
    "   OR r.total_revenue < d.q1 - 1.5 * (d.q3 - d.q1)\n",
    "   OR r.total_revenue > d.q3 + 1.5 * (d.q3 - d.q1)\n",
    "ORDER BY ABS((r.total_revenue - d.mean_revenue) / NULLIF(d.stddev_revenue, 0)) DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Data completeness monitoring\n",
    "data_completeness = \"\"\"\n",
    "SELECT \n",
    "    date,\n",
    "    COUNT(*) as total_rows,\n",
    "    COUNT(user_id) as user_id_count,\n",
    "    COUNT(campaign_id) as campaign_id_count,\n",
    "    COUNT(channel) as channel_count,\n",
    "    COUNT(revenue) as revenue_count,\n",
    "    -- Completeness percentages\n",
    "    COUNT(user_id) * 100.0 / COUNT(*) as user_id_completeness,\n",
    "    COUNT(campaign_id) * 100.0 / COUNT(*) as campaign_id_completeness,\n",
    "    COUNT(channel) * 100.0 / COUNT(*) as channel_completeness,\n",
    "    COUNT(revenue) * 100.0 / COUNT(*) as revenue_completeness,\n",
    "    -- Day-over-day change\n",
    "    COUNT(*) - LAG(COUNT(*)) OVER (ORDER BY date) as row_change,\n",
    "    (COUNT(*) - LAG(COUNT(*)) OVER (ORDER BY date)) * 100.0 / \n",
    "        NULLIF(LAG(COUNT(*)) OVER (ORDER BY date), 0) as row_change_pct\n",
    "FROM marketing_events\n",
    "WHERE date >= CURRENT_DATE - 30\n",
    "GROUP BY date\n",
    "ORDER BY date DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Data quality queries defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Project: Multi-Touch Attribution Optimization\n",
    "\n",
    "### Complete attribution system implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build user journey table\n",
    "create_user_journeys = \"\"\"\n",
    "CREATE TABLE user_journeys AS\n",
    "WITH ordered_events AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        event_id,\n",
    "        timestamp,\n",
    "        channel,\n",
    "        campaign_id,\n",
    "        event_type,\n",
    "        revenue,\n",
    "        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY timestamp) as event_sequence,\n",
    "        LEAD(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp) as next_event_time,\n",
    "        LEAD(event_type) OVER (PARTITION BY user_id ORDER BY timestamp) as next_event_type\n",
    "    FROM marketing_events\n",
    "    WHERE date >= '2024-01-01'\n",
    "),\n",
    "conversions AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        MAX(CASE WHEN event_type = 'conversion' THEN timestamp END) as conversion_time,\n",
    "        MAX(CASE WHEN event_type = 'conversion' THEN revenue END) as conversion_value\n",
    "    FROM ordered_events\n",
    "    GROUP BY user_id\n",
    "    HAVING MAX(CASE WHEN event_type = 'conversion' THEN 1 ELSE 0 END) = 1\n",
    ")\n",
    "SELECT \n",
    "    e.user_id,\n",
    "    e.event_id,\n",
    "    e.timestamp,\n",
    "    e.channel,\n",
    "    e.campaign_id,\n",
    "    e.event_type,\n",
    "    e.event_sequence,\n",
    "    c.conversion_time,\n",
    "    c.conversion_value,\n",
    "    DATEDIFF(second, e.timestamp, c.conversion_time) as seconds_to_conversion,\n",
    "    CASE WHEN e.event_sequence = 1 THEN 1 ELSE 0 END as is_first_touch,\n",
    "    CASE WHEN e.next_event_type = 'conversion' THEN 1 ELSE 0 END as is_last_touch\n",
    "FROM ordered_events e\n",
    "INNER JOIN conversions c ON e.user_id = c.user_id\n",
    "WHERE e.timestamp <= c.conversion_time\n",
    "DISTKEY(user_id)\n",
    "SORTKEY(user_id, event_sequence);\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: First-touch attribution\n",
    "first_touch_attribution = \"\"\"\n",
    "SELECT \n",
    "    channel,\n",
    "    campaign_id,\n",
    "    COUNT(DISTINCT user_id) as attributed_conversions,\n",
    "    SUM(conversion_value) as attributed_revenue\n",
    "FROM user_journeys\n",
    "WHERE is_first_touch = 1\n",
    "GROUP BY channel, campaign_id\n",
    "ORDER BY attributed_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Last-touch attribution\n",
    "last_touch_attribution = \"\"\"\n",
    "SELECT \n",
    "    channel,\n",
    "    campaign_id,\n",
    "    COUNT(DISTINCT user_id) as attributed_conversions,\n",
    "    SUM(conversion_value) as attributed_revenue\n",
    "FROM user_journeys\n",
    "WHERE is_last_touch = 1\n",
    "GROUP BY channel, campaign_id\n",
    "ORDER BY attributed_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Linear attribution\n",
    "linear_attribution = \"\"\"\n",
    "WITH touchpoint_counts AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        COUNT(*) as total_touchpoints\n",
    "    FROM user_journeys\n",
    "    GROUP BY user_id\n",
    ")\n",
    "SELECT \n",
    "    j.channel,\n",
    "    j.campaign_id,\n",
    "    COUNT(*) as total_touchpoints,\n",
    "    SUM(j.conversion_value / t.total_touchpoints) as attributed_revenue\n",
    "FROM user_journeys j\n",
    "INNER JOIN touchpoint_counts t ON j.user_id = t.user_id\n",
    "GROUP BY j.channel, j.campaign_id\n",
    "ORDER BY attributed_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Step 5: Time-decay attribution\n",
    "time_decay_attribution = \"\"\"\n",
    "WITH weighted_touchpoints AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        channel,\n",
    "        campaign_id,\n",
    "        conversion_value,\n",
    "        seconds_to_conversion,\n",
    "        -- Exponential decay: weight = e^(-lambda * time)\n",
    "        -- Using 7-day half-life: lambda = ln(2) / (7 * 86400)\n",
    "        EXP(-0.000001155 * seconds_to_conversion) as time_weight\n",
    "    FROM user_journeys\n",
    "),\n",
    "normalized_weights AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        channel,\n",
    "        campaign_id,\n",
    "        conversion_value,\n",
    "        time_weight,\n",
    "        SUM(time_weight) OVER (PARTITION BY user_id) as total_weight\n",
    "    FROM weighted_touchpoints\n",
    ")\n",
    "SELECT \n",
    "    channel,\n",
    "    campaign_id,\n",
    "    COUNT(DISTINCT user_id) as users_influenced,\n",
    "    SUM(conversion_value * time_weight / total_weight) as attributed_revenue\n",
    "FROM normalized_weights\n",
    "GROUP BY channel, campaign_id\n",
    "ORDER BY attributed_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Step 6: Position-based (U-shaped) attribution\n",
    "position_based_attribution = \"\"\"\n",
    "WITH journey_positions AS (\n",
    "    SELECT \n",
    "        user_id,\n",
    "        channel,\n",
    "        campaign_id,\n",
    "        conversion_value,\n",
    "        event_sequence,\n",
    "        MAX(event_sequence) OVER (PARTITION BY user_id) as total_touchpoints,\n",
    "        CASE \n",
    "            WHEN event_sequence = 1 THEN 0.4  -- First touch: 40%\n",
    "            WHEN event_sequence = MAX(event_sequence) OVER (PARTITION BY user_id) THEN 0.4  -- Last touch: 40%\n",
    "            ELSE 0.2 / NULLIF(MAX(event_sequence) OVER (PARTITION BY user_id) - 2, 0)  -- Middle touches: 20% divided\n",
    "        END as position_weight\n",
    "    FROM user_journeys\n",
    ")\n",
    "SELECT \n",
    "    channel,\n",
    "    campaign_id,\n",
    "    COUNT(DISTINCT user_id) as users_influenced,\n",
    "    SUM(conversion_value * position_weight) as attributed_revenue\n",
    "FROM journey_positions\n",
    "GROUP BY channel, campaign_id\n",
    "ORDER BY attributed_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Step 7: Compare all attribution models\n",
    "compare_attribution_models = \"\"\"\n",
    "WITH first_touch AS (\n",
    "    SELECT channel, SUM(conversion_value) as revenue\n",
    "    FROM user_journeys\n",
    "    WHERE is_first_touch = 1\n",
    "    GROUP BY channel\n",
    "),\n",
    "last_touch AS (\n",
    "    SELECT channel, SUM(conversion_value) as revenue\n",
    "    FROM user_journeys\n",
    "    WHERE is_last_touch = 1\n",
    "    GROUP BY channel\n",
    "),\n",
    "linear AS (\n",
    "    SELECT \n",
    "        j.channel,\n",
    "        SUM(j.conversion_value / t.total_touchpoints) as revenue\n",
    "    FROM user_journeys j\n",
    "    INNER JOIN (\n",
    "        SELECT user_id, COUNT(*) as total_touchpoints\n",
    "        FROM user_journeys\n",
    "        GROUP BY user_id\n",
    "    ) t ON j.user_id = t.user_id\n",
    "    GROUP BY j.channel\n",
    ")\n",
    "SELECT \n",
    "    COALESCE(ft.channel, lt.channel, l.channel) as channel,\n",
    "    ft.revenue as first_touch_revenue,\n",
    "    lt.revenue as last_touch_revenue,\n",
    "    l.revenue as linear_revenue,\n",
    "    -- Calculate variance across models\n",
    "    STDDEV_SAMP(ARRAY[ft.revenue, lt.revenue, l.revenue]) as model_variance\n",
    "FROM first_touch ft\n",
    "FULL OUTER JOIN last_touch lt ON ft.channel = lt.channel\n",
    "FULL OUTER JOIN linear l ON COALESCE(ft.channel, lt.channel) = l.channel\n",
    "ORDER BY first_touch_revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"\n",
    "✓ Multi-Touch Attribution System Created\n",
    "\n",
    "Attribution Models Implemented:\n",
    "1. First-Touch: All credit to first interaction\n",
    "2. Last-Touch: All credit to last interaction\n",
    "3. Linear: Equal credit to all touchpoints\n",
    "4. Time-Decay: More credit to recent interactions\n",
    "5. Position-Based: 40% first, 40% last, 20% middle\n",
    "\n",
    "Next Steps:\n",
    "- Create materialized views for each model\n",
    "- Build reporting dashboard\n",
    "- Schedule daily refreshes\n",
    "- Compare model predictions vs actual results\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Query Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization checklist\n",
    "optimization_techniques = \"\"\"\n",
    "-- 1. Use CTEs to break complex queries into steps\n",
    "-- 2. Filter early (WHERE clauses before joins)\n",
    "-- 3. Use appropriate distribution keys\n",
    "-- 4. Add sort keys for common filters\n",
    "-- 5. Avoid SELECT * - specify columns\n",
    "-- 6. Use EXPLAIN to analyze query plans\n",
    "-- 7. Materialize intermediate results for complex queries\n",
    "-- 8. Use UNLOAD for large exports\n",
    "-- 9. Partition large tables by date\n",
    "-- 10. Regular VACUUM and ANALYZE\n",
    "\"\"\"\n",
    "\n",
    "# Example: Optimize slow attribution query\n",
    "optimized_attribution = \"\"\"\n",
    "-- BEFORE: Slow query with multiple scans\n",
    "-- SELECT ... multiple window functions on full table\n",
    "\n",
    "-- AFTER: Optimized with CTEs and filtering\n",
    "WITH recent_events AS (\n",
    "    -- Filter early\n",
    "    SELECT user_id, channel, timestamp, revenue\n",
    "    FROM marketing_events\n",
    "    WHERE date >= '2024-01-01'\n",
    "      AND date < '2024-02-01'\n",
    "      AND event_type IN ('click', 'conversion')\n",
    "),\n",
    "user_conversions AS (\n",
    "    -- Identify converters only\n",
    "    SELECT user_id, MAX(timestamp) as conversion_time, MAX(revenue) as revenue\n",
    "    FROM recent_events\n",
    "    WHERE revenue > 0\n",
    "    GROUP BY user_id\n",
    "),\n",
    "attribution_data AS (\n",
    "    -- Join only necessary data\n",
    "    SELECT \n",
    "        e.channel,\n",
    "        c.revenue,\n",
    "        ROW_NUMBER() OVER (PARTITION BY e.user_id ORDER BY e.timestamp DESC) as rn\n",
    "    FROM recent_events e\n",
    "    INNER JOIN user_conversions c ON e.user_id = c.user_id AND e.timestamp <= c.conversion_time\n",
    ")\n",
    "SELECT \n",
    "    channel,\n",
    "    SUM(revenue) as attributed_revenue\n",
    "FROM attribution_data\n",
    "WHERE rn = 1  -- Last touch\n",
    "GROUP BY channel;\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Optimization techniques documented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices Summary\n",
    "\n",
    "### Advanced SQL Techniques\n",
    "1. Use Python UDFs for complex logic not possible in SQL\n",
    "2. Leverage recursive CTEs for hierarchical data\n",
    "3. Apply window functions for sequential analysis\n",
    "4. Implement multiple attribution models for comparison\n",
    "5. Use time-series functions for trend analysis\n",
    "\n",
    "### Performance\n",
    "1. Always use EXPLAIN for complex queries\n",
    "2. Filter early and often\n",
    "3. Use appropriate distribution and sort keys\n",
    "4. Materialize frequently-used intermediate results\n",
    "5. Monitor query performance regularly\n",
    "\n",
    "### Data Quality\n",
    "1. Implement comprehensive data quality checks\n",
    "2. Monitor data completeness and consistency\n",
    "3. Detect and handle outliers appropriately\n",
    "4. Validate business logic constraints\n",
    "5. Track data quality metrics over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercises\n",
    "\n",
    "### Exercise 1: Attribution Models\n",
    "Implement and compare:\n",
    "1. First-touch attribution\n",
    "2. Last-touch attribution\n",
    "3. Linear attribution\n",
    "4. Time-decay attribution\n",
    "5. Custom weighted attribution\n",
    "Analyze which model best fits your data.\n",
    "\n",
    "### Exercise 2: User Journey Analysis\n",
    "1. Build complete user journey table\n",
    "2. Identify common conversion paths\n",
    "3. Calculate path metrics (length, value, time)\n",
    "4. Find high-performing channel sequences\n",
    "5. Optimize marketing mix based on insights\n",
    "\n",
    "### Exercise 3: Time-Series Analysis\n",
    "1. Decompose revenue into trend and seasonality\n",
    "2. Detect anomalies in daily metrics\n",
    "3. Forecast next 30 days revenue\n",
    "4. Identify change points\n",
    "5. Create alerting rules\n",
    "\n",
    "### Exercise 4: Data Quality Framework\n",
    "Build a complete data quality framework:\n",
    "1. Define quality metrics\n",
    "2. Implement automated checks\n",
    "3. Create quality dashboards\n",
    "4. Set up alerting\n",
    "5. Track quality trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "### Documentation\n",
    "- [Redshift UDFs](https://docs.aws.amazon.com/redshift/latest/dg/user-defined-functions.html)\n",
    "- [Window Functions](https://docs.aws.amazon.com/redshift/latest/dg/c_Window_functions.html)\n",
    "- [Recursive CTEs](https://docs.aws.amazon.com/redshift/latest/dg/r_WITH_clause.html)\n",
    "- [Attribution Modeling Guide](https://support.google.com/analytics/answer/1662518)\n",
    "\n",
    "### Papers & Articles\n",
    "- Multi-Touch Attribution: Theory and Practice\n",
    "- Markov Chain Attribution Models\n",
    "- Shapley Value for Marketing Attribution\n",
    "- Time-Series Analysis for Marketing\n",
    "\n",
    "### Tools\n",
    "- Redshift Query Editor\n",
    "- Mode Analytics: SQL + visualization\n",
    "- Looker: BI platform\n",
    "- dbt: Data transformation tool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
